{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4209221127.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    from doctr\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import camelot\n",
    "from pdf2image import convert_from_path\n",
    "import layoutparser as lp\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "from doctr.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Advanced PDF Table Extraction Pipeline with Advanced Table Structure Parsing\n",
    "\n",
    "This script provides a complete pipeline to extract tables from PDF documents.\n",
    "It first detects if the PDF is digital (embedded text) or scanned (image–based) and\n",
    "then applies one of two extraction strategies:\n",
    "\n",
    "For Digital PDFs:\n",
    "    - Uses Camelot (with the “stream” flavor) to extract tables.\n",
    "\n",
    "For Scanned PDFs:\n",
    "    - Converts PDF pages to images using pdf2image.\n",
    "    - Uses LayoutParser (with a PubLayNet pre-trained model) to detect table regions.\n",
    "    - For each table region, the image is preprocessed and then an advanced table–structure\n",
    "      parser is applied. This parser uses Tesseract’s TSV output to reconstruct the table\n",
    "      layout, grouping recognized words into rows and columns based on their bounding boxes.\n",
    "\n",
    "Requirements:\n",
    "    - python3\n",
    "    - camelot-py[cv]\n",
    "    - pdf2image\n",
    "    - layoutparser\n",
    "    - pytesseract\n",
    "    - opencv-python\n",
    "    - PyPDF2\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - Tesseract OCR installed and available in your PATH\n",
    "\n",
    "Install the Python packages with:\n",
    "    pip install camelot-py[cv] pdf2image layoutparser pytesseract opencv-python PyPDF2 pandas numpy\n",
    "\n",
    "Make sure to install Tesseract OCR (see https://github.com/tesseract-ocr/tesseract) and configure LayoutParser’s Detectron2 environment.\n",
    "\n",
    "Usage:\n",
    "    python pdf_table_extractor.py path/to/your/document.pdf --output_dir tables_output --dpi 300\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def is_digital_pdf(pdf_path, threshold=100):\n",
    "    \"\"\"\n",
    "    Determine if a PDF is digital (has extractable text) by reading the first few pages.\n",
    "    Returns True if digital; otherwise, returns False (likely scanned).\n",
    "    \"\"\"\n",
    "    text_content = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text_content += page_text\n",
    "                if i >= 2:  # sample first 3 pages\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF for digital detection: {e}\")\n",
    "        return False\n",
    "    return len(text_content.strip()) > threshold\n",
    "\n",
    "def extract_tables_digital(pdf_path, pages=\"1-end\"):\n",
    "    \"\"\"\n",
    "    Extract tables from a digital PDF using Camelot.\n",
    "    Returns a list of Camelot Table objects.\n",
    "    \"\"\"\n",
    "    print(\"Extracting tables using Camelot...\")\n",
    "    try:\n",
    "        tables = camelot.read_pdf(pdf_path, pages=pages, flavor=\"stream\")\n",
    "        print(f\"Found {len(tables)} table(s).\")\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables with Camelot: {e}\")\n",
    "        return []\n",
    "\n",
    "def advanced_table_structure_parser(image, tesseract_config=r'--oem 3 --psm 6', row_threshold=10, col_threshold=10):\n",
    "    \"\"\"\n",
    "    Advanced table structure parser that uses Tesseract's TSV output to reconstruct the table layout.\n",
    "    \n",
    "    Parameters:\n",
    "        image              : Preprocessed image (as a NumPy array) of the table region.\n",
    "        tesseract_config   : Configuration string for Tesseract OCR.\n",
    "        row_threshold      : Pixel threshold to group words in the same row.\n",
    "        col_threshold      : Pixel threshold to group words in the same column.\n",
    "    \n",
    "    Returns:\n",
    "        A Pandas DataFrame representing the reconstructed table, or None if no text is found.\n",
    "    \"\"\"\n",
    "    # Obtain detailed OCR data (including bounding boxes) as a DataFrame.\n",
    "    ocr_data = pytesseract.image_to_data(image, config=tesseract_config, output_type=pytesseract.Output.DATAFRAME)\n",
    "    \n",
    "    # Drop rows with missing or empty text.\n",
    "    ocr_data = ocr_data.dropna(subset=['text'])\n",
    "    ocr_data = ocr_data[ocr_data['text'].str.strip() != '']\n",
    "    if ocr_data.empty:\n",
    "        return None\n",
    "\n",
    "    # Sort by the 'top' coordinate to group words vertically.\n",
    "    ocr_data = ocr_data.sort_values(by='top').reset_index(drop=True)\n",
    "    \n",
    "    # Cluster words into rows based on the 'top' coordinate.\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    current_top = None\n",
    "    for idx, word in ocr_data.iterrows():\n",
    "        if current_top is None:\n",
    "            current_top = word['top']\n",
    "            current_row.append(word)\n",
    "        else:\n",
    "            if abs(word['top'] - current_top) <= row_threshold:\n",
    "                current_row.append(word)\n",
    "            else:\n",
    "                rows.append(pd.DataFrame(current_row))\n",
    "                current_row = [word]\n",
    "                current_top = word['top']\n",
    "    if current_row:\n",
    "        rows.append(pd.DataFrame(current_row))\n",
    "    \n",
    "    # Determine column boundaries using all 'left' coordinates in the table.\n",
    "    all_lefts = sorted(ocr_data['left'].tolist())\n",
    "    col_boundaries = []\n",
    "    current_group = [all_lefts[0]]\n",
    "    for x in all_lefts[1:]:\n",
    "        if abs(x - current_group[-1]) <= col_threshold:\n",
    "            current_group.append(x)\n",
    "        else:\n",
    "            col_boundaries.append(np.mean(current_group))\n",
    "            current_group = [x]\n",
    "    if current_group:\n",
    "        col_boundaries.append(np.mean(current_group))\n",
    "    col_boundaries = sorted(col_boundaries)\n",
    "    \n",
    "    # For each row, assign words to the nearest column boundary.\n",
    "    table_data = []\n",
    "    for row_df in rows:\n",
    "        row_df = row_df.sort_values(by='left')\n",
    "        row_cells = [''] * len(col_boundaries)\n",
    "        for _, word in row_df.iterrows():\n",
    "            # Determine the nearest column by comparing the 'left' coordinate.\n",
    "            distances = [abs(word['left'] - b) for b in col_boundaries]\n",
    "            col_idx = int(np.argmin(distances))\n",
    "            if row_cells[col_idx]:\n",
    "                row_cells[col_idx] += ' ' + word['text']\n",
    "            else:\n",
    "                row_cells[col_idx] = word['text']\n",
    "        table_data.append(row_cells)\n",
    "    \n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    return table_df\n",
    "\n",
    "def extract_tables_scanned(pdf_path, dpi=300, tesseract_config=r'--oem 3 --psm 6'):\n",
    "    \"\"\"\n",
    "    Extract tables from a scanned PDF using LayoutParser for table detection and an advanced\n",
    "    table–structure parser for OCR reconstruction.\n",
    "    \n",
    "    Returns a list of Pandas DataFrame objects representing the detected tables.\n",
    "    \"\"\"\n",
    "    print(\"Converting PDF pages to images...\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting PDF to images: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Initialize LayoutParser's table detection model (PubLayNet pre-trained model)\n",
    "    model = lp.Detectron2LayoutModel(\n",
    "       config_path='lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config',\n",
    "       model_path='lp://PubLayNet/faster_rcnn_R_50_FPN_3x/model',\n",
    "       extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n",
    "       label_map={3: \"Table\"}\n",
    "    )\n",
    "    \n",
    "    extracted_tables = []\n",
    "    for page_num, image in enumerate(images, start=1):\n",
    "        print(f\"Processing page {page_num}...\")\n",
    "        image_np = np.array(image)\n",
    "        layout = model.detect(image_np)\n",
    "        # Filter for detected table regions.\n",
    "        table_blocks = [block for block in layout if block.type == 'Table']\n",
    "        print(f\"Detected {len(table_blocks)} table region(s) on page {page_num}.\")\n",
    "        \n",
    "        for idx, block in enumerate(table_blocks, start=1):\n",
    "            x1, y1, x2, y2 = map(int, block.coordinates)\n",
    "            cropped = image_np[y1:y2, x1:x2]\n",
    "            \n",
    "            # Preprocess the cropped region for OCR: convert to grayscale and apply thresholding.\n",
    "            gray = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
    "            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Use the advanced table–structure parser to reconstruct the table layout.\n",
    "            table_df = advanced_table_structure_parser(thresh, tesseract_config=tesseract_config)\n",
    "            if table_df is not None:\n",
    "                extracted_tables.append(table_df)\n",
    "                print(f\"Extracted advanced table {idx} on page {page_num} with shape {table_df.shape}.\")\n",
    "            else:\n",
    "                print(f\"Advanced parser failed to extract table region {idx} on page {page_num}.\")\n",
    "    return extracted_tables\n",
    "\n",
    "def save_tables(tables, output_dir, prefix=\"table\"):\n",
    "    \"\"\"\n",
    "    Save the extracted tables as CSV files in the specified output directory.\n",
    "    The input `tables` can be a list of Camelot Table objects or Pandas DataFrames.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for idx, table in enumerate(tables, start=1):\n",
    "        # For Camelot table objects, use the 'df' attribute.\n",
    "        if hasattr(table, 'df'):\n",
    "            df = table.df\n",
    "        elif isinstance(table, pd.DataFrame):\n",
    "            df = table\n",
    "        else:\n",
    "            continue\n",
    "        output_path = os.path.join(output_dir, f\"{prefix}_{idx}.csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved table {idx} to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"input/mml-book.pdf\"\n",
    "    output_dir = \"output\"\n",
    "    \n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"PDF file {pdf_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if is_digital_pdf(pdf_path):\n",
    "        print(\"Digital PDF detected. Using Camelot for table extraction.\")\n",
    "        tables = extract_tables_digital(pdf_path)\n",
    "    else:\n",
    "        print(\"Scanned PDF detected. Using LayoutParser and advanced table structure parser for table extraction.\")\n",
    "        tables = extract_tables_scanned(pdf_path, dpi=300)\n",
    "    \n",
    "    if not tables:\n",
    "        print(\"No tables were extracted.\")\n",
    "    else:\n",
    "        save_tables(tables, output_dir)\n",
    "        print(\"Table extraction completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('output/table_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Foreword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Acknowledgments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are grateful to many people who looked at e...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suffered through painful expositions of concep...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>their ideas that we did not vehemently disagre...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0                1\n",
       "0                                                  4         Foreword\n",
       "1                                                NaN  Acknowledgments\n",
       "2  We are grateful to many people who looked at e...              NaN\n",
       "3  suffered through painful expositions of concep...              NaN\n",
       "4  their ideas that we did not vehemently disagre...              NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from table-transformer-main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
