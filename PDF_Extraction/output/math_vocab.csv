Math Vocabulary,Theorems,Mathematical Concepts,Jargon

"feedback" "improvements via github.com, which greatly improved"
"Linear Algebra","The study of vectors and certain rules to manipulate vectors."
"Vectors","Special objects that can be added together and multiplied by scalars to produce another object of the same kind."
"Geometric Vectors","Vectors commonly known from school, usually denoted by a small arrow above the letter."
"machine learning","Ability of computers to learn from data and make predictions."
"data scientist" ,"A practitioner skilled in analyzing and interpreting complex data."
"algorithms","Step-by-step procedures for calculations or problem-solving."
"prediction","The outcome guessed based on data analysis."
"analysis pipeline","A sequence of data processing steps to analyze data."
"maximum likelihood","parameter estimation via maximum likelihood."
"maximum a posteriori estimation","parameter estimation via maximum a posteriori."
"Bayesian linear regression","we integrate the parameters out instead of optimizing them."
"dimensionality reduction","to find a compact, lower-dimensional representation of high-dimensional data."
"principal component analysis","a method used in dimensionality reduction."
"Gaussian","a type of probability distribution used in density estimation."
"machine learning","A data-driven approach to designing methodologies for extracting valuable patterns from data."
"data","Core component of machine learning, which is inherently data driven."
"model","A representation that describes a function mapping inputs to real-valued outputs."
"regression","A type of model that describes the relationship between inputs and real-valued outputs."
"generalize","The ability of a model to perform well on unseen data."
"machine learning","mathematical foundations of basic machine learning concepts"
"Mathematics for Machine Learning","a book published by Cambridge University Press"
"skills gap","the difference between the skills that workers have and the skills needed for a particular job"
"linearly dependent","Given vectors that can be expressed as a linear combination of others."
"linearly independent","Vectors that cannot be expressed as a linear combination of others."
"linear combination","A way of combining vectors using scalar multiplication and addition."
"Gaussian elimination","A method for determining whether a set of vectors are linearly independent by transforming them into a simpler form."
"Eigenvalue","Lagrange multiplier or a value associated with a linear transformation indicating how much the eigenvector is stretched or shrunk."
"Eigenspace","The set of all eigenvectors corresponding to a particular eigenvalue, together with the zero vector."
"machine learning","the mathematical basis of machine learning and uncover relationships between different tasks."
"music composers","create new and amazing pieces within the rules and structure of musical theory."
"technical books","provide a high-level overview for people who want to become composers of machine learning."
"researchers","able to propose and explore novel approaches for learning from data."
"Linear Mappings","The kth column of T is the coordinate representation of ¢, with respect to C. Note that both S and T are regular."
"Linearity","We exploited the linearity of 6."
"Theorem 2.20","This is the theorem proved in the text."
"ambiguity","The issue of uncertainty where the same expressions can mean different things depending on the context."
"mathematical concepts","The fundamental ideas and principles used to discuss the components of a machine learning system."
"data","Information that can be numerical or non-numerical, though often represented in number format for computational purposes."
"numerical representation","A format in which data is expressed in numerical terms suitable for reading into a computer program."
"vectors","Mathematical objects used to represent data in a format that can be processed by computations."
"vector space","the set of vectors that can result by starting with a small set of vectors, and adding them to each other and scaling them"
"machine learning","the concept of a vector space and its properties underlie much of machine learning"
"mathematical concepts","focus on the mathematical concepts behind the models themselves."
"machine learning algorithms","four representative examples of machine learning algorithms."
"mathematical background","intention is to provide the mathematical background, applied to four central machine learning problems."
"central machine learning problems","applied to four central machine learning problems."
"practical questions","connect practical questions arising from the use of machine learning with fundamental choices in the mathematical model."
"Learning","can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model."
"Machine Learning","has seen many success stories, and software is readily available to design and train rich and flexible systems."
"Mathematical Foundations","are important in order to understand fundamental principles upon which more complicated systems are built."
"Principles","can facilitate creating new solutions, understanding and debugging existing approaches, and learning about the inherent assumptions and limitations of the methodologies."
"general solution","The set of all solutions of the equation system."
"particular solution","A specific solution to the equation Ax = b."
"solutions to Ax = 0","All solutions that correspond to the homogeneous equation."
"Gaussian elimination","A constructive algorithmic way of transforming any system of linear equations into a particularly simple form."
"elementary transformations","Key operations used in Gaussian elimination to manipulate the matrix."
"gradient","the concept of gradients, which we subsequently use in Chapter 7, where we talk about optimization to find maxima/minima of functions."
"optimization","the process of finding maxima/minima of functions."
"maxima","the highest point of a function or dataset."
"minima","the lowest point of a function or dataset."
"machine learning","the second part of the book introduces four pillars of machine learning."
"parameter estimation","the three components of machine learning (data, models, and parameter estimation) in a mathematical fashion."
"linear regression","a close look at linear regression, our objective is to find functions that map inputs x € IR” to co."
"machine learning","Machine learning builds upon the language of mathematics to express concepts that seem intuitively obvious but that are surprisingly difficult to formalize."
"formalize","Once formalized properly, we can gain insights into the task we want to solve."
"topics","One common complaint of students of mathematics around the globe is that the topics covered seem to have li"
"vector","a vector as an array of numbers (a computer science view), a vector as an arrow with a direction and magnitude (a physics view), and a vector as an object that obeys addition and scaling (a mathematical view)"
"model","a model is typically used to describe a process for generating data, similar to the dataset at hand"
"training","training the model means to use the data available to optimize some parameters of the model with respect to a utility function that evaluates how well the model predicts the training data"
"utility function","a utility function that evaluates how well the model predicts the training data"
"underlying","principles."
"mathematical style","written in an academic mathematical style."
"machine learning","the concepts behind machine learning."
"derivatives","the reader should have seen derivatives."
"integrals","the reader should have seen integrals."
"geometric vectors","geometric vectors in two or three dimensions."
"two dimensions","geometric vectors in two or three dimensions."
"three dimensions","geometric vectors in two or three dimensions."
"generalize","we generalize these concepts."
"target audience","the target audience of the book includes undergraduate university students, evening learners and learners participating in online machine learning courses."
"data",""
"model",""
"learning",""
"linear algebra",""
"analytic geometry",""
"matrix decomposition",""
"Vector Calculus",""
"Probability & Distributions",""
"Optimization",""
"mathematical concepts","link mathematical concepts with machine learning algorithms"
"on form",""

"particular solution",""

"general solution",""

"pivot",""

"row-echelon form","(REF)."

"leading coefficient","first nonzero number from the left"

"basic variable",""

"free variable",""

"augmented matrix",""

"system of linear equations",""

"constructive way",""

"pivots",""

"staircase structure",""

"strictly to the right of",""
"algebra","Different types of vectors."
"vectors","Vectors can be surprising objects, including (a) geometric vectors and (b) polynomials."
"uncertainty","Allow us to express some sort of uncertainty, e.g., to quantify the confidence we have about the value of the prediction at a particular test data point."
"probability theory","Quantification of uncertainty is the realm of probability theory."
"parameters","To train machine learning models, we typically find parameters that maximize some performance measure."
"optimization techniques","Many optimization techniques require the concept of a gradient."
"gradient","Tells us the direction in which to search for a solution."
"predictor","a system that makes predictions based on input data"
"training","the adaptation of internal parameters of the predictor to perform well on future unseen input data"
"data as vectors","a representation of data in the form of vectors"
"model","a mathematical representation of a system used for predictions"
"algorithm","a system used for making predictions or adapting internal parameters in machine learning" 
"machine learning","a branch of artificial intelligence that focuses on the development of algorithms that enable computers to learn from and make predictions based on data"
"Machine learning","The latest in a long line of attempts to distill human knowledge and reasoning into a form suitable for constructing machines and engineering automated systems."
"Algorithms","A set of design decisions and methodologies that dictate how machine learning processes data."
"Programming languages","Languages used to write software or scripts for data analysis and automation."
"Data analysis tools","Software tools that assist in examining and interpreting data."
"Large-scale computation","The process of using at least one computer system to solve large data problems effectively."
"Mathematics","The abstract science of number, quantity, and space, used as a foundation in machine learning."
"Statistics","The branch of mathematics dealing with data collection, analysis, interpretation, presentation, and organization."
"optimal production plan","a plan of how many units x; of product N; should be produced if a total of b; units of resource R; are available and (ideally) no resources are left over."
"system of linear equations","a collection of one or more linear equations involving the same variables."
"solution","an n-tuple (21,...,2n) € R” that satisfies the system of linear equations."
"linear equations","equations that graph as straight lines and have the form ax + by + c = 0." 
"n-tuple","an ordered list of n elements, often used to refer to solutions in vector space R.”
"data","Noisy observations of some true underlying signal."
"machine learning","A method to identify the signal from the noise."
"noise","An unwanted or irrelevant signal that obscures the true signal."
"predictors","Variables used for forecasting outcomes."
"performance measure","A metric used to assess how well a model performs."
"training data","The dataset used to train a machine learning model."
"generalize","The ability of a model to perform well on unseen data."
"unseen data","Observations that were not part of the training dataset."
"Vectors","Sent data as vectors."
"Probabilistic","Using the probabilistic view to model data."
"Optimization","Using the optimization view to model data."
"Numerical optimization methods","Methods aimed at improving model performance on unseen data."
"Machine learning","Field of study focused on algorithms and statistical models."
"Bottom-up","Strategy of understanding concepts from foundational to advanced."
"Top-down","Strategy of understanding concepts from practical needs to basic requirements."
"dot product","the product of the corresponding row and column of matrices A and B, summing them up."
"matrices","mathematical structures consisting of rows and columns used for representing data or relationships."
"dimension","a measurable extent of some kind, such as width, height, or length, required for matrix multiplication to be possible."
"matrix multiplication","the process of multiplying two matrices by taking the dot product of rows and columns."
"element-wise operation","an operation that is performed independently on each corresponding element of the matrices."
"Hadamard product","element-wise multiplication of two matrices."
"polynomial","a mathematical expression involving a sum of powers in one or more variables multiplied by coefficients."
"scalar","a real number that can multiply a vector."
"vector","an object that has magnitude and direction, it can also represent an abstract quantity like polynomials or audio signals."
"audio signal","a representation of sound waves as a series of numbers that can be added or scaled."
"R”","the set of tuples of n real numbers, representing higher-dimensional vectors."
"component-wise","the method of adding or multiplying vectors such that corresponding components are operated on independently."
"principal diagonal","the collection of entries Ajj where i = j."
"symmetric matrix","a matrix that is equal to its transpose."
"square matrix","a matrix with the same number of rows and columns."
"associativity","the property that (AB)C = A(BC)."
"distributivity","the property that A(B + C) = AB + AC."
"matrix multiplication","a binary operation that produces a matrix from two matrices."
"system of linear equations","a collection of one or more linear equations with the same variables."
"R”","Represents a real coordinate space of n dimensions."
"scaled vector" ,"A vector that has been multiplied by a scalar."
"vectors" ,"Elements of R” that can be added or multiplied by scalars."
"linear algebra" ,"A branch of mathematics that studies vector spaces and linear mappings."
"finite-dimensional vector spaces" ,"Vector spaces where the number of dimensions is finite."
"1:1 correspondence" ,"A relationship where each element in one set relates to exactly one element in another set."
"closure" ,"A property related to a set being closed under a specific operation."
"array operations" ,"Operations that are performed on arrays, often supported by programming languages."
"machine learning","A field of study that involves algorithms and methodologies for teaching computers to learn from data."
"algorithms","Step-by-step procedures or formulas for solving problems, often used in computer programming and machine learning."
"methodologies","Structured approaches or techniques used in a particular field of study, such as machine learning."
"mathematics","The abstract science of number, quantity, and space, either as abstract concepts (pure mathematics), or as applied to other disciplines such as physics and engineering (applied mathematics)."
"statistics","The study of collecting, analyzing, interpreting, presenting, and organizing data."
"Linearly Independent Vectors","A set of vectors that have no redundancy; removing any vector from the set results in loss of information."
"Linear Combinations","A combination of vectors where each vector is multiplied by a scalar and summed together."
"Vector Space","A mathematical construct consisting of a set of vectors that can be added together and multiplied by scalars."
"Matrix","A rectangular array of numbers or symbols arranged in rows and columns."
"Testing Linearly Independent","The act of determining whether a set of vectors are linearly independent by checking if a linear combination equals zero."
"Linear algebra","plays an important role in machine learning and general mathematics."
"geometry","the idea expanded in Chapter 3."
"vector calculus","where a principled knowledge of matrix operations is essential."
"projections","for dimensionality reduction with principal component analysis (PCA)."
"linear regression","plays a central role for solving least-squares problems."
"Systems of linear equations","play a central part of linear algebra."
"least-squares problems","many problems can be formulated as systems of linear equations."
"matrix operations","essential for vector calculus."
"concept","a particular idea or theme in mathematics that has importance for understanding"
"foundational concepts","basic mathematical ideas that serve as the basis for further knowledge or application"
"machine learning","a field of study that involves algorithms and statistical models that enable computers to perform tasks without explicit instructions"
"regression","a statistical method for modeling the relationship between a dependent variable and one or more independent variables"
"dimensionality reduction","the process of reducing the number of variables under consideration to simplify a dataset"
"density estimation","a technique used to estimate the probability distribution of a random variable"
"classification","a process in machine learning of identifying the category of new observations based on training data"
"standard basis","of IR"
"matrix","P» that performs the basis change from C to OC’"
"homomorphism","@ : R? —> IR°"
"transformation matrix","A of ® with respect to the ordered bases B and C"
"coordinates","of x in B’ are [2,3]'"
"basis","B"
"basis","C"
"solution space","the set of all possible solutions to a system of equations"
"linear equation","an equation that models a linear relationship between variables"
"plane","a flat, two-dimensional surface in three-dimensional space"
"intersect","to meet or cross at a point or line"
"solution set","a collection of values that satisfy a given system of equations"
"coefficients","numerical factors that multiply the variables in an equation"
"vectors","mathematical objects that have both a magnitude and a direction"
"matrices","rectangular arrays of numbers used to represent a system of equations"
"geometrically interpreted","understood in terms of geometric representations or visualizations"
"intersection of two lines","the point at which two lines meet in a plane"
"Linear Independence","the “506 km Northwest” vector and the “374 km Southwest” vector are linearly independent."
"Vector Space","the geographic coordinate system may be considered a two-dimensional vector space."
"Linear Combination","the third “751 km West” vector (black) is a linear combination of the other two vectors."
"methods","approach analogous to climbing a hill to reach its peak."
"classification","discussed in the context of support vector machines."
"support vector machines","a type of machine learning algorithm used for classification tasks."
"regression","where labels are real-valued."
"inputs","variables used in algorithms that correspond to outputs."
"labels","values corresponding to the inputs in a classification task."
"integers","the type of labels used in classification, as opposed to real-valued labels in regression."
"exercises","tasks provided to reinforce learning in Part I of the book."
"programming tutorials","instructional materials provided in Part II, using jupyter notebooks."
"machine learning algorithms","methods and techniques discussed in the book for analyzing data."
"Vectors","Sets of mathematical entities with both magnitude and direction.",
"Linear Transformations","Mappings between vector spaces that preserve the operations of vector addition and scalar multiplication.",
"Rotation","A transformation that turns a figure around a fixed point by a certain angle.",
"Stretching","A transformation that increases or decreases the distance of points from a certain axis.",
"Reflection","A transformation that flips a figure over a specified line or plane.",
"Transformation Matrices","Matrices that represent linear transformations.",
"Linear Mappings","Maps that maintain the structure of vector spaces under addition and multiplication."
"system of linear equations","A collection of one or more linear equations involving the same variables."
"particular solution","A specific solution to a system of equations."
"special solution","A unique solution that satisfies the conditions of a specific case."
"general solution","A solution that contains all possible solutions of the system."
"elementary transformations","Operations that can be performed on a matrix to alter its form without changing its solution set." 
"scaling","Multiplying a solution vector by a non-zero scalar to produce another solution." 
"0 vector","A vector in which all components are zero."
"columns of the matrix","Vertical arrays of numbers in a matrix that represent coefficients of variables in equations."
"Inverse","the desired inverse is given as its right-hand side"
"Multi-plication","performing the multiplication AA~' and observing that we recover I"
"System of Linear Equations","solving a system of linear equations of the form Aa = b"
"Linear Regression","approach of linear regression, which we discuss in detail in Chapter 9"
"Square Matrix","only possible if A is a square matrix and invertible"
"Invertible","only possible if A is a square matrix and invertible"
"system of linear equations","a collection of two or more linear equations with the same set of variables."
"elementary transformations","transformations that keep the solution set the same but simplify the equation system."
"Exchange of two equations","a transformation that involves swapping two equations (rows in the matrix) representing the system of equations."
"Multiplication of an equation with a constant","a transformation that involves multiplying an equation (row) by a non-zero constant from the set of real numbers."
"Addition of two equations","a transformation that involves adding two equations (rows) in the matrix."
"compact matrix notation","a concise representation of a system of equations in the form Aw = b."
"Systems of Linear Equations","A set of equations where each equation is linear and the solutions are the values of the variables that satisfy all equations simultaneously."
"solution set","The set of all possible solutions to a system of linear equations."
"infinitely many solutions","A condition where there are countless solutions to the system of linear equations."
"Linear regression","A statistical method used to solve a linear equation system, particularly when a direct solution is not possible."
"Geometric Interpretation of Systems of Linear Equations","A visual representation where each linear equation corresponds to a line on a plane, and solutions are found at the intersections of these lines."
"intersection of these lines","The set of points where the lines from the linear equations meet, representing the solutions."
"parallel lines","Lines that never intersect and thus represent a system of linear equations with no solutions."
"Systems of Linear Equations", "A collection of one or more linear equations involving the same set of variables."

"pivot columns", "Columns in a matrix that correspond to the leading 1s in the row-echelon form or reduced row-echelon form."

"particular solution", "A specific solution to a system of linear equations that satisfies all equations in the system."

"reduced row-echelon form", "A matrix form where it is in row-echelon form, every pivot is 1, and the pivot is the only nonzero entry in its column."

"row-echelon form", "A form of a matrix where all nonzero rows are above any rows of all zeros and the leading coefficient of a nonzero row is always to the right of the leading coefficient of the previous row."
"Angle","The angle between two vectors tells us how similar their orientations are."
"Dot product","A method to compute the angle between two vectors using their magnitudes and orientations."
"Inner product","A generalization of the dot product used in various vector spaces."
"Orthogonality","Two vectors are orthogonal if their inner product is zero, meaning they are at right angles to each other."
"Arccos","The inverse function of cosine, used to determine the angle from its cosine value."
"linear independence","{a1,...,@,,} are linearly independent if and only if the column vectors {A1,..., Am} are linearly independent."
"linear combinations","m linear combinations of k vectors 2, ..., Lj are linearly dependent if m > k."
"linearly dependent","m linear combinations of k vectors 2, ..., Lj are linearly dependent if m > k."
"vector space","Let V = (V,+,-) be a vector space and B C V,B # ()"
"basis","B is a basis of V."
"minimal generating set","B is a minimal generating set."
"maximal linearly independent set","B is a maximal linearly independent set of vectors."
"sets of vectors","A that possess the property that any vector v € V can be obtained by a linear combination of vectors in A."
"linear combination","A combination of vectors formed by multiplying each vector by a scalar and adding the results."
"generating set","A set of vectors A and the property that every vector v € V can be expressed as a linear combination of those vectors."
"span","The set of all linear combinations of vectors in A."
"vector space","A collection of vectors that can be added together and multiplied by scalars."
"basis","The smallest generating set that spans a vector (sub)space."
"matrix","A real-valued (m,n) matrix A is an m-n-tuple of elements a;;,i = 1,...,m, j =1,...,n."
"row","A horizontal line of elements in a matrix."
"column","A vertical line of elements in a matrix."
"row vector","A matrix with a single row."
"column vector","A matrix with a single column."
"linear functions","Represent linear mappings, as indicated in the text."
"linear algebra","The branch of mathematics dealing with vector spaces and linear mappings between them."
"operations","Actions that can be performed with matrices, such as addition and multiplication."
"linear mapping","a function that maps vectors from one vector space to another while preserving the operations of vector addition and scalar multiplication."
"transformation matrix","a matrix that represents a linear transformation in a specific basis."
"basis change","the process of changing the set of vectors that span a vector space."
"ordered bases","a specific arrangement of basis vectors in a vector space."
"transformation mapping","the relationship between the original and transformed vector spaces under a linear mapping."
"reflection","a transformation that flips points over a specified line or plane."
"rotation","a transformation that turns a figure around a fixed point."
"stretch","a transformation that expands or compresses points away from or towards a fixed point."
"Row-Echelon Form","A matrix is in row-echelon form if all rows that contain only zeros are at the bottom of the matrix; correspondingly, all rows that contain at least one nonzero element are on top of rows that contain only zeros."
"Pivot","The first nonzero number from the left in a nonzero row, also called the leading coefficient."
"Basic Variables","The variables corresponding to the pivots in the row-echelon form."
"Free Variables","The variables that are not basic variables in the row-echelon form."
"Inverse element","If the inverse exists (A is regular), then A-' is the inverse element of A € R”*”"
"General Linear Group","The set of regular (invertible) matrices A € R”*” is a group with respect to matrix multiplication"
"Matrix multiplication","Matrix multiplication as defined in (2.13)"
"Group","A set that is a group with respect to matrix multiplication"
"Abelian","Since matrix multiplication is not commutative, the group is not Abelian"
"Vector Spaces","Sets that in addition to an inner operation + also contain an outer operation -"
"Scalar multiplication","The multiplication of a vector x € G by a scalar \ €"
"solution" ,"Aw = b represents a mathematical equation where we seek to find a vector x that satisfies the equation."
"iteration" ,"An iterative method involves repeating a process to reach a desired outcome, often expressed in the form att) = Cg") +4."
"residual error" ,"||a‘**+) — a,|| measures the difference between successive approximations to ascertain convergence."
"norm" ,"|| - || is a mathematical function that assigns a length or size to vectors, allowing comparison between them."
"vector spaces" ,"A vector space is a structured space in which vectors live, defined by operations that add vectors and multiply by scalars."
"matrix-vector notation" ,"A compact representation of systems of linear equations using matrices and vectors to simplify analysis."
"group" ,"A group is a set of elements with an operation that satisfies certain axioms such as closure, associativity, identity, and invertibility."
"element-wise multiplication","c = ab with c; = a,b; this “array multiplication” is common to many programming languages but makes mathematically limited sense using the standard rules for matrix multiplication."
"matrix multiplication","By treating vectors as n x 1 matrices, we can use the matrix multiplication as defined in (2.13)."
"outer product","ab’ € R"*” (outer product) indicates a specific type of multiplication for vectors."
"inner product","a'b € R (inner/scalar/dot product) indicates another specific type of multiplication for vectors."
"vector space","V = R",n € N is a vector space with operations defined."
"vector addition","Addition: x+y = (%1,...,"
"Linear Combination","Every v € V of the form v = Σ ai xi with ai € R is a linear combination of the vectors x1,..., xk."
"0-vector","The 0-vector can always be written as the linear combination of k vectors a1,..., ak because 0 = Σ Oai is always true."
"Linear (In)dependence","If there is a non-trivial linear combination such that 0 = Σ ai xi with at least one ai ≠ 0, the vectors a1,..., ak are linearly dependent."
"Trivial Solution","If only the trivial solution exists, i.e., ai = ... = ak = 0, the vectors a1,..., ak are linearly independent."
"Linear Independence","Linear independence is one of the most important concepts in linear algebra."
"matrix multiplication","matrix multiplication is not commutative, i.e., AB # BA"
"identity matrix","In R^n, we define the identity matrix"
"associativity","Associativity refers to the way in which the elements are grouped in operations."
"distributivity","Distributivity relates to how multiplication distributes over addition."
"dimensions","the dimensions of the results can be different."
"inverse","An inverse matrix, when multiplied with the original matrix, yields the identity matrix."
"regular","A regular matrix is one that is invertible."
"invertible","An invertible matrix has an inverse."
"nonsingular","A nonsingular matrix is one that is invertible."
"singular","A singular matrix is one that does not have an inverse."
"noninvertible","A noninvertible matrix does not have an inverse."
"linear combination","Every vector x € V is a linear combination of vectors from B, and every linear combination is unique."
"linearly dependent","Adding any other vector to this set will make it linearly dependent."
"canonical/standard basis","In R3, the canonical/standard basis is e={ [1,0,0], [0,1,0], [0,0,1] }."
"linearly independent","The set A is linearly independent, but not a generating set (and no basis) of IR*."
"generating set","The set A is linearly independent, but not a generating set (and no basis) of IR*."
"basis","Every vector space V possesses a basis B."
"finite-dimensional vector spaces","We only consider finite-dimensional vector spaces V."
"inner product","the inner product (-, -) is uniquely determined through A."
"symmetric","the symmetry of the inner product means that A is symmetric."
"positive definite","the positive definiteness of the inner product implies that Va €V\\{0}:a' Aw >0."
"inner product space","a vector space with inner product."
"vector space","a collection of vectors that can be added together and multiplied by scalars."
"symmetric positive definite","a symmetric matrix A © R”*” that satisfies (3.11)."
"positive semidefinite","if only > holds in (3.11), then A is called symmetric, positive semidefinite."
"linear combination","A way to combine vectors by adding them together and scaling them with scalars."
"linearly dependent","Vectors are linearly dependent if at least one of them can be expressed as a combination of the others."
"linearly independent","Vectors are linearly independent if no vector in the set can be written as a combination of the others."
"basis","A set of vectors that can represent every vector in the vector space through linear combinations."
"closure property","The property that guarantees operations on vectors yield results within the same vector space."
"subspace","A subset of a vector space that is also a vector space under the same operations." 
"homogeneous linear equations","A type of system of equations where all the constant terms are zero." 
"solution space","The set of all possible solutions to a given system of equations."
"nonzero entry","the first nonzero entry per row must be | and all other entries in the corresponding column must be 0."
"standard unit vectors","the columns j;,...,3, with the pivots (marked in bold) are the standard unit vectors €,,...,e, € R*."
"augmented matrix","we extend this matrix to an n x n-matrix A by adding n — k rows."
"diagonal","the diagonal of the augmented matrix A contains either 1 or —1."
"Gaussian Elimination","we use Gaussian elimination to bring it into reduced row-echelon form."
"inverse matrix","To determine the inverse of the matrix A." 
"reduced row-echelon form","use Gaussian elimination to bring it into reduced row-echelon form."
"Inverse","To compute the inverse A~' of A € R”*”, we need to find a matrix X that satisfies AX = I,,."
"Matrix","We can write this down as a set of simultaneous linear equations AX = I,,, where we solve for X = [a,|---|a,,]."
"Augmented matrix","We use the augmented matrix notation for a compact representation of this set of systems of linear equations."
"Reduced row-echelon form","This means that if we bring the augmented equation system into reduced row-echelon form, we can read out the inverse on the right-hand side of the equation system."
"Systems of linear equations","Hence, determining the inverse of a matrix is equivalent to solving systems of linear equations."
"Kernel","Not defined in the provided text."
"Null space","Not defined in the provided text."
"Pivots","Now, we look at the fifth column, which is our second non-pivot column."
"mapping","a function that associates every element of one set with an element of another set."
"homomorphisms","structure-preserving mappings between algebraic structures."
"transformation matrices","matrices that represent linear transformations between vector spaces."
"vector spaces","collections of vectors that can be added together and multiplied by scalars."
"linear mapping","a mapping between two vector spaces that preserves the operations of vector addition and scalar multiplication."
"coordinates","numerical values representing a point in a space, based on a specific basis."
"basis","a set of vectors in a vector space such that every vector in the space can be expressed as a linear combination of the basis vectors."
"composition","the combining of two or more functions to form a new function."
"Reduced Row Echelon Form","It allows us to determine the general solution of a system of linear equations in a straightforward way."
"Gaussian Elimination","An algorithm that performs elementary transformations to bring a system of linear equations into reduced row-echelon form."
"System of Linear Equations","A collection of linear equations that can be solved together."
"Elementary Transformations","Operations used in Gaussian elimination to change a system of equations."
"Non-Pivot Columns","Columns in a matrix that do not contain a leading entry (or pivot)."
"Pivot Columns","Columns in a matrix that contain a leading entry after the matrix has been transformed into row-echelon form."
"Linear Combination","An expression formed from a set of terms by multiplying each term by a constant and adding the results."
"Image and Kernel","For ® : V — W, we define the kernel/null space ker(®) := © (Ow) = {v EV: ®(v) = Ow} and the image/range Im(®) := ®(V) = {w € W|dv EV : Ov) = w}."
"Kernel","The kernel is the set of vectors in v € V that ® maps onto the neutral element Oy € W."
"Image","The image is the set of vectors w € W that can be 'reached' by ® from any vector in V."
"Linear mapping","A mapping ® : V — W, where V, W are vector spaces."
"Subspace","Im(®) C W is a subspace of W, and ker(®) C V is a subspace of V."
"Null space","The null space is never empty."
"orthogonal matrices","special matrices that preserve the length of a vector when transformed."
"dot product","a mathematical operation that takes two equal-length sequences of numbers and returns a single number."
"inner product","a generalization of the dot product that measures the angle between vectors."
"angle","the measure of the rotation needed to align one vector with another."
"preserve","to maintain or keep unchanged."
"transformations","operations that change the position, size, or shape of geometric figures."
"rotations","transformations that turn a figure about a fixed point."
"orthonormal basis","a set of vectors that are all unit vectors and orthogonal to each other."
"angle","computed using the inner product."
"inner product","a mathematical operation that combines two vectors to produce a scalar."
"orthogonal","vectors that are perpendicular to each other."
"orthonormal","matrices that are orthogonal and have unit length."
"orthogonal matrix","A square matrix A € R”*” is orthogonal if its columns are orthonormal."
"distances","the measurement of space between two points."
"cosine","a trigonometric function used to relate the angle and sides of a triangle."
"transpose","the operation of converting a matrix into its transposed form."
"affine subspace","a solution of a linear inhomogeneous equation system Aga = b, where A € R”*”", b € R” and rk(A) = n — k."
"homogeneous equation systems","the solution was a vector subspace, which we can also think of as a special affine space with support point ao = 0."
"affine mappings","mappings between two affine spaces that are closely related to linear mappings."
"linear mappings","mappings between vector spaces where the composition is also a linear mapping."  
"vector spaces","mathematical structures consisting of vectors that can be added together and multiplied by scalars."
"composition of linear mappings","the operation where two linear mappings are combined to form another linear mapping."
"Theorem 2.20","With a basis change in V (B is replaced with B) and W (C is replaced with C), the transformation matrix A of a linear mapping ® : V — W is replaced by an equivalent matrix A g."
"transformation matrix","The matrix that represents a linear mapping between vector spaces with respect to given bases."
"linear mapping","A function that maps vectors from one vector space to another while preserving vector addition and scalar multiplication."
"basis change","A replacement of the basis vectors of a vector space with another set of basis vectors."
"ordered bases","A sequence of basis vectors that defines the structure of vector spaces V and W."
"homomorphism","A structure-preserving map between two algebraic structures, in this context between vector spaces V and W."
"automorphism","A transformation that maps a vector space onto itself while preserving its structure."
"identity mapping","The mapping that sends every element to itself in a vector space."
"inner product","An inner product of two functions u : R — R and v: R — R can be defined as the definite integral."
"integral","Then the sum over individual components of vectors turns into an integral."
"norms","As with our usual inner product, we can define norms and orthogonality by looking at the inner product."
"orthogonality","As with our usual inner product, we can define norms and orthogonality by looking at the inner product."
"orthogonal","If (3.37) evaluates to 0, the functions u and v are orthogonal."
"Hilbert space","leading to the definition of a Hilbert space."
"linear combination","how to linearly combine e, and e2 to obtain 2."
"basis","any basis of IR? defines a valid coordinate system."
"coordinate representation","the same vector x from before may have a different coordinate representation in the (b,, b2) basis."
"standard basis","coordinates of x with respect to the standard basis (e;, e2)"
"geometric vector","a geometric vector x € R? with coordinates [2,3]."
"basis vectors","If we use the basis vectors b; = [1, —1]', bs = [1, 1]"."
"n-dimensional vector space","For an n-dimensional vector space V and an ordered."
"vectors","We represent numerical data as vectors and represent a table of such data as a matrix."
"matrices","We represent a table of such data as a matrix."
"linear algebra","The study of vectors and matrices is called linear algebra."
"similarity","The idea is that vectors that are similar should be predicted to have similar outputs by our machine learning algorithm."
"operations","To formalize the idea of similarity between vectors, we need to introduce operations that take two vectors as input and return a numerical value representing their similarity."
"similarity and distances","The construction of similarity and distances is central to analytic geometry."
"analytic geometry","The construction of similarity and distances is central to analytic geometry."
"matrix decomposition","In Chapter 4, we introduce some fundamental concepts about matrices and matrix decomposition."
"machine learning","Some operations on matrices are extremely useful in machine learning."
"augmented matrix","The augmented matrix [A | b] compactly represents the system of linear equations Aw = b."
"elementary transformations","We use ~» to indicate a transformation of the augmented matrix using elementary transformations."
"Swapping Rows","Swapping Rows | and 3 leads to."
"Row","Row 1, Row 2, Row 3, etc. refers to the specific rows in a matrix."
"linear equations","The augmented matrix represents the system of linear equations."
"d(a,y)","distance function defined as d(a,y) = 0 if and only if r=y."
"symmetry","d(x, y) = d(y, x) for all a, y in V."
"triangle inequality","d(x, z) < d(a,y) + d(y, z) for all a, y, z in V."
"inner products","mathematical operations that produce a scalar from two vectors, with properties resembling metrics."
"metrics","functions that define a distance between elements of a set."
"angle","the geometric interpretation of the relationship between two vectors in a vector space."
"Cauchy-Schwarz inequality","an inequality that provides a fundamental relationship between inner products and magnitudes of vectors."
"Inner Product","(-, -) is an inner product but different from the dot product."
"Dot Product","The standard inner product defined as the sum of the products of corresponding entries."
"Symmetric Matrices","Matrices that are equal to their transpose."
"Positive Definite Matrices","Matrices that satisfy the condition x^T A x > 0 for all non-zero vectors x."
"Matrix Decompositions","The process of breaking down a matrix into simpler components."
"Semi-definite Matrices","Matrices that satisfy the condition x^T A x ≥ 0 for all vectors x."
"Vector Space","A collection of vectors where vector addition and scalar multiplication are defined."
"Linear Combinations","A combination of vectors multiplied by scalars."
"Bilinearity","A property of a function that is linear in each of its arguments separately."
"group","Consider a set G and an operation @:GxG > G defined on G. Then G := (G, ®) is called a group if the following hold:"
"closure","Closure of G under @: Vxz,yEG: cx @yeG"
"associativity","Associativity: Vx,y,zEG:(«#@y)®@z=xL@(y@z)"
"neutral element","Neutral element: Je € GVx EG: x@e=xande@r=a2x"
"inverse element","Inverse element: Vx © Gay €G: x ®y=eandy®: write x~! to denote the inverse element of x."
"Abelian group","An Abelian group is a group where the operation @ is commutative."
"pivot column","a column in a matrix that corresponds to a leading 1 in the reduced row-echelon form."
"homogeneous equation system","a system of linear equations in which all of the constant terms are zero."
"reduced row-echelon form","a form of a matrix where each leading entry of a row is 1 and is the only non-zero entry in its column."
"linear equations","equations that represent straight lines when graphed."
"basis B","a set of vectors in vector space V that is used to define other vectors in V."
"linear mapping","a function between two vector spaces that preserves the operations of vector addition and scalar multiplication."
"Theorem 2.17","a theorem stating that the mapping is an isomorphism."
"transformation matrix","the m x n-matrix A whose elements represent the coordinates of a linear mapping with respect to ordered bases."
"ordered bases","a sequence of basis vectors arranged in a specific order for vector spaces."
"coordinates","the numeric values that describe a vector in the context of an ordered basis."
"vector spaces","mathematical structures formed by a collection of vectors."
"finite-dimensional","referring to vector spaces that have a finite number of basis vectors."
"inner product","A major purpose of inner products is to determine whether vectors are orthogonal to each other."
"dot product","A specific type of inner product, the scalar product/dot product in IR^n."
"bi-linear mapping","A mapping with two arguments that is linear in each argument."
"linear mapping","Rearrangement with respect to addition and multiplication with a scalar."
"vector space","A mathematical structure formed by a collection of vectors."
"linear in the first argument","In a bi-linear mapping, asserts linearity with respect to the first variable."
"linear in the second argument","In a bi-linear mapping, asserts linearity with respect to the second variable."
"semidefinite","ve semidefinite."
"Symmetric, Positive Definite Matrices","A is positive definite because it is symmetric."
"positive definite","If A € R”*” is symmetric, positive definite."
"inner product","(x,y) = @' Ay defines an inner product."
"ordered basis","with respect to an ordered basis B."
"Theorem","Theorem 3.5. For a real-valued, finite-dimensional vector space V."
"vector space","for a real-valued, finite-dimensional vector space V."
"properties","The following properties hold if A € R”*” is symmetric and positive definite."
"null space","The null space (kernel) of A."
"orthogonal projections","For a given lower-dimensional subspace, orthogonal projections of high-dimensional data retain as much information as possible and minimize the difference/error between the original data and the corresponding projection."
"projection","A linear mapping 7 : V — U is called a projection if wW=TonT=T."
"linear mapping","Since linear mappings can be expressed by transformation matrices."
"transformation matrices","Since linear mappings can be expressed by transformation matrices."
"projection matrices","The projection matrices P,,, which exhibit the property that P = P,."
"inner product space","We will derive orthogonal projections of vectors in the inner product space (IR”, (-,-)) onto subspaces."
"Ap","Ap: B+ C, Ap: B+ C,S:B > BT: C > C and T ':C + C"
"T'AS","Ay =T'AS."
"transformation matrix","Consider a linear mapping © : IR*® — R* whose transformation matrix is"
"B","B=(j0},]1},]0}), C=( ; ; ; )."
"C","C=( ; ; ; )."
"standard bases","with respect to the standard bases"
"transformation matrix As","We seek the transformation matrix As of ® with respect to the new bases"
"coordinate representation","where the ith column of S is the coordinate representation of b; in terms of the basis vectors of B."
"linear mapping","Consider a linear mapping © : IR*® — R*"
"basis vectors","in terms of the basis vectors of B."
"linear","we would need to solve a linear"
"Gaussian elimination","Find all solutions of the inhomogeneous equation system Aw = b using this method."
"inhomogeneous equation system","An equation system where the equations are not equal to zero."
"Abelian group","A group where the operation is commutative."
"congruence class","The set of integers equivalent to a specified integer under modulo n."
"modulo n","An arithmetic operation that finds the remainder when dividing by n."
"Euclidean division","A division approach that results in a quotient and a remainder."
"finite set","A set that contains a limited number of elements."
"probability space","allows us to quantify the idea of a probability."
"random variables","transfers the probability to a more convenient (often numerical) space."
"distribution","law associated with a random variable."
"modern probability","based on a set of axioms proposed by Kolmogorov."
"Rotations", "A geometric transformation that turns a figure about a fixed point."
"Matrix decomposition", "The process of breaking a matrix into a product of matrices to simplify operations."
"Norms", "A function that assigns a length to each vector in a vector space." 
"geometric vectors", "Directed line segments that have both magnitude and direction."
"vector space", "A mathematical structure formed by a collection of vectors that can be added together and multiplied by scalars."
"length of a vector", "The distance from the origin to the end of a directed line segment representing the vector."
"function", "A relation that assigns each element in a set to exactly one element in another set."
"distance", "A numerical measurement of the space between two points."
"reduced row-echelon form","A matrix form where each leading entry of a row is 1, all leading 1's are in a column to the right of leading 1's in the previous row, and all entries in the column above and below a leading 1 are zeros."
"Gaussian elimination","A method for solving systems of linear equations by transforming the matrix into row-echelon form using elementary row operations."
"homogeneous equation","An equation set to zero, specifically in the form Ax = 0 where A is a matrix and x is a vector."
"basis","A set of linearly independent vectors that span a vector space."
"solution space","The set of all possible solutions to a given linear equation or system of equations."
"kernel","The set of all vectors that map to the zero vector under a given linear transformation, often represented as Ax = 0."
"null space","The set of all solutions to the homogeneous equation Ax = 0, representing vectors that are mapped to the zero vector by the matrix A."
"REF","Abbreviation for 'row-echelon form', a specific structure of a matrix used in solving linear equations."
"equations","mathematical statements that assert the equality of two expressions."
"basis","a set of vectors in a vector space that can be combined to form any vector in that space."
"rank","the number of linearly independent columns of a matrix A."
"linearly independent","a set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others."
"matrix","a rectangular array of numbers or expressions arranged in rows and columns."
"Gaussian elimination","a method for solving systems of linear equations by transforming the matrix into a row echelon form."
"subspace","a vector space that is contained within another vector space."
"dimensionality (dim)","the number of vectors in a basis for the vector space."
"image (or range)","the set of all output values or results of a function."
"regular (invertible)","a matrix is regular if it has an inverse, which occurs if its rank equals its number of columns."
"linear equation system","a collection of one or more linear equations involving the same set of variables."
"system of linear equations","Generally, a system of linear equations can be compactly represented in their matrix form as Ax = 6;"
"matrix form","a compact way of formulating systems of linear equations so that we can write Ax = b"
"linear combination","the product Az is a (linear) combination of the columns of A"
"matrices","we saw that matrices can be used as a compact way of formulating systems of linear equations"
"addition and multiplication of matrices","we defined basic matrix operations, such as addition and multiplication of matrices"
"a(a, 3)","N/A"
"Beta distribution","A probability distribution defined by two parameters, in this case, a and 3."
"GMM","Gaussian mixture model"
"ie.","Id est (Latin: this means)"
"iid.","Independent, identically distributed"
"MAP","Maximum a posteriori"
"MLE","Maximum likelihood estimation/estimator"
"ONB","Orthonormal basis"
"PCA","Principal component analysis"
"PPCA","Probabilistic principal component analysis"
"REF","Row-echelon form"
"SPD","Symmetric, positive definite"
"SVM","Support vector machine"
"a,b,c,a, B,y"," Scalars are lowercase"
"L,Y,z","Vectors are bold lowercase"
"A,B,C","Matrices are bold uppercase"
"a’, A’","Transpose of a vector or matrix"
"A!","Inverse of a matrix"
"(x,y)","Inner product of x and y"
"aly","Dot product of x and y"
"B= (b,,b2,b3)","(Ordered) tuple"
"B = [b,,b,,b3]","Matrix of column vectors stacked horizontally"
"B = {b,,by,b3}","Set of vector"
"pivot column","A column that corresponds to a leading 1 in the reduced row echelon form of a matrix."
"non-pivot column","A column that does not correspond to a leading 1 and often has a value that can be expressed in terms of pivot columns."
"linearly independent","A set of vectors is linearly independent if the only solution to the linear combination equating them to zero is the trivial solution, where all coefficients are zero."
"linearly dependent","A set of vectors is linearly dependent if at least one of the vectors can be expressed as a linear combination of the others."
"elementary row operations","Operations performed on rows of a matrix that include row swapping, scaling rows, and adding multiples of one row to another, used to achieve row echelon form."
"trivial solution","The solution to a homogeneous equation where all coefficients are equal to zero."
"minimal","if there exists no smaller set AC ACY that spans V."
"basis","Every linearly independent generating set of V is minimal and is called a basis of V."
"dimension","The dimension of a vector space corresponds to the number of its basis vectors."
"dim(V)","if and only if U = V."
"independent directions","the dimension of a vector space can be thought of as the number of independent directions in this vector space."
"basis of a vector space","The dimension of a vector space is not necessarily the number of elements in a vector."
"one-dimensional","the vector space V = span{ 1 ] is one-dimensional, although the basis vector possesses two elements."
"subspace","A basis of a subspace U = span|a,...,2@m] C IR” can be found..."
"spanning vectors","Write the spanning vectors as columns of a matrix A."
"row-echelon form","Determine the row-echelon form of A."
"pivot columns","The spanning vectors associated with the pivot columns are a basis of U."
"Existence of the Inverse of a 2 x 2-matrix","A condition where a matrix A has an inverse, such that the product of A and its inverse yields the identity matrix."
"2 x 2-matrix","A matrix consisting of 2 rows and 2 columns."
"Inverse Matrix","Two matrices A and B are called inverse to each other if their product AB equals the identity matrix I."
"Transpose","For matrix A, the matrix B with elements bi; = aj; is called the transpose of A."
"Determinant","A scalar value that can be computed from the elements of a square matrix, used to determine if the matrix is invertible."
"Invertible","A matrix is invertible if it has an inverse, which is a matrix that, when multiplied with the original matrix, results in the identity matrix."
"Identity matrix","A square matrix in which all the elements of the principal diagonal are ones and all other elements are zeros."
"AA'=I","The property that the product of a matrix and its transpose equals the identity matrix."
"A'","The notation used for the transpose of the matrix A."
"Properties of inverses and transposes","Important relationships that define how inverses and transposes of matrices interact."
"vector space","A mathematical structure formed by a collection of vectors that can be added together and multiplied by scalars."
"subspace","A subset of a vector space that is itself a vector space under the same operations."
"Abelian group properties","Properties of a group where the group operation is commutative."
"distributivity","A property that relates addition and multiplication, stating that a(b + c) = ab + ac."
"associativity","A property that states that the way in which numbers are grouped in addition or multiplication does not change their result."
"neutral element","An element in a set which, when combined with any other element, results in that same element, often referred to as the identity element."
"closure","A property indicating that performing a specific operation on elements from a set results in an element that is also within that set."
"trivial subspaces","The simplest subspaces of a vector space, which are the vector space itself and the zero vector space."
"homogeneous system of linear equations","A system of equations in which all of the constant terms are zero."
"subspaces","A set that is a space over a field and also a subset of a larger vector space."
"dimension","The number of vectors in a basis of a vector space, representing its size."
"solution space","The set of all possible solutions to a given system of equations."
"homogeneous equation system","An equation or set of equations where the constant term is zero."
"spanned by","A set of vectors spans a vector space if linear combinations of those vectors fill that space."
"basis","A set of linearly independent vectors that span a vector space."
"N","The intersection of two subspaces."
"FM G","The intersection of two sets, typically representing shared elements."
"linear mappings","Functions between vector spaces that preserve vector addition and scalar multiplication."
"equation","system to find the A; such that"
"kernel","a vector subspace associated with a linear mapping"
"null space","the set of vectors that are mapped to the zero vector by a linear transformation"
"image","the set of all output vectors produced by a linear mapping"
"range","the set of output values from a function or mapping"
"domain","the set of input values for a given function or mapping"
"codomain","the set of all potential output values of a function or mapping"
"basis change","a transformation of the basis vectors in a vector space"
"transformation matrix","a matrix that represents a linear transformation from one vector space to another"
"endomorphism","a linear mapping from a vector space to itself"
"vector subspaces","subsets of a vector space that themselves form a vector space under the same operations"
"compression loss","the amount of data lost when compressing information"
"Affine Subspaces","A subset defined as L=a + U = {a + u: u ∈ U}, where U is a subspace of vector space V and a is a support point."
"Vector Space","A mathematical structure formed by a set of vectors that can be added together and multiplied by scalars."
"Subspace","A subset of a vector space that is also a vector space under the same operations."
"Direction Space","The subspace U in the definition of an affine subspace, representing the directions in which the affine subspace can extend."
"Support Point","The point a in the definition of an affine subspace, where the subspace is anchored."
"Hyperplane","An affine subspace referred to in a later chapter, typically a flat affine subspace of one dimension less than its ambient space."
"Dimension","The number of independent directions or parameters in a space, indicated by k in the k-dimensional affine space L = a + U."
"machine learning","a direct motivation for people to learn mathematics."
"mathematical concepts","foundations of modern machine learning."
"fundamental machine learning problems","context where mathematical concepts are useful." 
"basic concepts","equipped knowledge for understanding machine learning."
"mathematical background","provides a brief but precisely stated glimpse of machine learning."
"methods and models","focus of other books on machine learning."
"Symmetric Matrix","A matrix A € R^n*n is symmetric if A=A'."
"n,n-matrices","Only (n,n)-matrices can be symmetric; they are also called square matrices."
"Invertible Matrix","If A is invertible, then so is A‘."
"Transpose","(A*)T=(AT) t=: ATT."
"Sum of Symmetric Matrices","The sum of symmetric matrices A,B € R^n*n is always symmetric."
"Product of Symmetric Matrices","The product of symmetric matrices is generally not symmetric."
"Scalar Multiplication","Let us look at what happens to matrices when they are multiplied by a scalar λ € R."
"Associativity","For A € R^m*n and C € R^m, (AY)C=AWC)."
"vector","Different coordinate representations of a vector «, depending on the choice of basis."
"basis" ,"The set of vectors in a linear space that defines a coordinate system."
"transformation matrix","we obtain a diagonal transformation matrix"
"diagonal transformation matrix","a transformation matrix that has non-zero elements only on its diagonal."
"mapping","mappings that transform coordinate vectors with respect to one basis into coordinate vectors with respect to a different basis."
"Theorem 2.20 (Basis Change)","For a linear mapping ® : V — W, ordered bases B=(b,,...,b,), B= (bi,...,b,) of V and CH= (C1,---4€m), C= (@1,---5 Gm) of W, and a transformation matrix A» of ® with respect to B and C."
"transformation matrix A»","the corresponding transformation matrix with respect to the bases B and C."
"R"","S € R"*" is the transformation matr."
"Absolutely homogeneous","||Ax|| = |A\|||2]|
"Triangle inequality","||a + y|| < ||x|| + |ly|l"
"Positive definite","||a|| > 0 and ||a|| =0 —+ «=0"
"Triangle inequality","the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side"
"vector space","a general vector space V"
"finite-dimensional vector space","we will only consider a finite-dimensional vector space R”"
"Manhattan norm","defined for 2 € R” as lel: = So laid, (3.3)"
"absolute value","| - | is the absolute value"
"norm","The Manhattan norm is also called ¢; norm"
"mixture models","Iterative scheme to find the parameters of a model, used for density estimation without labeled data."
"dimensionality reduction","The process of reducing the number of variables or dimensions in a dataset, often without losing important information."
"density model","A mathematical representation that describes the distribution of data points in a dataset."
"probability theory","The branch of mathematics that deals with the analysis of random phenomena and the quantification of uncertainty."
"vector calculus","A branch of mathematics concerning differentiation and integration of vector fields, typically involving multivariable functions."
"optimization","The process of making a system, design, or decision as effective or functional as possible, often involving the minimization or maximization of a function."
"linear regression","A statistical method used to model the relationship between a dependent variable and one or more independent variables using a linear equation."
"density estimation","The construction of an estimate of the function that describes the probability distribution of a dataset."
"vector subspace","spanned by the vectors"
"basis","a set of vectors that can linearly combine to form a vector space"
"linearly independent","vectors are linearly independent if no vector in the set can be written as a linear combination of the others"
"homogeneous system of equations","a system of equations where all constant terms are zero"
"matrix","a rectangular array of numbers arranged in rows and columns"
"row-echelon form","a form of a matrix where all non-zero rows are above any rows of all zeros, and leading coefficients are to the right"
"pivot columns","columns in a matrix that contain the leading coefficients in row-echelon form"
"rank","the dimension of the vector space spanned by the rows or columns of a matrix"
"transformation matrix","the matrix that represents a linear transformation with respect to a specific basis."
"ker(®)","the kernel of a linear transformation, consisting of all vectors that are mapped to the zero vector."
"Im()","the image of a linear transformation, which is the set of all vectors that can be obtained by applying the transformation to any vector in the domain."
"basis change","the process of transforming a matrix representation of a vector space from one basis to another."
"ordered bases","a set of vectors that form a basis and are listed in a specific sequence."
"determinants","a scalar value that can be computed from the elements of a square matrix, used to determine if a set of vectors is linearly independent." 
"basis of R2","a set of vectors in R² that are linearly independent and span the space." 
"matrix P","the matrix that performs a change of basis from one set of vectors to another."
"random variable","X, which maps states to outcomes, we see in the right-hand side of (6.8) that this is the probability of the set of states (in .) that have the property."
"probability distribution","We say that a random variable X is distributed according to a particular probability distribution Py, which defines the probability mapping between the event and the probability of the outcome of the random variable."
"discrete random variable","When 7 is finite or countably infinite, this is called a discrete random variable."
"continuous random variable","For continuous random variables (Section 6.2.2), we only consider J = R or T = R.”"
"ordered","according to a rectangular scheme consisting of m rows and n columns"
"matrix","a rectangular array of numbers or symbols arranged in rows and columns"
"row/column vectors","(1, 7)-matrices are called rows and (m, 1)-matrices are called columns"
"R™*","the set of all real-valued (m,n)-matrices"
"Matrix Addition","the sum of two matrices A € R’”*", B € R™*” is defined as the element-wise sum"
"element-wise sum","the sum of two matrices defined as GQmit+ bmi ++ Amn + bmn"
"Matrix Multiplication","elements ci; of the product C= AB e€R™** are computed as ci; = Y— aabr;, i=1,...,.m, jg=l,...,k"
"backpropagation","a special case of a general technique in numerical analysis called automatic differentiation."
"automatic differentiation","a set of techniques to numerically evaluate the exact gradient of a function by working with intermediate variables and applying the chain rule."
"chain rule","a mathematical method to compute the derivative of a composite function."
"elementary arithmetic operations","basic mathematical operations such as addition and multiplication."
"elementary functions","basic mathematical functions such as sin, cos, exp, and log."
"machine precision","the level of accuracy with which a computer can represent numbers."
"n-dimensional vector space","a vector space that has n dimensions."
"basis vectors","vectors that are linearly independent and form a basis for a vector space."
"linearly independent","a set of vectors where no vector can be expressed as a linear combination of the others."
"inner products","a mathematical operation that calculates the product of two vectors, providing a measure of their length and angle."
"orthogonal","vectors that are perpendicular to each other, with an inner product of zero."
"orthonormal basis","a basis consisting of orthogonal vectors, each having a length of 1."
"orthogonal basis","a basis where the vectors are orthogonal to each other, allowing for non-unit length."
"length/norm","the magnitude of a vector, often calculated using inner products."
"complex numbers","the set of numbers that include all real numbers and imaginary numbers."
"Theorem 2.17","Finite-dimensional vector spaces V and W are isomorphic if and only if dim(V) = dim(W)."
"isomorphic","a term used to describe two structures that can be mapped onto each other in a one-to-one correspondence."
"dim(V)","the dimension of vector space V."
"linear mapping","a function between two vector spaces that preserves the operations of vector addition and scalar multiplication."
"bijective mapping","a one-to-one and onto mapping that pairs elements of one set with elements of another set."
"R™*”","the vector space of m x n-matrices."
"R””","the vector space of vectors of length mn."
"linear mappings","functions that comply with the principles of linearity."
"isomorphism","a structure-preserving mapping between two structures that demonstrates the two structures are essentially the same."
"V > W","a notation indicating a mapping from vector space V to vector space W." 
"V > X","a notation indicating a mapping from vector space V to vector space X."
"R","Set of real numbers."
"integrable functions","Functions for which the integral is defined on the interval [a, b]."
"C*","Set of k times continuously differentiable functions."
"C°","Set of continuous functions."
"cos(x)","Cosine function of x."
"sin(x)","Sine function of x."
"linear mapping","A function between two vector spaces that preserves the operations of vector addition and scalar multiplication."
"transformation matrix","Matrix that represents a linear mapping."
"rk(A)","Rank of matrix A, the dimension of the image of the linear mapping."
"kernel","Set of vectors that are mapped to the zero vector by the linear transformation."
"image","Set of all possible outputs of a linear transformation."
"dim(ker(f))","Dimension of the kernel of the function f."
"dim(Im(f))","Dimension of the image of the function f."
"automorphisms","Isomorphisms from a mathematical object to itself."
"identity mapping","Mapping that leaves objects unchanged."
"endomorphism","A morphism in a category which maps an object to itself."
"dimensional","referring to the number of dimensions in a feature space."
"feature space","a representation of data points in a high-dimensional space."
"machine learning algorithms","algorithms that enable computers to learn from data and make predictions."
"principal component analysis (PCA)","a method for reducing the dimensionality of data while preserving as much variance as possible."
"orthogonal projection","a method of projecting data points onto a lower-dimensional subspace."
"projection matrix","a matrix used to transform data points into a different dimensional space."
"linear dimensionality reduction","the process of reducing the number of features in a dataset while retaining essential information."
"linear regression","a statistical method for modeling the relationship between a dependent variable and one or more independent variables."
"Abelian","A group that is commutative, meaning the operation yields the same result regardless of the order of the elements."
"operation @","A defined operation for all a and b in Zn, described by the formula G@@b=axb."
"Z5\{0}","The set of integers modulo 5 excluding the element 0."
"neutral element","An element in a set under a given operation that, when combined with any element in the set, results in that same element."
"inverse","An element that, when combined with a given element under a defined operation, results in the neutral element."
"Bézout theorem","States that two integers a and b are relatively prime if and only if there exist integers w and v such that au + bv = 1."
"relatively prime","Two integers a and b whose greatest common divisor (gcd) is 1."
"group","A set combined with an operation that satisfies closure, associativity, the existence of an identity element, and the existence of inverses."
"prime","An integer greater than 1 that has no positive divisors other than 1 and itself."
"matrix multiplication","A method of multiplying two matrices by taking the dot product of rows and columns."
"inner operation","a form of addition"
"outer operation","a form of scaling"
"inner/outer products","operations that have nothing to do with inner/outer operations"
"real-valued vector space","a set V with two operations (+, -)"
"Abelian group","(V, +) is an Abelian group"
"Distributivity","LVAER a, y ∈ V: a - (a + y) = a - a + y"
"Associativity","VA, y ∈ R, a ∈ V: a - (y - x) = (a y) - a"
"Neutral element","Vz ∈ V: 1 - rx = a @"
"zero vector","the neutral element of (V, +) is the zero vector 0 = [0,...,0T]"
"vector addition","the inner operation + is called vector addition"
"scalars","the elements ∈ R are called scalars"
"multiplication by scalars","the outer operation - is a multiplication by scalars"
"scalar product","something different than the discussed operations"
"vector multiplication","ab, a, b ∈ R"
"R™","a notation indicating a vector space over the real numbers."
"m","the height of the matrix."
"rk(A)","the rank of matrix A, defined as the dimension of its image."
"dim(Im(®))","the dimension of the image of the linear transformation denoted by ®."
"kernel/null space","the set of all solutions to the homogeneous system of linear equations Aw = O."
"linear combinations","a sum of scalar multiples of vectors."
"subspace","a subset of a vector space that is also a vector space."
"n","the width of the matrix."
"linear combination of other columns","expressing one column as a sum of multiples of other columns."
"rank-nullity theorem","a theorem relating the rank and nullity of a linear transformation."
"fundamental theorem of linear mappings","a principle that underlies the properties of linear transformations."
"old" basis,"6% = S60 0 Pon o Upp = E560 Ocp Ups. (2.114)"
equivalence,"Two matrices A, A are equivalent if there exist regular matrices S and T, such that A=T''AS."
similarity,"Two matrices A, A are similar if there exists a regular matrix S with A= S'AS."
regular matrices,"Matrices that are invertible or have an inverse."
linear mappings,"Mappings that preserve the operations of vector addition and scalar multiplication."
transformation matrices,"Matrices that represent linear transformations between vector spaces."
basis changes,"Modifications in the set of vectors that span a vector space."
"transformation matrix","the matrix that transforms coordinates from one basis to another."
"linear combination","an expression formed by multiplying each vector by a scalar and adding the results together."
"basis vectors","a set of vectors in a vector space such that any vector in the space can be expressed as a linear combination of these vectors."
"coordinates","a set of values that show an exact position in a mathematical space."
"basis","a set of linearly independent vectors in a vector space that spans the space."
"V","a vector space referred to in the context."
"W","another vector space referred to in the context."
"S","the transformation matrix that maps coordinates with respect to B onto coordinates with respect to B."
"T","the transformation matrix that maps coordinates with respect to C onto coordinates with respect to C."
"subspace","the subspace of solutions for Ax = 0 possesses dimension n — rk(A)."
"kernel","Later, we will call this subspace the kernel or the null space."
"full rank","A matrix A € R™*” has full rank if its rank equals the largest possible rank for a matrix of the same dimensions."
"rank","This means that the rank of a full-rank matrix is the lesser of the number of rows and columns, i.e., rk(A) = min(m,n)."
"rank deficient","A matrix is said to be rank deficient if it does not have full rank."
"linearly independent","A has two linearly independent rows/columns so that rk(A) = 2."
"ordered basis","Two different coordinate systems defined by two sets of basis vectors."
"coordinate representation","A vector x has different coordinate representations depending on which coordinate system is chosen."
"coordinate vector","A vector x has different coordinate representations depending on which coordinate system is chosen."
"Analytic Geometry","Adding geometric interpretation and intuition to concepts like vectors and vector spaces."
"Vectors","Entities that have both magnitude and direction, studied in terms of their lengths and angles."
"Vector Spaces","A mathematical structure formed by a collection of vectors."
"Linear Mappings","Functions that map vectors to vectors while preserving the operations of vector addition and scalar multiplication."
"Geometric Vectors","Vectors that have geometric interpretations, often represented visually."
"Inner Product","A mathematical operation that induces geometry in a vector space and captures notions of similarity."
"Norms","Functions that assign a positive length or size to each vector in a vector space."
"Metrics","Measures that define a distance between elements in a space."
"Support Vector Machine","A supervised learning model used for classification and regression analysis."
"Orthogonal Projections","The process of projecting a vector onto another vector, maintaining right angles."
"Principal Component Analysis","A dimensionality reduction technique used in statistical analysis."
"Maximum Likelihood Estimation","A method for estimating the parameters of a statistical model that maximizes the likelihood function."
"projection","Examples of projections onto one-dimensional subspaces."
"inner product","With a general inner product, we get d= (a,b) if |b) =1."
"one-dimensional subspace","The horizontal axis is a one-dimensional subspace."
"projection matrices","Projection matrices are always symmetric."
"dot product","The last equality holds for the dot product only."
"length","We can also compute the length of zy(a) by means of Definition 3.1."
"coordinate","This also adds the intuition that . is the coordinate of 7;(a) with respect to the basis vector 6."
"dimension","the number of basis vectors of V, and we write dim(V)."
"subspace","if U C V is a subspace of V, then dim(U) < dim(V)."
"basis","a minimal generating set and a maximal linearly independent set of vectors."
"basis vector","a vector that is part of a basis for a vector space."
"generating set","a set of vectors whose span covers the vector space."
"span","the set of all linear combinations of a given set of vectors."
"linearly independent","a set of vectors are linearly independent if no vector can be expressed as a linear combination of the others."
"linearly dependent","a set of vectors are linearly dependent if at least one vector can be expressed as a linear combination of others."
"linear combination","an expression composed of a set of vectors multiplied by scalars and added together."
"pivot column","a column in a matrix that contains a leading entry in reduced row-echelon form."
"reduced row-echelon form","a form of a matrix where each leading entry is 1 and is the only non-zero entry in its column."
"random variable","a function or lookup table that maps the sample space to possible outcomes."
"sample space","the set of all possible outcomes of a random experiment."
"independent","two draws that do not affect each other."
"probability mass function","describes the probability distribution for a discrete random variable."
"experimental outcomes","the results that come from conducting an experiment."
"event","a set of outcomes to which a probability is assigned."
"P(X =2)","the probability that the random variable X equals 2."
"P(X= Y=","not fully defined in the text but implies a probability related to random variable X."
"draw","the action of selecting an item from a set, such as drawing a coin from a bag."
"identity mapping","different coordinate representations of the identity mapping idy."
"coordinate representations","mapping coordinates with respect to (e1, €2) onto coordinates with respect to (b,, by)."
"basis","a basis effectively defines a coordinate system."
"transformation matrix","the transformation matrix with respect to this new basis can have a particularly simple form."
"canonical basis","with respect to the canonical basis in R?."
"Cartesian coordinate system","the Cartesian coordinate system in two dimensions, which is spanned by the canonical basis vectors e,, €2."
"vector representation","a vector x € R? has a representation tha."
"matrix products","The result of multiplying two matrices together, if the dimensions are compatible."
"solutions","Values of variables that satisfy the given equation."
"system","A set of equations or inequalities that are analyzed together."
"inverse","A matrix that, when multiplied by the original matrix, results in the identity matrix."
"subspaces","A set of vectors that satisfies the properties of being closed under vector addition and scalar multiplication."
"linearly independent","A set of vectors that does not express any vector as a linear combination of the others."
"linear combination","An expression formed by adding together scalar multiples of vectors."
"span","The set of all possible linear combinations of a given set of vectors."
"basis","A set of linearly independent vectors that span a vector space."
"diverge","have infinite value."
"real analysis","a branch of mathematics dealing with real numbers and real-valued functions."
"functional analysis","the study of vector spaces and operators acting upon them, particularly in infinite-dimensional spaces."
"Inner Product","a mathematical operation that generalizes the dot product to functions, allowing the measurement of angle and length in a function space."
"odd function","a function f(x) such that f(—x) = —f(x)."
"integral","a fundamental concept in calculus representing the area under a curve."
"orthogonal functions","functions that are perpendicular to each other in the context of an inner product, leading to an inner product of zero."
"collection of functions","a set of functions treated as a whole in a mathematical context."
"subspace","a subset of a vector space that is also a vector space under the same operations."
"Fourier series","a way to represent a function as a sum of sine and cosine functions, capturing its periodic characteristics."
"Taylor Polynomial","The Taylor polynomial of degree n of f at x0 contains the first n + 1 components of the series."
"k-th derivative","Dé f (ao) is the k-th (total) derivative of f with respect to x, evaluated at x."
"k-th order tensor","Both D‘f and 6* are k-th order tensors, i.e., k-dimensional arrays."
"k-fold outer product","The k times k-th order tensor 6” is obtained as a k-fold outer product, denoted by ®, of the vector 6."
"Auto-differentiation","Auto-differentiation in reverse mode requires a parse tree."
"Hessian","A concept related to vector calculus, but the definition was not provided in the text."
"inhomogeneous system of linear equations","The solution of an inhomogeneous system of linear equations Ar = b, b Z O is not a subspace of R."
"subspace","The intersection of arbitrarily many subspaces is a subspace itself."
"vector subspace","Not all subsets of IR? are subspaces."
"linear subspace","Not all subsets of IR? are subspaces."
"closure property","In A and C, the closure property is violated."
"Gaussian elimination","Perform Gaussian elimination until the matrix is in row echelon form."
"row echelon form","Perform Gaussian elimination until the matrix is in row echelon form."
"reduced row-echelon form","The reduced row-echelon form is unnecessary here."
"pivot columns","The pivot columns indicate the vectors, which are linearly independent of the vectors on the left."
"linearly independent","The pivot columns indicate the vectors, which are linearly independent of the vectors on the left."
"non-pivot columns","The non-pivot columns can be expressed as linear combinations of the pivot columns on their left."
"linear combinations","The non-pivot columns can be expressed as linear combinations of the pivot columns on their left."
"transformation matrix","is the matrix associated with a linear mapping with respect to specified bases."
"linear mapping","a function that maps vectors from one vector space to another while preserving vector addition and scalar multiplication."
"automorphism","a linear mapping from a vector space to itself that preserves the structure of the space."
"identity mapping","the function that maps every element to itself, denoted as id."
"injective","a function is injective (or one-to-one) if different inputs map to different outputs."
"ker(®)","the kernel of a linear mapping, which is the set of vectors that map to the zero vector."
"Null Space","the null space of a matrix is the set of vectors that get mapped to the zero vector by the corresponding linear transformation."
"Column Space","the column space of a matrix is the span of its columns, defining the set of vectors that can be formed by linear combinations of the matrix's columns."
"subspace","a subset of a vector space that is itself a vector space under the same operations."
"vector space","A collection of vectors that can be scaled and added together according to certain rules."
"bilinear mapping","A function that takes two vectors and maps them onto a real number."
"symmetric","A property where the order of the arguments does not matter: Q(a, y) = Q(y, x)."
"positive definite","A property indicating that for any non-zero vector Va, the bilinear form Q(a, x^2) > 0, and Q(0, 0) = 0."
"inner product","A positive definite, symmetric bilinear mapping that associates a real number to each pair of vectors."
"inner product space","A vector space equipped with an inner product."
"Euclidean vector space","An inner product space specifically using the dot product."
"inner product vector space","A vector space V equipped with an inner product that induces a norm."
"Cauchy-Schwarz inequality","An inequality that states |(x,y)| ≤ ||x|| ||y|| for any vectors x and y."
"norm","A function that assigns lengths to vectors in a vector space."
"lengths of vectors","The magnitude of a vector computed using the inner product."
"dot product","An operation that takes two equal-length sequences of numbers and returns a single number, often used as an inner product."
"distance","A numerical measurement of the space between two points or vectors."
"metric","A function that defines a distance between elements of a set."
"Abelian group","G = (G,®) is an Abelian group (commutative)."
"group","(Z,+) is a group."
"not a group","(INo, +) is not a group: Although (No, +) possesses a neutral element (0), the inverse elements are missing."
"not a group","(Z,-) is not a group: Although (Z, -) contains a neutral element (1), the inverse elements for any z € Z, z # +1, are missing."
"not a group","(R,-) is not a group since 0 does not possess an inverse element."
"Abelian","(R\{0},-) is Abelian."
"Abelian","(R",+),(Z", +), € IN are Abelian if + is defined componentwise."
"inverse element","(21,--- ,%)~' := (—a,--: ,—a,) is the inverse element."
"neutral element","e = (0,--- ,0) is the neutral element."
"Abelian","(R™*", +), the set of m x n-matrices is Abelian (with componentwise addition as defined in (2.61))."
"matrices","Let us have a closer look at (R"*”, -), i.e., the set of n x n-matrices with matrix multiplication."
"preserves","the structure of the vector space if O(a + y) = O(x) + O(y) and (Aa) = O(a) for all x,y € V and X € R."
"linear mapping","a mapping ® : V — W is called a linear mapping (or vector space homomorphism/linear transformation) if for all x,y in V and all r in R: O(x + y) = rA¥(x) + pO(y)."
"matrix","we can represent linear mappings as matrices."
"vector","a set of vectors can be collected as columns of a matrix."
"injective","a mapping © is called injective if for all a,y in V: O(a) = O(y) implies a = y."
"surjective","a mapping © is called surjective if ®(V) = W."
"bijective","a mapping © is called bijective if it is both injective and surjective."
"Symbol","Typical meaning"
"0","Parameter vector"
"of","Partial derivative of f with respect to x"
"a","Total derivative of f with respect to x"
"V","Gradient"
"£","Lagrangian"
"L","Negative log-likelihood"
"(2)","Binomial coefficient, n choose k"
"V x(a]","Variance of x with respect to the random variable X"
"Ex [a]","Expectation of a with respect to the random variable X"
"Covx,y[x,y]","Covariance between a and y"
"XILY|Z","X is conditionally independent of Y given Z"
"X~p","Random variable X is distributed according to p"
"N (u, x)","Gaussian distribution with mean ps and covariance &"
"Ber(j1)","Bernoulli distribution with parameter"
"Bin(N, 1)","Binomial distribution with parameters N, ju"
"a" is the coordinate vector/coordinate representation of x with respect to the ordered basis B.
"coordinate vector" is the coordinate representation of x with respect to the ordered basis B.
"linear mapping" a function that maps linear combinations of vectors in one vector space to linear combinations in another.
"vector space" a collection of vectors that can be added together and multiplied by scalars.
"homomorphism" a structure-preserving map between two algebraic structures.
"linear" relating to a straight line; in mathematics, it involves proportional relationships.
"transformation" a function that converts one object or function into another.
"injective" a function that maps distinct elements to distinct elements (one-to-one).
"surjective" a function that covers every element of the codomain (onto).
"bijective" a function that is both injective and surjective (one-to-one and onto).
"Gaussian elimination" an algorithm for solving linear equations and finding the rank of a matrix.
"rank" the dimension of the vector space generated by the rows or columns of a matrix.
"homogeneous equation system","a system of equations that has at least one solution where all the constant terms are zero."
"Gaussian elimination","a method for solving linear systems by transforming the matrix into row-echelon form."
"reduced row-echelon form","a form of a matrix where all leading coefficients are 1 and are the only non-zero entries in their columns."
"kernel","the set of all vectors that are mapped to the zero vector by a linear transformation."
"null space","the kernel of a linear mapping, consisting of all vectors that are sent to the zero vector."
"non-pivot columns","columns in a matrix that do not contain leading 1's in their row-echelon form."
"pivot columns","columns that contain leading 1's in their row-echelon form."
"Rank-Nullity Theorem","states that dim(ker(®)) + dim(Im(®)) = dim(V) for a linear mapping."
"dim","denotes the dimension of a vector space, representing the number of vectors in a basis for that space."
"linear mapping","a function between two vector spaces that preserves vector addition and scalar multiplication."
"dim(ker(®))","the dimension of the kernel of the linear mapping ®."
"dim(Im(®))","the dimension of the image of the linear mapping ®."
"dim(V)","the dimension of the vector space V."
"Surjective","If ® is surjective, then every element in W can be “reached” from V using ®."
"Bijective","A bijective ® can be “undone”, i.e., there exists a mapping W : W — V so that VW o ®(x) = a."
"Inverse","This mapping W is then called the inverse of ® and normally denoted by ®~!"
"Linear Mapping","With these definitions, we introduce the following special cases of linear mappings between vector spaces V and W."
"Isomorphism","Isomorphism: ® : V — W linear and bijective"
"Endomorphism","Endomorphism: ® : V — V linear"
"Automorphism","Automorphism: ® : V — V linear and bijective"
"Identity Mapping","We define idy : V + V, a ++ @ as the identity mapping or identity automorphism in V."
"Homomorphism","The mapping ® : R? > C, ®(x) = x; + ize, is a homomorphism."
"ion","as defined in (2.13)."
"Closure","follows directly from the definition of matrix multiplication."
"Associativity","follows directly from the definition of matrix multiplication."
"Neutral element","The identity matrix I,, is the neutral element with respect to matrix multiplication."
"Matrix multiplication","The operation denoted by “.” in (R”*”, -)."
"Moore-Penrose pseudo-inverse","(A'A)~!.A' to determine the solution (2.59) that solves Aw = b."
"Minimum norm least-squares solution","The solution that solves Aw = b."
"Matrix-matrix product","requires many computations."
"Inverse","computing the inverse of A'.A."
"Numerical precision","generally not recommended to compute the inverse or pseudo-inverse."
"Gaussian elimination","plays an important role when computing determinants."
"systems of linear equations","a mathematical model of equations where each equation is linear and consists of variables combined using addition, subtraction, and multiplication by constants."
"algorithm","a step-by-step procedure for calculations or problem-solving, particularly in this context finding the inverse of a matrix."
"inverse of a matrix","a matrix that, when multiplied with the original matrix, yields the identity matrix."
"particular solution","a specific solution to a system of equations that satisfies the given constraints."
"general solution","a solution that encompasses all possible solutions to a system of equations."
"scalars","real numbers that multiply the columns of a matrix in linear equations."
"columns of the matrix","vertical arrangements of numbers in a matrix that represent variables in a system of equations."
"right-hand-side","the constant or result part of an equation, typically found on the right side of the equal sign."
"linear algebra","The branch of mathematics concerning linear equations, linear functions, and their representations through matrices and vector spaces."
"vectors","Mathematical objects that have both a magnitude and direction, often represented as arrows in geometry."
"matrices","Rectangular arrays of numbers, symbols, or expressions arranged in rows and columns, used to represent linear transformations."
"linear independence","A condition where a set of vectors is independent if no vector in the set can be written as a linear combination of the others."
"basis","A set of linearly independent vectors in a vector space that can be combined to express any vector in that space."
"inner product","A generalization of the dot product that defines a concept of angle and length in vector spaces."
"norm","A function that assigns a positive length or size to vectors in a vector space, derived from the inner product."
"angles","Measurements of the space between two intersecting lines or surfaces in geometry."
"lengths","The measurement or extent of a vector from its initial point to its terminal point."
"distances","A numeric value representing the space between two points in geometry."
"orthogonal projections","The process of projecting a vector onto a subspace such that the vector is closest to the subspace."
"machine learning algorithms","Mathematical methods and models that enable computers to perform tasks without being explicitly programmed, often using data."
"linear regression","A statistical method for modeling the relationship between a dependent variable and one or more independent variables using a linear equation."
"principal component analysis","A technique used to emphasize variation and bring out strong patterns in a dataset by transforming it into a new coordinate system."
"linear mappings","If d: V SW, U: V OW are linear, then 6 + W and \®, \€ R, are linear, too."
"n-dimensional vector space","Any n-dimensional vector space is isomorphic to R” (Theorem 2.17)."
"basis","We consider a basis {b;,..., b,,} of an n-dimensional vector space V."
"ordered basis","Therefore, we write (2.89) and call this n-tuple an ordered basis of V."
"unordered basis","B = {b,,...,b,} is an (unordered) basis."
"matrix","B = [b;,...,6,] is a matrix whose columns are the vectors bj,..., Bn."
"coordinates","For any x € V we obtain a unique representation (linear combination) w=aib)+...+anbn (2.90) of x with respect to B."
"linear combination","For any x € V we obtain a unique representation (linear combination) w=aib)+...+anbn (2.90) of x with respect to B."
"chain rule","we can use the chain rule to compute the derivative of the function in a step-by-step fashion."
"derivative","we can use the chain rule to compute the derivative of the function in a step-by-step fashion."
"forward propagation","Equation (5.143) is the forward propagation of a function."
"backpropagation","(5.145) is the backpropagation of the gradient through the computation graph."
"computation graph","a function that can be expressed as a computation graph, where the elementary functions are differentiable."
"automatic differentiation","The automatic differentiation approach above works whenever we have a function that can be expressed as a computation graph."
"gradient","(5.145) is the backpropagation of the gradient through the computation graph."
"neural network training","For neural network training, we backpropagate the error of the prediction with respect to the label."
"parametric equation","This representation is called parametric equation of L with directional vectors b;,..., b;,, and parameters A1,...,Ax- ©"
"affine subspace","One-dimensional affine subspaces are called lines and can be written as y = a + Aa, where U = span{a,| C R” is a one-dimensional subspace of R”."
"direction","This means that a line is defined by a support point a, and a vector a, that defines the direction."
"support point","a line is defined by a support point a, and a vector a, that defines the direction."
"linear manifold",""
"direction space",""
"hyperplane",""
"parameters",""
"line","One-dimensional affine subspaces are called lines and can be written as y = a + Aa."
"plane","Two-dimensional affine subspaces of IR” are called planes."
"hyperplane",""
"solution","is a value or set of values that satisfies an equation or system of equations."
"linear equation system","a collection of one or more linear equations involving the same set of variables."
"system of linear equations","a set of equations where each equation is linear in its parameters."
"redundancy","occurs when an equation in a system does not provide additional information and can be omitted."
"unique solution","a solution that is the only one that satisfies the equation or system of equations."
"free variable","a variable that can take any value in a system of equations, often leading to infinitely many solutions."
"mathematical expression","function f(a) in (5.109)"
"Automatic differentiation","a formalization of Example 5.14"
"input variables","71,...,be the input variables to the function"
"intermediate variables","1441,...,p_1 be the intermediate variables"
"output variable","xp the output variable"
"computation graph","the computation graph can be expressed as follows"
"linearization","Linearization and Multivariate Taylor Series"
"twice (continuously) differentiable function","f(x,y) is a twice (continuously) differentiable function"
"order of differentiation","the order of differentiation does not matter"
"Hessian matrix","the corresponding Hessian matrix"
"Hessian","the Hessian is denoted as View f(x,y)"
"curvature","The Hessian measures the curvature of the function locally around (x, y)"
"n x n matrix","the Hessian is an n x n matrix"
"Algebra","The branch of mathematics dealing with symbols and the rules for manipulating those symbols."
"n x n-matrix","A square matrix with the same number of rows and columns."
"identity matrix","The n x n-matrix containing 1 on the diagonal and 0 everywhere else."
"matrix multiplication","A mathematical operation that produces a new matrix from two matrices."
"matrix addition","A mathematical operation that combines two matrices to produce a new matrix."
"associativity","For matrices A, B, and C, (AB)C = A(BC)."
"distributivity","For matrices A, B, and C, (A + B)C = AC + BC; A(C + D) = AC + AD."
"multiplication with the identity matrix","For any matrix A, I A = A I = A, where I is the identity matrix."
"inverse","For a square matrix A, matrix B is called the inverse of A if AB = I and BA = I."
"regular/invertible/nonsingular","Terms describing a matrix that possesses an inverse."
"singular/noninvertible","Terms describing a matrix that does not possess an inverse."
"unique","When the matrix inverse exists, it is one of a kind."
"dom variable X","takes the value smaller than or equal to."
"cdf","can be expressed also as the integral of the probability density function f(a)."
"pdf","is a nonnegative function that sums to one."
"law","of a random variable X, that is, the association of a random variable X with the pdf f(x)."
"probability density function","pdf."
"cumulative distribution function","cdf."
"measure zero","P(X = 2) is a set of measure zero."
"discrete uniform distributions","examples of cdfs which do not have corresponding pdfs."
"continuous uniform distributions","examples of cdfs which do not have corresponding pdfs."
"uniform distribution","examples of discrete and continuous uniform distributions."
"isomorphism","W — V is an isomorphism, too."
"endomorphism",""
"automorphism",""
"identity mapping",""
"identity",""
"transformation matrix","the transformation matrix can be used to map coordinates with respect to an ordered basis in V to coordinates with respect to an ordered basis in W."
"homomorphism","Consider a homomorphism @ : V — W and ordered bases B = (b,,...,b3) of V and C = (c1,...,c4) of W."
"coordinate vector","if & is the coordinate vector of x € V with respect to B and y the coordinate vector of y = ©(a) € W with respect to C,"
"ordered basis","with respect to an ordered basis in V to coordinates with respect to an ordered basis in W."
"transformation matrix A","the transformation matrix A» with respect to B and C satisfies ®(b,) = an aie; fork =1,...,3 and is given as"
"f : R” — R”" ,"is a vector field, the Hessian is an (m x n x n)-tensor."
"gradient" ,"Vf of a function f is often used for a locally linear approximation of f around a9."
"linear approximation" ,"f(w) © f(@o) + Vef)(®o)(@ — #0)."
"multivariate Taylor series" ,"a special case of a multivariate Taylor series expansion of f at a, where we consider only the first two terms." 
"gradient of f" ,"(V2f)(xo) is the gradient of f with respect to a, evaluated at ap." 
"locally accurate" ,"This approximation is locally accurate, but the farther we move away from xo the worse the approximation gets."
"set of","properties that we are interested in."
"random variable","a function that measures the probability that a particular outcome (or set of outcomes) will occur."
"probability distribution","the function associated with a random variable that measures probabilities of outcomes."
"probabilistic modeling","a building block for other concepts related to probability distributions."
"graphical models","models used in conjunction with probability distributions."
"model selection","a concept that utilizes probability distributions."
"probability space","defined by three concepts: the sample space, the events, and the probability of an event."
"sample space","one of the three concepts that define a probability space."
"events","one of the three concepts that define a probability space."
"probability of an event","one of the three concepts that define a probability space."
"mathematical structure","a framework for defining and analyzing random outcomes."
"operations","Mathematical procedures that are performed on numbers or vectors."
"row vector","A matrix with a single row, typically denoted by variables such as R' or R."
"column vector","A matrix with a single column, typically denoted by variable x."
"transpose","An operation that flips a matrix over its diagonal, converting rows to columns and vice versa."
"vector spaces","A mathematical structure formed by a collection of vectors that can be added together and multiplied by scalars."
"vector subspaces","Sets contained within the original vector space that remain closed under vector space operations."
"dimensionality reduction","A process in machine learning used to reduce the number of variables under consideration by obtaining a set of principal variables." 
"linear subspace","A vector subspace that is itself a vector space with the same operations of addition and scalar multiplication."
"Linear Algebra","A branch of mathematics concerning linear equations, linear functions, and their representations through matrices and vector spaces."
"Systems of Linear Equations","A collection of one or more linear equations involving the same variables."
"Matrices","Rectangular arrays of numbers, symbols, or expressions, arranged in rows and columns."
"Solving Systems of Linear Equations","Finding the values of variables that satisfy all equations in the system."
"Vector Spaces","A collection of vectors, which are objects that can be added together and multiplied by scalars."
"Linear Independence","A property of a set of vectors where no vector can be expressed as a linear combination of the others."
"Basis and Rank","Basis refers to a set of vectors that span a vector space, while rank is the dimension of the vector space spanned by a set of vectors."
"Linear Mappings","Functions that map vectors from one vector space to another while preserving the operations of vector addition and scalar multiplication."
"Affine Spaces","Geometric structures that generalize vector spaces but do not have a natural origin."
"Analytic Geometry","The study of geometry using a coordinate system and the principles of algebra."
"Norms","Functions that assign a positive length or size to vectors in a vector space."
"Inner Products","Generalization of the dot product that allows the measurement of angles and lengths in vector spaces."
"Lengths and Distances","Quantities that measure how far apart two points or vectors are."
"Angles and Orthogonality","Measures of the separation between two intersecting lines or vectors, where orthogonality refers to right angles (90 degrees)."
"Orthonormal Basis","A basis consisting of orthogonal vectors that are all of unit length."
"Orthogonal Complement","A subspace of a vector space that consists of all vectors orthogonal to every vector in a given subspace."
"Inner Product of Functions","A generalization of the inner product concept applied to functions, allowing measurement of similarity."
"Orthogonal Projections","The process of projecting a vector onto a subspace such that the difference is orthogonal to the subspace."
"Rotations","Transformations that turn a shape or vector around a fixed point in space."
"Determinant and Trace","Determinant is a scalar value that can be computed from the elements of a square matrix indicating the volume scaling factor of linear transformations; Trace is the sum of the entries on the main diagonal of a matrix."
"Theorem","A statement that has been proven based on previously established statements such as other theorems, and axioms."
"Random variable","A variable whose possible values are numerical outcomes of a random phenomenon."
"Independence","A condition where two events are unrelated to each other; the occurrence of one does not affect the occurrence of the other."
"Sufficient statistics","Statistics that capture all the information needed to compute any estimate of a parameter."
"Exponential family","A set of probability distributions that can be expressed in a certain form, often related to sufficient statistics."
"Probability","A measure of the likelihood that an event will occur."
"linearly independent","A property of a set of vectors indicating that no vector in the set can be expressed as a linear combination of the others."
"inverse of a matrix","A matrix that, when multiplied by the original matrix, yields the identity matrix."
"rank of a matrix","The dimension of the vector space generated by its rows or columns."
"basis of a vector space","A set of vectors that are linearly independent and span the vector space."
"Gaussian elimination","An algorithm for solving systems of linear equations by transforming the matrix to a simpler form."
"stationary iterative methods","A class of algorithms for solving linear systems by iteratively refining an approximation."
"Richardson method","A stationary iterative method for solving linear equations."
"Jacobi method","An iterative algorithm for solving a diagonal dominant system of linear equations."
"Gauss-Seidel method","An improvement over the Jacobi method for solving linear equations by updating variables as soon as new values are available."
"successive over-relaxation method","An iterative method that accelerates convergence of the Gauss-Seidel method."
"Krylov subspace methods","A group of algorithms for solving linear systems using the Krylov subspace generated by the vectors."
"conjugate gradients","An algorithm for solving systems of linear equations, particularly those that are large and sparse."
"generalized minimal residual","An iterative method for solving linear systems by minimizing the residual."
"biconjugate gradients","An extension of the conjugate gradient method useful for non-symmetric linear systems."
"Eigenvalues and Eigenvectors","A pair of concepts in linear algebra that represent special values associated with a linear transformation."
"Cholesky Decomposition","A method of decomposing a matrix into a lower triangular matrix and its transpose."
"Eigendecomposition and Diagonalization","The process of expressing a matrix in terms of its eigenvectors and eigenvalues."
"Singular Value Decomposition","A factorization of a matrix into three matrices that reveal much about the original matrix's properties."
"Matrix Approximation","A process of finding a simpler matrix that is close to a given matrix."
"Matrix Phylogeny","The study of evolutionary relationships using matrix representations."
"Vector Calculus","A branch of calculus that deals with vector fields and differential operators."
"Differentiation of Univariate Functions","The process of finding the derivative of a single-variable function."
"Partial Differentiation and Gradients","The technique of finding the derivative of a function with respect to one variable while holding others constant."
"Gradients of Vector-Valued Functions","Vectors that represent the rates of change of a vector-valued function concerning its variables."
"Gradients of Matrices","The extension of the concept of gradient to matrix functions."
"Useful Identities for Computing Gradients","Mathematical statements that simplify the process of calculating gradients."
"Backpropagation and Automatic Differentiation","Techniques used in neural networks for computing gradients efficiently."
"Higher-Order Derivatives","Derivatives of derivatives used to understand the behavior of functions more deeply."
"Linearization and Multivariate Taylor Series","Approximations of functions using linear terms and higher-order derivatives."
"Construction of a Probability Space","The formulation of a mathematical model that encapsulates the outcomes of a probabilistic experiment."
"plane","a plane is defined by a support point a, and two linearly independent vectors x,, x, that span the direction space."
"hypermplane","the (n — 1)-dimensional affine subspaces are called hyperplanes."
"parametric equation","the corresponding parametric equation is y = a + an ALi."
"basis","where 21,...,%n—1 form a basis of an (n — 1)-dimensional subspace U of R."
"affine subspace","an affine subspace of R” of dimension n — rk(A)."
"linear equation","the solution of the linear equation sys- tem Ax = 6b is either the empty set or an affine subspace of R.”"
"dimension","the solution of the linear equation Ax1 +... + AnLy = b, is a hyperplane in R”."
"Theorem 2.24","Direct consequences of a theorem related to linear transformations and dimensions of images and kernels."
"dim(Im(®))","Dimension of the image of the transformation ®."
"dim(V)","Dimension of the vector space V."
"ker(®)","Kernel of the transformation ®, representing the set of vectors that map to the zero vector."
"non-trivial","Indicates that the kernel contains more than just the zero vector."
"dim(ker(®))","Dimension of the kernel of the transformation ®."
"transformation matrix","A matrix that represents a linear transformation with respect to an ordered basis."
"system of linear equations","A set of equations that can be solved simultaneously to find the values of variables."
"Aga = 0","A linear equation representing the transformation matrix acting on vector a resulting in the zero vector."
"infinitely many solutions","Indicates that there are an unlimited number of solutions to the system of linear equations."
"injective","A property of a function where it maps distinct elements to distinct images."
"surjective","A property of a function where every element in the codomain has a pre-image in the domain."
"bijective","A property of a function that is both injective and surjective."
"Im(®)","Image of the transformation ®, representing the outputs of the transformation."
"Affine Spaces","Spaces that are offset from the origin and are no longer vector subspaces."
"linear mappings","Functions between vector spaces that preserve the operations of vector addition and scalar multiplication."
"norm","set of vectors with norm 1"
"length","the Euclidean distance of x from the origin"
"absolutely homogeneous","property of the norm"
"triangle inequality","a principle in mathematics stating that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side"
"positive definite","a property of a function where it is always greater than zero"
"Manhattan norm","a norm that measures the distance in a grid-like path"
"Euclidean norm","also called £2 norm, a standard measure of distance in Euclidean space"
"Euclidean distance","the length of the shortest path between two points in Euclidean space"
"scalar product","a product that results in a scalar quantity"
"dot product","another term for the scalar product"
"bilinear mapping","a mapping that is linear in each of its arguments"
"Euclidean norm (3.4)","defined as ||x||2 = √(∑(xi^2)) for vectors in R^n"
"Inner products","allow for the introduction of classical geometric concepts"
"Multiplication by scalars","Aw = A(21,...,%) = (At1,..., AX) for allA € R,a € R”"
"Vector space","V=R"™*",m,n © N is a vector space"
"Addition","A+ B= : : is defined elementwise for all A, Be V"
"Scalar multiplication","AA = : : as defined in Ami ++: AAmn"
"Equivalent spaces","R”*" is equivalent to R””"
"Complex numbers","VY =C, with the standard definition of addition of complex numbers"
"Standard vector addition","We will denote a vector space (V,+,-) by V when + and - are the standard vector addition and scalar multiplication"
"Notation","We will use the notation a € V for vectors in V to simplify notation"
"Distinction between spaces","The vector spaces R”, R"*', R'*\" are only different in the way we write vectors"
"n-tuples as column vectors","this allows us to write n-tuples as column vectors"
"machine learning","The democratization of machine learning by the provision of open-source software, online tutorials and cloud-based tools allows users to not worry about the specifics of pipelines."
"open-source software","Software that is made available to the public for use, modification, and distribution."
"cloud-based tools","Digital tools and software services that are hosted in the cloud and accessible over the internet."
"data","Facts and statistics collected for analysis."
"insights","Understanding gained through the analysis of data."
"domain experts","Individuals with specialized knowledge or skills in a specific area."
"certification","The process of verifying that a machine learning system meets certain standards."
"risk management","The identification, assessment, and prioritization of risks associated with machine learning systems."
"ethics","Moral principles that govern a person's or group's behavior in the context of machine learning."
"fairness","The quality of making judgments that are free from discrimination or bias in machine learning."
"privacy","The right of individuals to keep their personal information secure and confidential in the context of machine learning."
"probability","measures the probability or degree of belief that the event will occur."
"P(A)","the probability of A."
"interval","the probability of a single event must lie in the interval (0, 1]."
"total probability","the total probability over all outcomes in the sample space must be 1, i.e., P(Q) = 1."
"probability space","Given a probability space (Q, A, P), we want to use it to model some real-world phenomenon."
"target space","we refer to T as the target space."
"states","elements of T are referred to as states."
"function","We introduce a function X : 2 — T that takes an element of 2."
"random variable","this association/mapping from 2 to T is called a random variable."
"outcome","takes an element of 2 (an outcome) and returns a particular quantity of interest x."
"screte and Continuous Probabilities","Probabilities distinguished by whether they take on discrete values or continuous ranges."
"Sum Rule","A rule used in probability to compute the total probability of two mutually exclusive events."
"Product Rule","A rule used in probability to find the joint probability of two independent events."
"Bayes’ Theorem","A theorem that describes how to update the probabilities of hypotheses when given evidence."
"Summary Statistics","Statistics that summarize or describe the main features of a dataset."
"Independence","A property where the occurrence of one event does not affect the occurrence of another."
"Gaussian Distribution","A continuous probability distribution characterized by a bell-shaped curve, also known as the normal distribution."
"Conjugacy","A property in Bayesian statistics where prior and posterior distributions belong to the same family."
"Exponential Family","A class of probability distributions that includes the exponential distribution, normal distribution, and more."
"Change of Variables","A method in calculus used to transform integrals into a different variable."
"Inverse Transform","A technique in probability to obtain random variables with a desired distribution by using the inverse of the cumulative distribution function."
"Continuous Optimization","The process of optimizing an objective function that is continuous."
"Optimization Using Gradient Descent","A method to find the minimum of a function by iteratively moving in the direction of the steepest descent."
"Constrained Optimization","Optimization of a function subject to constraints on the variables."
"Lagrange Multipliers","A method to find the local maxima and minima of a function subject to equality constraints."
"Convex Optimization","A subfield of optimization that studies the problem of minimizing convex functions over convex sets."
"Empirical Risk Minimization","A principle in statistical learning theory that aims to minimize the average loss on a given dataset."
"Parameter Estimation","The process of using data to determine the parameters of a statistical model."
"Probabilistic Modeling and Inference","Creating models that incorporate uncertainty and making predictions or decisions based on those models."
"Directed Graphical Models","Probabilistic models that use directed graphs to represent the conditional dependencies between random variables."
"rs","unordered"
"Z","Integers"
"IN","Natural numbers"
"R","Real numbers"
"C","Complex numbers"
"R"","n-dimensional vector space of real numbers"
"Va","Universal quantifier: for all x"
"da","Existential quantifier: there exists x"
"a:=b","ais defined as b"
"a=:b","b is defined as a"
"axb","a is proportional to b, i.e., a = constant - b"
"gof","Function composition: “g after f”"
"=","If and only if"
"=>","Implies"
"A","Sets"
"C","Sets"
"aceA","ais an element of the set A"
")","Empty set"
"D","Number of dimensions; indexed by d = 1,..., D"
"N","Number of data points; indexed by n = 1,..., N"
"In","Identity matrix of size m x m"
"Onn","Matrix of zeros of size m x n"
"LTin.n","Matrix of ones of size m x n"
"e;","Standard/canonical vector (where i is the component that is 1)"
"dim","Dimensionality of vector space"
"tk(A)","Rank of matrix A"
"Im(®)","Image of linear mapping ®"
"ker(®)","Kernel (null space) of a linear mapping ®"
"span[b;]","Span (generating set) of b;"
"tr(A)","Trace of A"
"det (A)","Determinant of A"
"-|","Absolute value or determinant (depending on context)"
"lI-ll","Norm; Euc"
"wpoint","any object that satisfies two properties can be considered a vector"
"Geometric vectors","directed segments that can be drawn in at least two dimensions"
"vector","an object that can be added and multiplied by a scalar"
"scalar","a real number used to scale a vector"
"addition of vectors","the operation of combining two geometric vectors resulting in another geometric vector" 
"polynomials","mathematical expressions that can also be considered as vectors"
"Linear Regression","A statistical method used to model the relationship between a dependent variable and one or more independent variables."
"Parameter Estimation","The process of using data to estimate the parameters of a statistical model."
"Bayesian Linear Regression","A form of linear regression that incorporates prior distributions and Bayesian inference."
"Maximum Likelihood as Orthogonal Projection","A method for estimating parameters by maximizing the likelihood function, which can be interpreted as a projection in a statistical space."
"Dimensionality Reduction","The process of reducing the number of random variables under consideration, by obtaining a set of principal variables."
"Principal Component Analysis","A statistical procedure that transforms correlated variables into a set of uncorrelated variables called principal components."
"Maximum Variance Perspective","An approach in PCA that seeks to find the directions of maximum variance in the data."
"Eigenvector Computation","The process of finding eigenvectors, which are vectors that only scale under a linear transformation."
"Low-Rank Approximations","Approximations of a matrix that reduce its rank, often used in dimensionality reduction."
"Gaussian Mixture Model","A probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions."
"Parameter Learning via Maximum Likelihood","The process of estimating model parameters by maximizing the likelihood function given the data."
"EM Algorithm","An iterative method for finding maximum likelihood estimates in the presence of latent variables."
"Support Vector Machines","A set of supervised learning methods used for classification and regression analysis."
"Separating Hyperplanes","The decision boundary that separates different classes in a Support Vector Machine."
"Primal Support Vector Machine","A form of Support Vector Machine that directly optimized the decision boundary in the original feature space."
"Dual Support Vector Machine","A form of Support Vector Machine that optimizes a dual problem, often leading to better computational efficiency."
"Kernels","Functions used in Support Vector Machines to enable the algorithm to operate in a higher dimensional space without explicitly transforming the data."
"Numerical Solution","A solution to a mathematical problem obtained through numerical methods rather than analytical expressions."
"inner product space","A vector space V equipped with an inner product (-.-)."
"distance","Defined as d(x, y) := ||x — y||."
"Euclidean distance","The distance when using the dot product as the inner product."
"inner products","Functions that induce norms."
"Cauchy-Schwarz inequality","A fundamental inequality relating to inner products."
"metric","A function d: V x V -> R that measures distance."
"positive definite","A property of a metric where d(x, y) is greater than or equal to zero."
"symmetric","A property of a metric where d(x, y) = d(y, x)."
"triangle inequality","A condition of a metric that states d(x, z) ≤ d(x, y) + d(y, z)."
"angle","A geometric concept related to the relationship between vectors."
"0","The value that represents the lowest possible point in a numerical scale."
"x' Ax > 0","A mathematical expression indicating that the quadratic form associated with matrix A is positive for non-zero vector x."
"Aw 4 0","A matrix-vector product indicating that the resulting vector Aw is non-negative if vector a is non-zero."
"diagonal elements","Elements of a matrix that are located on the line from the top left to the bottom right."
"a;; = e} Ae; > 0","An expression that defines the positive nature of the diagonal elements using the standard basis vector."
"norms","Functions that assign a positive length or size to vectors, used for computing vector lengths."
"inner product","An operation that generalizes the dot product and induces a norm on a vector space."
"|x|] = \/ (@, x)","An expression to compute the norm of vector x using its inner product with another vector."
"Manhattan norm","A specific type of norm that calculates the distance between points in a grid-based system, without a corresponding inner product."
"Cauchy-Schwarz Inequality","A fundamental inequality involving inner products that has significant implications in various areas of mathematics."
"Automatic differentiation","A method used in computing derivatives of functions efficiently, often used in machine learning."
"expectations","In probability and statistics, the expected value or mean of a random variable."
"integrals","A fundamental concept in calculus representing the area under a curve or the accumulation of quantities."
"Taylor series expansion","A mathematical series that represents a function as an infinite sum of terms calculated from the values of its derivatives at a single point."
"Gaussian","Referring to a normal distribution characterized by its bell-shaped curve, defined by its mean and variance."
"mean","The average value of a set of numbers, calculated by dividing the sum of the values by the count of values."
"covariance","A measure of the degree to which two random variables change together, indicating the direction of their linear relationship."
"Gaussian distributed","Referring to a random variable that follows a normal distribution."
"extended Kalman filter","An algorithm used for estimating the state of a nonlinear system through the use of linear approximations." 
"nonlinear functions","Functions in which the relationship between the variables is not a straight line, often characterized by curves or complex behaviors."
"state-space models","Mathematical models that describe a system by a set of input, output, and state variables related by first-order differential equations."
"W. "The vector a is called the translation vector of ¢." ,"The vector a that is associated with affine mapping ¢."
"affine mapping" ,"A mapping ¢ : V — W that can be expressed as the composition of a linear mapping and a translation."
"linear mapping" ,"A mapping ® : V > W that preserves the operations of vector addition and scalar multiplication."
"translation" ,"A mapping 7 : W — W that shifts points in the vector space W."
"composition" ,"The operation of combining two functions, such as creating affine mapping by combining linear mapping and translation."
"geometric structure" ,"The arrangement and properties of geometric elements preserved by affine mappings."
"dimension" ,"The number of coordinates needed to specify a point in a space."
"parallelism" ,"The property where two lines or planes do not intersect and maintain the same direction." 
"Gaussian elimination" ,"An algorithm for solving systems of linear equations."
"linear equations" ,"Equations that represent straight lines and can be expressed in the form ax + by = c."
"numerical linear algebra" ,"The branch of mathematics concerned with algorithms for performing linear algebra computations."
"dual SVM","examples whose dual parameters lie strictly inside the box constraints,"
"Karush Kuhn Tucker conditions","conditions derived that relate to the dual support vector machine."
"convex hull","the smallest convex set that contains a set of points."
"support vectors","data points that lie on the margin in a support vector machine."
"hyperplane","this distance r is the margin."
"margin","the distance from the hyperplane."
"positive examples","examples that should be further than r from the hyperplane."
"negative examples","examples that should be further than distance r in the negative direction from the hyperplane."
"inequality","a mathematical statement that combines the requirements that examples are at least r away from the hyperplane."
"parameter vector w","a vector that is of unit length, |w| = 1."
"Euclidean norm","a method of measuring the length of the vector ||w||."
"scaling factor","the factor that relates the distance r to a vector of length 1."
"regularization","term does not contain b."
"unregularized term","b com- wy cae) t also plicates theoretical analysis."
"Soft Margin SVM","Loss Function View."
"empirical risk minimization","principle of empirical risk minimization."
"hinge loss","a convex upper bound of zero-one loss."
"zero-one loss","max{0, 1 — t}."
"loss term","can be interpreted as never allowing any examples inside the margin."
"training set","{(a,,41),...,(@n,yn)}."
"total loss","seek to minimize the total loss."
"/-regularization","regularizing the objective with /-regularization."
"Laplacian Eigenmaps","Dimensionality Reduction and Data Representation."

"Support Vector Machines","A set of supervised learning methods used for classification and regression."

"Duality","A concept that refers to two different perspectives or formulations of a problem in mathematics."

"Geometry","A branch of mathematics concerned with the properties and relations of points, lines, surfaces, and solids."
"regression","observed noisy function values from which we wish to infer the underlying function that generated the data."
"Lagrange multiplier","A technique used to find the local maxima and minima of a function subject to equality constraints."
"Lagrangian","A function that summarizes the dynamics of a system in classical mechanics, often used in optimization."
"primal problem","The original optimization problem that we want to solve."
"Lagrangian dual problem","An associated optimization problem derived from the primal problem that can provide bounds on the solution."
"minimax inequality","A principle in mathematical optimization concerning the minimization of the maximum loss."
"margin","when the margin is large, the “complexity” of the function class is low, and hence learning is possible"
"complexity","the “complexity” of the function class is low, and hence learning is possible"
"generalization error","useful for various different approaches for theoretically analyzing generalization error"
"parameter vector","we choose a scale for the data. We choose this scale such that the value of the predictor (w, a) + b is 1 at the closest example"
"predictor","the value of the predictor (w, a) + b is 1 at the closest example"
"hyper","the example in the dataset that is closest to the hyper"
"margin","the distance from the hyperplane to the closest data points."
"||w||","the norm of the weight vector w, representing its length."
"hyperplane","a flat affine subspace of one dimension less than its ambient space."
"SVM","support vector machine, a supervised learning model for classification."
"max","used to denote the maximization of a function."
"min","used to denote the minimization of a function."
"reciprocal of the norm","the inverse of the norm of a vector, often used in optimization."
"gradient","a vector representing the direction and rate of fastest increase of a scalar function."
"hard margin SVM","a type of support vector machine that requires maximum margin with no misclassifications."
"objective","the function we aim to maximize or minimize."
"Support Vector Machine","SVM is a supervised learning model for classification and regression tasks, particularly known for its effectiveness in high-dimensional spaces."
"Constrained Optimization","A mathematical approach focusing on maximizing or minimizing an objective function subject to constraints."
"Inner Product","A mathematical operation that takes two vectors and returns a scalar, used in various applications including determining angles and distances."
"Margin","The distance between the decision boundary and the closest data points; maximizing the margin can lead to better generalization."
"Linearly Separable Data","Data that can be separated by a straight line (or hyperplane) in its feature space."
"Objective Function","The function that is being optimized in a mathematical problem."
"Normalization","The process of adjusting values in a dataset to a common scale, often to improve performance in machine learning tasks."
"Pattern Recognition","A method in machine learning for identifying patterns in data."
"Variational Inference","A statistical method for approximating complex distributions."
"Numerical Optimization","The process of finding the best solution from all feasible solutions."
"Convex Analysis","A subfield of mathematics that studies convex sets and convex functions."
"Online Algorithms","Algorithms that process data in a serial fashion, providing output without access to future data."
"Stochastic Approximations","A method for finding solutions to problems that involve randomness."
"Gradient Method","A method for optimizing multi-stage allocation processes."
"Convex Optimization","A field of optimization dealing with convex functions and sets."
"Dimension Reduction","A process of reducing the number of random variables under consideration."
"High-Dimensional Data","Data that has a large number of features or dimensions."
"N-Way Generalization","A generalization technique in multidimensional scaling."
"Statistical Inference","The process of drawing conclusions about population parameters based on sample statistics."
"projection matrix","a matrix that is used to project data points onto a lower-dimensional space."
"trace operator","an operator that is linear and invariant to cyclic permutations of its arguments."
"data covariance matrix","a matrix that represents the covariance between different dimensions of the dataset."
"rank-one matrices","matrices that can be expressed as the outer product of two vectors."
"average squared reconstruction error","a metric that measures the error between the original data and its reconstructed approximation."
"rank-D","the number of linearly independent rows or columns in a matrix."
"rank-M","an approximation of a matrix that retains M dimensions."
"min","Minimum value in a mathematical context."
"convex hull","The smallest convex set that contains a given set of points."
"coefficients","Values that multiply a variable in an equation or expression."
"constraint","A condition that must be satisfied in an optimization problem."
"reproducing kernel Hilbert space","A specific type of Hilbert space associated with kernel functions."
"canonical feature map","A function that maps input data into a feature space using kernel functions."
"kernel function","A function that computes a dot product in a transformed feature space."
"kernel trick","A technique that allows operations in a high-dimensional space without explicitly mapping data."
"Support Vector Machines","A Library for Support Vector Machines."
"Probability","In Defense of Probability."
"Deep Learning","A method within machine learning that uses neural networks."
"Relational Model","A framework for managing databases."
"Linear Dimensionality Reduction","A technique to reduce the number of variables in a dataset."
"Numerical Linear Algebra","A branch of mathematics focused on linear equations and matrices."
"Bootstrap Methods","A statistical resampling method to estimate the distribution of a sample statistic."
"Large Scale Distributed Deep Networks","Networks used in deep learning that operate on large datasets across multiple machines."
"Halfspace","A mathematical concept referring to a subset of a space that divides the space into two parts."
"Depth","A statistical measure that provides a way to identify the most central points in a dataset." 
"Gaussian Processes","A collection of random variables, any finite number of which have a joint Gaussian distribution, used in statistics for regression and classification."
"Bayesian Skill Rating System","A system that uses Bayesian methods to rate skills based on performance, allowing for updates as new performance data becomes available."
"convex combination","The convex combination of points spans a two-dimensional area.",
"convex hull","The convex hull is the triangle formed by the edges corresponding to each pair of points.",
"non-negative weights","Non-negative weights corresponding to each example are used to build a convex hull.",
"conv (X)","The set conv (X) is defined with weights such that the sum of weights equals 1 and each weight is positive."
"inequality constraints","Ax=b is replaced by Ax <b and Agx>b."
"convex optimization","Particular software implementations of convex optimization methods may provide the ability to express equality constraints."
"SVM","The SVM is one of many approaches for studying binary classification."
"optimization problem","Since SVMs have a clear and well-defined optimization problem, many approaches based on numerical optimization techniques can be applied."
"numerical optimization techniques","Many approaches based on numerical optimization techniques can be applied."
"predictor","one of two labels {+1,-1}"
"error/loss function","needs to be appropriate for binary classification"
"squared loss","not suitable for binary classification"
"ideal loss function","to count the number of mismatches between the prediction and the label"
"loss","defined to be zero if they match, and one if they do not match"
"zero-one loss","denoted by 1(f(x) # y) and is called the zero-one loss"
"combinatorial optimization problem","more challenging to solve than continuous optimization problems"
"PCA","maximum-variance formulation of PCA discussed in Section 10.2."
"projection loss","average squared reconstruction error when projecting onto the /-dimensional principal subspace."
"D","the number of dimensions in the principal subspace."
"eigenvalues","the eigenvalues of the data covariance matrix."
"covariance matrix","data covariance matrix."
"minimum","to minimize (10.44) we need to select the smallest D - M eigenvalues."
"orthogonal complement","the eigenvectors are the basis of the orthogonal complement of the principal subspace."
"principal subspace","the basis of the principal subspace comprises the eigenvectors that are associated with the largest M eigenvalues."
"Multivariate Statistics","A Vector Space Approach."
"Approximation of One Matrix by Another of Lower Rank","A method for estimating a matrix using another matrix of reduced dimensions."
"Bootstrap","A statistical method that involves resampling with replacement to estimate properties of an estimator."
"Beautiful Differentiation","Concept related to the study of derivatives in calculus."
"Statistical Learning Theory","A framework for understanding the principles of learning from data."
"bel pair","a pair that allows an example to be within the margin or on the wrong side of the hyperplane"
"hyperplane","a flat affine subspace of one dimension less than its ambient space, used in classification"
"soft margin SVM","a Support Vector Machine that allows some examples to be within the margin or on the wrong side of the hyperplane"
"slack variable","a variable that measures the distance of a positive example to the positive margin hyperplane"
"objective","the function that is to be optimized in a mathematical problem"
"non-negative","a term used to describe a variable that is constrained to be zero or positive"
"optimization problem","a mathematical problem that seeks to maximize or minimize a particular function subject to constraints"
"Machine Learning" "Field of study that uses algorithms and statistical models to enable computers to perform tasks without explicit instructions."

"Primal Support Vector Machine" "A type of SVM that works directly in the input feature space rather than transforming it into a higher-dimensional space."

"inner product" "An algebraic operation that generalizes the dot product to more abstract vector spaces."

"bilinearity" "A property of a function that is linear in each of its arguments separately."

"normal vector" "A vector that is perpendicular to a given surface or hyperplane."

"hyperplane" "A subspace of one dimension less than its ambient space, used to divide a space into two half-spaces."

"distance" "A measure of the interval between two points in space."

"temporary variable" "A variable used for a specific, temporary purpose in a calculation or derivation."
"convex","A function or set that can be checked from first principles by recalling the definitions."
"negative entropy","A function that is 10 convex and is used in the context of optimization."
"nonnegative weighted sum","A sum of convex functions multiplied by nonnegative scalars, which results in a convex function."
"scalar","A nonnegative value used in the context of functions."
"closure","The idea introduced in vector spaces that relates to preserving convexity."
"Convex Analysis","Fundamentals of Convex Analysis."
"Online Learning","A method used for Latent Dirichlet Allocation."
"Latent Dirichlet Allocation","A generative statistical model used for topic modeling."
"Stochastic Variational Inference","A technique used in machine learning for approximating complex distributions."
"Kernel Methods","A class of algorithms for pattern analysis in machine learning."
"Linear Algebra","A branch of mathematics concerning linear equations and their representations."
"Matrix Analysis","A field of study in mathematics dealing with matrices and their properties."
"Principal Components","New variables obtained by transforming original correlated variables into a set of values of linearly uncorrelated variables."
"subspace","M-dimensional subspace with maximal variance"
"eigenvalue decomposition","the eigenvalue decomposition"
"eigenvalues","Eigenvalues sorted in descending order"
"variance","Variance captured by the principal components associated with the largest eigenvalues"
"principal component analysis","Dimensionality Reduction with Principal Component Analysis"
"principal subspace","the principal subspace onto which B,,_1 projects"
"ONB","an ONB of this principal subspace"
"eigenvector","b; is also an eigenvector of S with eigen-value λ;"
"eigenvalue","λ is the largest eigenvalue of S"
"equation","Equation (10.21) reveals that b,, is not only an eigenvector of S' but also of S"
"hyperplane","A flat affine subspace of one dimension less than its ambient space, defined by a linear equation."
"distance","The amount of space between two points, measured along a straight line."
"orthogonal projection","The projection of a point onto a line or a plane, which is perpendicular to the line or plane."
"scaling","The process of multiplying a vector by a scalar value to adjust its magnitude."
"unit length","A vector whose length (or norm) is equal to 1."
"norm","A function that assigns a positive length or size to a vector."
"vector addition","The operation of adding two vectors together to form a new vector."
"subspace","A vector space that is a subset of a larger vector space."
"SVM","Support Vector Machine, a supervised learning model used for classification and regression tasks."
"Variational Methods","Techniques used in mathematical optimization and in the field of graphical models."
"Kalman Filter","An algorithm that uses a series of measurements observed over time to produce estimates of unknown variables."
"SVD","Singular Value Decomposition, a factorization method for matrices that reveals valuable properties of the matrix." 
"Linear Filtering","A process that applies a linear operation to a signal in order to extract information or reduce noise."
"basis vectors","the basis vectors b,,...,6; of the principal subspace"
"loss function","the mechanism we use to quantify the difference between predicted and actual outcomes"
"dot product","a mathematical operation that takes two equal-length sequences of numbers and returns a single number"
"orthogonal projection","the coordinate of the orthogonal projection of a onto the subspace spanned by b;."
"subspace","a vector space that is a subset of another vector space"
"linear combination","an expression constructed from a set of terms by multiplying each term by a constant and adding the results"
"Analytic Geometry","Study of geometry using a coordinate system and the principles of algebra."
"Givens rotation","A rotation operation in a plane spanned by two coordinates."
"Identity matrix","A square matrix with ones on the diagonal and zeros elsewhere."
"Orthogonal matrices","Matrices that represent rotations and preserve lengths and angles."
"Preserve distances","Rotations maintain the distance between points before and after transformation."
"Preserve angles","Rotations maintain the angle between vectors after transformation."
"Non-commutative","Order of operations matters; changing the order affects the outcome."
"class probability estimation","this is called class probability estimation."
"binary classifier","The SVM is a binary classifier that does not naturally lend itself to a probabilistic interpretation."
"calibrated class probability estimate","converting the raw output of the linear function (the score) into a calibrated class probability estimate."
"probabilistic interpretation","The SVM is a binary classifier that does not naturally lend itself to a probabilistic interpretation."
"calibration step","involve an additional calibration step."
"probabilistic approaches","From the training perspective, there are many related probabilistic approaches."
"loading of the unit vector b","represents the standard deviation of the data accounted for by the principal subspace"
"principal component","not defined in the text"
"constrained optimization problem","max b, Sb; subject to ||b;||? =1"
"Lagrangian","£(b1, A) = b] Sb; + (1 — By by)"
"partial derivatives","the derivatives of £ with respect to b, and A"
"eigenvalue decomposition","compares with the definition of an eigenvalue decomposition"
"eigenvector","b; is an eigenvector of the data covariance matrix S"
"covariance matrix","the matrix S represents the data covariance"
"Lagrange multiplier","\, plays the role of the corresponding eigenvalue"
"variance objective","V, = b] Sb;"
"dimension","the subspace onto which we project the data is M."
"optimal linear projection","to find this optimal linear projection, we need to find the orthonormal basis of the principal subspace."
"orthonormal basis (ONB)","the basis of the principal subspace."
"coordinates","the coordinates z,, € R™ of the projections with respect to this basis."
"principal subspace","the subspace related to the optimal linear projection."
"two-step approach","First, we optimize the coordinates z,, for a given ONB; second, we find the optimal ONB."
"optimal coordinates","finding the optimal coordinates z,,,,..., Z,z, of the projections."
"linear projection","the representation of the linear projection & with respect to b."
"orthogonal projection","this will be the orthogonal projection."
"Large-Scale Machine Learning","A field focusing on algorithms and techniques that handle large datasets efficiently."
"Concentration Inequalities","A mathematical theory dealing with the behaviors of random variables and their deviations from expected values."
"Convex Optimization","A subfield of optimization that studies the problem of minimizing convex functions over convex sets."
"Applied Linear Algebra","The application of linear algebra techniques in various fields, focusing on practical problem-solving."
"Bayesian Optimization","A method for optimizing expensive cost functions using Bayesian inference."
"Markov Chain Monte Carlo","A class of algorithms for sampling from probability distributions using Markov chains."
"Statistical Exponential Families","A class of probability distributions characterized by their exponential form, used in statistical decision theory."
"Maximum Likelihood","A method for estimating the parameters of a statistical model by maximizing the likelihood function."
"Orthogonal Projection","A geometric interpretation in which a point is projected onto a line or plane that minimizes the distance."
"Linear Regression","A statistical method for modeling the relationship between a dependent variable and one or more independent variables using a linear equation."
"Slope Parameter","The coefficient that represents the rate of change of the dependent variable relative to the independent variable in a linear equation."
"Maximum Likelihood Estimator","A statistical estimator that maximizes the likelihood function for given data."
"Training Dataset","A set of data used to train a model in machine learning, consisting of input-output pairs."
"Reconstruction of Training Targets","The process of estimating the outcomes for the training dataset based on the model."
"convex hull","the smallest convex set that contains a given set of points."
"difference vector","a vector that represents the difference between two points in space."
"minimizing the length/norm","the process of finding the smallest value of the length or magnitude of a vector."
"optimization problem","a mathematical problem that aims to find the best solution from a set of feasible solutions."
"convex combination","a linear combination of points where the coefficients are non-negative and sum up to one."
"non-negative coefficients","coefficients that are either positive or zero."
"objective","the function that is being optimized in an optimization problem."
"projection","Find a subspace (line) that minimizes the length of the difference vector between projected (orange) and original (blue) data."
"subspace","One-dimensional subspace U C RR? spanned by b."
"vector","A vector a € IR? (red cross) shall be projected onto a one-dimensional subspace."
"difference vector","The difference vectors between x and some candidates z."
"dimensionality","The dimensionality of the plane is 2, but the vectors still have three coordinates."
"coordinates","The vectors still have three coordinates with respect to the standard basis of R."
"principal component analysis","Dimensionality Reduction with Principal Component Analysis." 
"reconstruction","Minimize this distance so that x, and z, are as close as possible."
"Lagrange multiplier","a > 0 as the Lagrange multiplier corresponding to the constraint (12.26b) that the examples are classified correctly and y,, > 0 as the Lagrange multiplier corresponding to the non-negativity constraint of the slack variable"

"Lagrangian","The Lagrangian is then given by L&(w,b,€,a,7) = 1/N ∑(an(yn((w, an) +) 14+ En) — So rnEn"

"representer theorem","The representer theorem is actually a collection of theorems saying that the solution of minimizing empirical risk lies in the subspace defined by the examples."

"support vector","Classification with Support Vector Machines"
"PCA","In PCA, we find a compressed version z of original data x."
"compressed data","The compressed data can be reconstructed into &, which lives in the original data space, but has an intrinsic lower-dimensional representation."
"vector space","The dimension of a vector space corresponds to the number of its basis vectors."
"basis vectors","The number of its basis vectors."
"dimensionality reduction","Dimensionality Reduction with Principal Component Analysis."
"handwritten digits","Examples of handwritten digits from the MNIST dataset."
"grayscale image","Each digit is a grayscale image of size 28 x 28."
"vector","We can interpret every image in this dataset as a vector a € R™+."
"maximum variance perspective","Maximum Variance Perspective."
"symmetric","we can find an ONB of eigenvectors"
"ONB","orthonormal basis"
"eigenvectors","distinct eigenvectors for both S and S"
"eigenvector","a vector b; such that Sb; = λb;"
"principal components","first m — 1 principal components"
"orthogonal","b; is orthogonal to the first m—1 principal components"
"basis vector","b; is a basis vector among the first m — 1 principal components"
"matrix","X := [@1,...,@n] € R^d*N"
"compressed","data that has not yet been compressed"
"intimate connection","there is an intimate connection between"
"classification","a supervised learning task where examples are labeled for assignment into categories."
"support vector machine (SVM)","an approach that solves the binary classification task."
"binary classification","a task where examples are classified into one of two categories."
"supervised learning","a type of machine learning where a model is trained on labeled data."
"training data set","a set of example-label pairs used to estimate model parameters."
"classification error","the difference between the predicted and actual labels in a classification task."
"linear model","a model that assumes a linear relationship between input features and the output."
"nonlinearity","a transformation that is not linear, affecting how data can be modeled."
"theoretical guarantees","statements that provide assurance about the performance and reliability of a model."
"posterior odds","The ratio of the posteriors."
"prior odds","Measures how much our prior (initial) beliefs favor one model over another."
"Bayes factor","Measures how well the data D is predicted by one model compared to another."
"Jeffreys-Lindley paradox","States that the Bayes factor always favors the simpler model since the probability of the data under a complex model with a diffuse prior will be very small."
"diffuse prior","A prior that does not favor specific models, meaning many models are a priori plausible under this prior."
"Maximal Variance","We maximize the variance of the low-dimensional code using a sequential approach."
"Variance","The measure of how much values differ from the mean."
"Low-dimensional code","A representation of data in a lower-dimensional space."
"Vector","An element of a vector space, often represented as an array of numbers."
"Projected data","Data transformed into a lower-dimensional space."
"Coordinate","A set of values that define points in a space."
"i.i.d. assumption","Assumes that the data points are independent and identically distributed."
"Orthogonal projection","The component of a vector that lies along another vector or subspace."
"Covariance matrix","A matrix representing the covariance between elements of a dataset."
"Dot product","An algebraic operation that combines two sequences of numbers to yield a single number."
"Symmetric","A property indicating that an operation yields the same result regardless of the order of inputs."
"gradient descent","A method used to minimize a function by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient."
"unbiased estimate","An estimate that is mathematically expected to be equal to the true value it estimates."
"empirical estimate","An estimate derived from observed and experimental data rather than theoretical data."
"expected value","The long-term average or mean value of a random variable."
"stochastic gradient descent","A variation of gradient descent where a random subset of data is used to estimate the gradient, allowing for faster convergence."
"local minimum","A point where the function value is lower than its neighboring values, but not necessarily the lowest point overall."
"eigenvector","associated with a matrix and indicates a direction where the matrix acts by scaling."
"eigenvalue","a scalar associated with an eigenvector, indicating the factor by which the eigenvector is scaled."
"null space","the set of vectors that, when multiplied by a given matrix, result in the zero vector."
"principal subspace","the subspace spanned by the principal components, which represent directions of maximum variance in the data."
"variance","a measure of the dispersion of a set of values, indicating how much the values differ from the mean."
"data covariance matrix","a matrix that describes the covariance between pairs of variables in the data set."
"Search Engine","A system for retrieving information stored in databases, typically on the internet."
"Automatic Differentiation","A technique used to evaluate the derivative of a function efficiently and accurately."
"Derivatives","Measures of how a function changes as its input changes, representing the slope of the function."
"Algorithmic Differentiation","A computational method to obtain derivatives of functions using algorithmic techniques."
"Probability","A branch of mathematics dealing with the likelihood of events occurring."
"Inductive Logic","A form of reasoning that involves drawing general conclusions from specific observations."
"Bootstrap","A statistical method used to estimate the distribution of a statistic by resampling with replacement."
"Edgeworth Expansion","A series that provides an approximation of the distribution of a statistic under certain conditions."
"Multivariate Quantiles","Quantiles that are extended to more than one variable or dimension in statistics."
"Multiple-Output Regression Quantiles","A statistical analysis technique to understand the relationships between multiple inputs and outputs."
"Theorem" "A statement that has been proven on the basis of previously established statements or other theorems."
"Soft Margin SVM" "A model that allows for some classification errors in the case where data is not linearly separable."
"Margin" "The region around the hyperplane where data points may be classified with some error."
"Hyperplane" "A flat affine subspace of one dimension less than its ambient space."
"Optimization problem" "A mathematical problem of finding the best solution from a set of possible choices."
"Lagrange multipliers" "A strategy for finding the local maxima and minima of a function subject to equality constraints."
"Dual optimization problem" "An optimization problem that is derived from another optimization problem, providing a different perspective on the constraints and objectives."
"Geometric arguments" "Reasoning or proofs based on geometric properties and relationships."
"Convex hulls" "The smallest convex set that contains a given set of points." 
"Slack variable" "A variable that allows for flexibility in a model by permitting certain constraints to be relaxed."
"Bayesian Gaussian Process Latent Variable Model","A statistical model that uses Bayesian inference and Gaussian processes to model latent variables."
"Gradient Descent","An optimization algorithm used to minimize a function by iteratively moving in the direction of the steepest descent."
"Numerical Linear Algebra","A branch of mathematics concerning numerical methods for linear algebra problems."
"Three-Mode Factor Analysis","A statistical method for analyzing data that involves three different modes or dimensions."
"Statistical Learning Theory","A framework for understanding the principles behind statistical learning, including the generalization of learning algorithms."
"Graph Kernels","A method used in machine learning to measure similarities between graphs, often for classification tasks."
"connectivity" ,"the state of being connected or linked, specifically relating to network nodes."
"adjacency matrix" ,"a matrix that represents the connections between nodes in a network."
"neural network" ,"a network of interconnected neurons, such as those in the brain."
"eigenvalues" ,"values that characterize the behavior of a linear transformation and are associated with a matrix."
"symmetrized version" ,"a modified form of a matrix that has been adjusted to create symmetry."
"eigenspectrum" ,"the set of eigenvalues associated with a matrix."
"Lagrangian","A function used in optimization that combines the objective function and constraints."
"partial derivatives","The derivatives of a function with respect to one variable while keeping other variables constant."
"maximum","The largest value of a function."
"representer theorem","States that the optimal weight vector in the primal is a linear combination of the examples."
"linear combination","A mathematical operation where coefficients are multiplied by variables and then summed."
"optimal weight vector","The set of weights that minimizes or maximizes the objective function."
"affine combination","A combination of points where the coefficients sum to one."
"regularized empirical risk minimization","A framework in statistical learning that aims to minimize the expected risk with a penalty for complexity."
"Conditional Expectation","Expect the Unexpected from Conditional Expectation."
"Occam’s Razor","A principle that suggests the simplest explanation is often the best one."
"Bayesian Monte Carlo","A method that combines Bayesian inference with Monte Carlo techniques."
"Gaussian Processes","A collection of random variables, any finite number of which have a joint Gaussian distribution."
"Information Divergence","A measure of how one probability distribution diverges from a second, expected probability distribution."
"Value Regularization","A technique used to prevent overfitting in machine learning by adding a penalty for complexity."
"Fenchel Duality","A concept in mathematical optimization that refers to the relationship between a function and its dual."
"Gaussian processes","are the current state of the art in probabilistic regression (fitting curves to data points)."
"kernels","the idea of kernels is explored further in Chapter 12."
"Projections","often used in computer graphics, e.g., to generate shadows."
"orthogonal projections","often used to (iteratively) minimize residual errors."
"residual errors","errors that we want to minimize in linear regression."
"linear function","a function that minimizes the residual errors."
"PCA","also uses projections to reduce the dimensionality of high-dimensional data."
"Theorem","a statement that has been proven based on previously established statements such as other theorems or axioms."
"maximizing the margin","maximizing the space between data points and decision boundaries in classification."
"normalized weights","weights that have been scaled to have a unit norm."
"margin","the smallest distance between the data points of different classes."
"data fitting","the process of adjusting a model to match a set of observations."
"scaling the data","the process of adjusting the range of data features for better model performance."
"strictly monotonic transformation","a function that preserves the order of its inputs."
"reparametrize","to express a parameter in a different form."
"weight vector","a vector representing the weights assigned to the features in a model."
"constraint","a condition that a solution to an optimization problem must satisfy."
"positive distance","a requirement that a quantity must be greater than zero."
"rotation","A transformation that moves objects in a plane about the origin, typically defined by an angle."
"rotation angle","The angle through which an object is rotated; if positive, the rotation is counterclockwise."
"rotation matrix","A matrix used to perform a rotation in a coordinate plane."
"linear mappings","Functions that map lines to lines, preserving the operations of vector addition and scalar multiplication."
"basis change","A transformation that converts one basis of a vector space into another."
"trigonometry","The branch of mathematics dealing with the relationships between the angles and sides of triangles."
"Kernels","Introduce the idea of kernels in the context of support vector machines (SVMs) for modifying inner products based on feature representations."
"Dual SVM","Refers to the formulation of the dual problem in support vector machines, focusing on inner products between examples."
"Inner product","A mathematical operation that combines two vectors, often used in the context of linear classifiers."
"SVM","A classification method, specifically support vector machines, that can assume either linear or non-linear classifiers based on the representation of the data."
"Feature representation","Refers to the way in which examples are represented in a feature space, denoted as @(a)."
"greater than or equal to","always greater than or equal to dual values."
"dual values","values related to the dual optimization problem."
"indicator function","function that indicates the membership of an element in a set."
"Lagrangian","a function used in optimization that combines the objective function and constraints."
"lower bound","a value that is less than or equal to every value in a given set."
"maximum","the largest value in a given set or function."
"minimizing","the process of finding the smallest value of a function."
"minimax inequality","a theorem stating that the minimum of a maximum is greater than or equal to the maximum of a minimum."
"weak duality","an optimization principle stating that a certain relation holds between primal and dual problems."
"dual objective function","the function that represents the dual problem's objective."
"unconstrained optimization problem","an optimization problem without constraints on the variables."
"D x N matrix","Our data matrix X is a D x N matrix instead of the conventional N x D matrix."
"N x D matrix","The conventional N x D matrix."
"algebra operations","The reason for our choice is that the algebra operations work out smoothly without the need to either transpose the matrix or to redefine vectors."
"principal component","To find the mth principal component, we maximize the variance."
"variance","To find the mth principal component, we maximize the variance."
"covariance matrix","We defined S as the data covariance matrix of the transformed dataset."
"constrained optimization problem","We solve a constrained optimization problem and discover that the optimal solution is the eigenvector."
"eigenvector","The optimal solution b,,, is the eigenvector of S that is associated with the largest eigenvalue of S."
"eigenvalue","The largest eigenvalue of S."
"adaptive gradient methods","rescale the step-size at each iteration, depending on local properties of the function."
"step-size","also called the learning rate."
"gradient descent","a method that smoothes out erratic behavior of gradient updates and dampens oscillations."
"oscillations","erratic behavior in the updates during the optimization process."
"heuristics","simple rules used to adjust the step-size based on the function value." 
"condition number","a measure of the sensitivity of the solution of a system of equations to changes in the input."
"preconditioner","a matrix that is used to transform a linear system into an equivalent system that is easier to solve."
"Bayesian inference","A method that uses Bayes' theorem to update the probability for a hypothesis as more evidence or information becomes available."
"Likelihood function","A function of the parameters of a statistical model that describes the probability of observing the data given those parameters."
"Binary classification","A classification task where there are only two possible outcomes."
"Gaussian likelihood","A likelihood function based on the normal distribution, which is used for continuous data."
"Bernoulli likelihood","A likelihood function appropriate for binary outcomes, indicating the probability of success in a Bernoulli trial."
"Count data","Data that represent counts of events, which are non-negative integers."
"Binomial likelihood","A likelihood function that models the number of successes in a sequence of independent Bernoulli trials."
"Poisson likelihood","A likelihood function used for modeling count data that assumes events occur independently and the average rate is constant."
"Generalized linear models (GLM)","A flexible generalization of linear regression that allows for response variables with error distributions other than a Gaussian distribution."
"homogeneous linear equation system","a system of linear equations in which all the equations are homogeneous, meaning they equal zero."
"normal equation","the expression used in linear regression that allows solving for coefficients or coordinates derived from the least squares method."
"linear independence","a property of a set of vectors in which no vector in the set can be written as a linear combination of the others."
"regular","a term referring to matrices that are invertible or non-singular."
"pseudo-inverse","a generalization of the inverse matrix that can be computed for non-square matrices."
"positive definite","a property of a matrix where all its eigenvalues are positive, ensuring that it is invertible and stable."
"full rank","a condition of a matrix in which its rank is equal to the smallest dimension of the matrix, ensuring linear independence of its rows or columns."
"R°","Three-dimensional Euclidean space."
"Rotations","Transformations that change the position of points in space around an axis."
"Standard basis vectors","Vectors that define each dimension in a coordinate system."
"R1(0)","Rotation matrix for rotation about the e1-axis."
"Rotation about the e2-axis","Rotation performed in the e1e3 plane about the e2-axis."
"Rotation about the e3-axis","Rotation performed in the e1e2 plane about the e3-axis."
"n-dimensional Euclidean vector spaces","Generalization of Euclidean spaces to n dimensions, where n is greater than 3."
"Component Analysis","Dent Component Analysis."
"Causal Inference","Causal Inference for Statistics, Social and Biomedical Sciences."
"Probability Essentials","Probability Essentials."
"Probability Theory","Probability Theory: The Logic of Science."
"Ockham's Razor","Ockham’s Razor and Bayesian Analysis."
"Theory of Probability","Theory of Probability."
"Variational Inference","Variational Inference with Normalizing Flows."
"Stochastic Backpropagation","Stochastic Backpropagation and Approximate Inference in Deep Generative Models."
"Kernel Methods","Advances in Kernel Methods — Support Vector."
"features","the number of individual measurable properties or characteristics used as input in a model."
"optimization problem","a mathematical problem that seeks the best solution from a set of possible solutions."
"dual view","an equivalent perspective on an optimization problem that often simplifies calculations."
"parameters","quantities that define a model and can be adjusted based on learning."
"training set","a collection of examples used to train a model."
"dual SVM","a support vector machine approach that applies the dual formulation of the optimization problem."
"kernels","functions used in machine learning to transform data into a higher-dimensional space."
"convex","a property of a set or function where a line segment connecting any two points in the set lies entirely within the set."
"duality","a relationship between two mathematical concepts, often providing a way to view one problem as an alternative form of another."
"convex duality","the concept that relates the primal problem to its dual problem in optimization."
"Lagrange Multipliers","a strategy used in optimization to find the local maxima and minima of a function subject to equality constraints."
"primal soft margin SVM","a form of support vector machine that allows for some misclassifications in the training data while trying to maximize the margin."
"error","the difference between the output of a predictor and the label."
"predictor","a function f(a) used to estimate or predict an output based on input data."
"loss","describes the error that is made on the training data."
"hinge loss","a specific loss function defined as e(t) = max{0,1—t}."
"hyperplane","a flat affine subspace in a higher-dimensional space, used for classification."
"margin","the distance between the hyperplane and the data points, affecting the hinge loss."
"penalty","the additional cost incurred when predictions are incorrect or too close to the hyperplane."
"linear pieces","refers to the representation of hinge loss as two linear functions depending on the value of t."
"Gradient descent with momentum","A method that introduces an additional term to remember what happened in the previous iteration, dampening oscillations and smoothing out the gradient updates."
"momentum term","Emulates the phenomenon of a heavy ball that is reluctant to change directions."
"gradient update","An update in the optimization process that adjusts the parameters based on the gradient of the loss function."
"moving average","A technique that averages out past values to smooth fluctuations."
"linear combination","Combining two or more quantities with scalar coefficients."
"gradient","A vector that represents the direction and rate of fastest increase of a function."
"iteration","A single cycle of updating the parameters in an optimization algorithm."
"approximation","An estimated value that is close to, but not exactly equal to, the actual value."
"PageRank","This vector is called the PageRank and satisfies Ax* = x*, i.e., it is an eigenvector (with corresponding eigenvalue 1) of A."
"eigenvector","It is an eigenvector (with corresponding eigenvalue 1) of A."
"eigenvalue","It is an eigenvector (with corresponding eigenvalue 1) of A."
"Cholesky Decomposition","There are many ways to factorize special types of matrices that we encounter often in machine learning."
"square-root operation","The square-root operation that gives us a decomposition of the number into identical components."
"symmetric, positive definite matrices","We need to be careful that we compute a square-root-like operation on positive quantities."
"subspaces","also called lines. If not mentioned otherwise, we assume the dot product (x, y) = «'y as the inner product."
"one-dimensional subspace","through the origin with basis vector b € IR”."
"basis vector","a vector that spans the one-dimensional subspace U C R”."
"projection","the vector my (a) € U that is closest to x."
"closest","implies that the distance ||a — 7;;(a)|| is minimal."
"orthogonal projections","a method for finding the closest vector in a subspace."
"Bayesian logistic regression","Estimating a posterior distribution using Bayesian techniques."
"Prior","Specification that includes design choices such as conjugacy with the likelihood."
"Conjugacy","A design choice in Bayesian analysis that relates prior and likelihood."
"Latent functions","Considered as priors in the context of Gaussian process classification."
"Gaussian process classification","A machine learning method that uses Gaussian processes for classification tasks."
"Kernel methods","A broader set of techniques in machine learning that involves methods based on kernel functions."
"Linear algebra","A branch of mathematics concerning linear equations and linear functions."
"Dual SVM","An alternative formulation of Support Vector Machines obtained through specific mathematical transformations."
"Legendre-Fenchel transform","A mathematical tool used in deriving the dual formulation of SVM."
"Eigenvalues","sorted in descending order of the data covariance matrix of all digits “8” in the MNIST training set."
"Variance","captured by the principal components."
"Data covariance matrix","shows the relationship between different variables in a dataset."
"Principal components","the directions of maximum variance in the data, determined by the eigenvectors of the covariance matrix."
"PCA","Principal Component Analysis, a method used to reduce the dimensionality of data while retaining as much variance as possible."
"Eigenvectors","the vectors associated with each eigenvalue that define the directions of the principal components."
"Subspace","a space that is a lower-dimensional subset of a higher-dimensional space."
"R","denotes the set of real numbers, used here in context to describe the dimensionality of a space."
"determinant","a scalar value that can be computed from the elements of a square matrix, providing important properties about the matrix."
"matrix","a rectangular array of numbers, symbols, or expressions, arranged in rows and columns."
"invertible matrix","a square matrix that has an inverse, meaning there exists another matrix that, when multiplied with the original matrix, results in the identity matrix."
"identity matrix","a square matrix that has ones on the diagonal and zeros elsewhere, acting as the multiplicative identity in matrix multiplication."
"scalar number","a single number, often representing a quantity in mathematical contexts."
"n x n matrix","a matrix with n rows and n columns."
"2 x 2 matrix","a matrix composed of 2 rows and 2 columns."
"computational time", "The time required to perform calculations, particularly in the context of algorithm efficiency and performance."
"subset", "A part of a larger set, used here in the context of estimating a gradient."
"gradient", "A vector that represents the direction and rate of fastest increase of a function."
"variance", "A measure of how much values differ from the mean; in this text, it refers to the variability in parameter updates."
"mini-batch sizes", "Small subsets of data used in gradient descent to compute updates."
"parameter update", "An adjustment made to the parameters of a model based on computed gradients."
"matrix operations", "Mathematical operations that involve arrays of numbers, used in optimized calculations."
"convergence", "The process of approaching a limit or the solution of an optimization problem."
"local optima", "Points where a function achieves a minimum or maximum value in a small neighborhood but not necessarily globally."
"optimization methods", "Techniques used to improve performance by finding the best parameters of a model."
"objective function", "A mathematical function that is being minimized or maximized during optimization."
"generalization performance", "The ability of a model to perform well on unseen data, not just the training data."
"variance","the variance of the data projected onto a one-dimensional subspace equals the eigenvalue that is associated with the basis vector b; that spans this subspace."
"eigenvalue","the value associated with the basis vector b that spans this subspace."
"basis vector","the vector that is used to span a subspace."
"eigenvector","the vector that corresponds to an eigenvalue in the data covariance matrix."
"first principal component","the eigenvector associated with the largest eigenvalue of the data covariance matrix."
"principal component","the effect/contribution of the eigenvector in the original data space."
"data covariance matrix","a matrix that contains the covariances between pairs of elements of the data."
"coordinate","a value that represents a point in the space with respect to the basis vector."
"D-dimensional vector","a vector that exists in D dimensions but can be represented in a lower dimension."
"M-dimensional subspace","a subspace that is defined in M dimensions with maximal variance."
"eigenvectors","the vectors related to the eigenvalues that explain the distribution of data in the covariance matrix."
"Probabilistic Model Predictive Control","Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control."
"Bayesian Estimation","A Correspondence between Bayesian Estimation on Stochastic Processes and Smoothing by Splines."
"Stochastic Processes","A Correspondence between Bayesian Estimation on Stochastic Processes and Smoothing by Splines."
"Smoothing by Splines","A Correspondence between Bayesian Estimation on Stochastic Processes and Smoothing by Splines."
"Gradient Theory","Gradient Theory of Optimal Flight Paths."
"Optimal Flight Paths","Gradient Theory of Optimal Flight Paths."
"Auto-Encoding Variational Bayes","Auto-Encoding Variational Bayes." 
"Contextual Classification","Contextual Classification of Multispectral Pixel Data."
"Multispectral Pixel Data","Contextual Classification of Multispectral Pixel Data."
"Tensors","Tensor Decomposition."
"elimination","to find a basis for a vector space spanned by a set of vectors."
"basis","a set of non-orthogonal and unnormalized basis vectors."
"matrix","concatenation of basis vectors into a structured array."
"Gaussian elimination","a method applied to an augmented matrix to obtain an orthonormal basis."
"orthonormal basis","a basis where the inner product is the dot product of vectors and each vector has unit length."
"Gram-Schmidt process","a constructive way to iteratively build an orthonormal basis."
"Euclidean vector space","a space characterized by the standard basis vectors and inner product defined as dot product."
"inner product","a mathematical operation that takes two vectors and returns a scalar."
"dot product","a specific type of inner product that measures the angle and magnitude between vectors."
"orthogonal complement","the set of all vectors that are orthogonal to a given set."
"subspace","A subspace U is a vector space that is contained within another vector space."
"orthogonal projection","An operation that maps a vector onto a subspace such that the resulting vector is the closest point in the subspace to the original vector."
"affine space","A geometric structure that generalizes the properties of Euclidean space and does not have a fixed origin."
"d(x, L)","The distance of point x from the affine space L."
"Gram-Schmidt Orthogonalization","A method for orthonormalizing a set of vectors in an inner product space."
"distance","A measure of how far two points are from each other in a given space, often denoted as d(x, L)."
"subspace","the set of all linear combinations of a given set of vectors, such as the columns of a matrix B."
"data compression","the process of reducing the amount of data required to represent a particular dataset while retaining important information."
"variance","a measure of how much values in a dataset differ from the mean of the dataset."
"covariance matrix","a matrix that captures the covariance (degree to which two variables vary together) between pairs of variables."
"centered data","data that has had its mean subtracted so that the mean is zero."
"low-dimensional code","a representation of data in fewer dimensions that retains essential information."
"mean","the average value of a set of numbers, calculated by summing the values and dividing by the quantity of values."
"vector spaces","spaces consisting of vectors where vector addition and scalar multiplication are defined."
"orthogonal","vectors that are perpendicular to each other in a vector space."
"dimensionality reduction","the process of reducing the number of random variables under consideration."
"orthonormal basis","a basis where all vectors are orthogonal and each has a unit length."
"orthogonal complement","the set of all vectors in a vector space that are orthogonal to a given subspace."
"subspace","a vector space that is entirely contained within another vector space."
"decomposed","breaking down a vector into components that can be expressed in terms of basis vectors."
"determinant","det is invariant to the choice of basis of a linear mapping."
"linear mapping","A mapping ® : V — V in which all transformation matrices A» have the same determinant."
"det(A)","determinant of matrix A."
"Gaussian elimination","A method used to compute det(A) by bringing A into row-echelon form."
"row-echelon form","A form of matrix A where it has non-zero rows at the top and zero rows at the bottom."
"triangular form","A form of matrix A where the elements below the diagonal are all 0."
"determinant of a triangular matrix","The product of the diagonal elements."
"full rank","A matrix A is considered full rank if rk(A) = n."
"invertible","A square matrix A is invertible if and only if it has a non-zero determinant."
"vector","a quantity with both magnitude and direction, often represented as an array of numbers."
"linear combination","an expression formed by multiplying each vector by a scalar and adding the results together."
"basis vectors","a set of vectors in a vector space that, through linear combinations, can represent every vector in that space."
"subspace","a space that is wholly contained within another space, retaining the structure of a vector space."
"dim(U)","the dimension of a vector space U, representing the number of basis vectors in that space."
"span","the set of all possible linear combinations of a given set of vectors."
"PCA","Principal Component Analysis, a technique used for dimensionality reduction that identifies the directions (principal components) that maximize the variance in the data."
"compression loss","the loss of information that occurs when data is compressed."
"squared reconstruction error","a measure of the difference between the original data and its reconstruction, calculated as the square of the Euclidean distance between them."
"eigenvalues","Eigenvalues are special numbers associated with a linear transformation represented by a matrix, indicating the factor by which the transformation scales the eigenvector."
"spectral theorem","The spectral theorem states that a symmetric matrix can be diagonalized by an orthonormal basis of eigenvectors."
"orthonormal eigenbasis","An orthonormal eigenbasis is a set of eigenvectors that are mutually orthogonal and each of unit length."
"principal component","A principal component is a direction in which the data varies the most, identified by performing principal component analysis (PCA)."
"projection matrix","A projection matrix is a square matrix that transforms a vector space onto a subspace."
"subspace","A subspace is a set of vectors that is closed under vector addition and scalar multiplication, forming a vector space of lower dimension within a higher-dimensional space."
"data matrix","A data matrix is a structured array of data points organized as rows or columns to facilitate mathematical operations."
"S" ,"If S is invertible, it is sufficient to ensure that a0 # 0."
"eigenvector" ,"A vector that corresponds to a specific eigenvalue in an eigenvalue problem."
"eigenvalue" ,"A scalar that indicates how much an eigenvector is stretched or compressed."
"matrix" ,"A rectangular array of numbers or variables that can represent data or transformations."
"rank" ,"The dimension of the vector space generated by the rows or columns of a matrix."
"covariance matrix" ,"A matrix that contains the covariance between each pair of elements in a dataset."
"principal component analysis" ,"A statistical method used to reduce the dimensionality of a dataset."
"inequality constraint","the condition imposed on Lagrange multipliers of inequality constraints."
"box constraints","limit the vector a = [ay,--- ,an]! € R* of Lagrange multipliers to be inside the box defined by 0 and C on each axis."
"dual parameters","parameters obtained from a dual formulation which can be used to recover primal parameters."
"primal parameters","the original parameters in the optimization problem that are recovered from the dual parameters."
"representer theorem","a theorem used to express the optimal primal parameter w* using dual parameters."
"margin's boundary","the threshold defined by the optimal decision boundary in a support vector machine."
"12.5°","A specific measure of an angle."
"3.9 Rotations","A statement referencing the number of full turns in a circular motion."
"Length and angle preservation","Characteristics of linear mappings with orthogonal transformation matrices."
"linear mapping","A function between two vector spaces that preserves the operations of vector addition and scalar multiplication."
"orthogonal transformation matrices","Matrices that represent rotations or reflections in a vector space while preserving lengths and angles."
"rotation","A linear mapping that rotates a plane by an angle about the origin."
"Euclidean vector space","A type of space characterized by the properties of Euclidean geometry, where distance and angle measurements are defined."
"counterclockwise direction","The direction of rotation that is opposite to the direction of clock hands, typically associated with positive angles."
"transformation matrix","A matrix that, when multiplied by a vector, transforms the vector according to certain rules or operations."
"application areas of rotations","Fields such as computer graphics and robotics where rotations are significant for functionality."
"Predicting Structured Data","A book edited by Bakir et al. that discusses methods for predicting data that has a special structure."
"Bayesian Reasoning","A statistical method for reasoning about uncertain events, using Bayes' theorem to update the probability estimate as more evidence becomes available."
"Exponential Families","A class of probability distributions characterized by their probability density function being expressed in a specific exponential form."
"Latent Variable Models","Statistical models where the main variables of interest are not directly observed but are inferred from other variables that are observed."
"Factor Analysis","A statistical method used to describe variability among observed variables in terms of fewer unobserved variables (factors)."
"Automatic Differentiation","A computational technique for efficiently and accurately calculating derivatives of functions expressed as computer programs."
"Convex Optimization","A subfield of optimization that studies the problem of minimizing convex functions over convex sets."
"Spectral Methods","Techniques that involve the eigenvalues and eigenvectors of matrices, often used for analyzing data and signals."
"eigenvalue","A scalar value that satisfies the equation A(ca) = cAxv = c\x = X(cx)."
"eigenvector","Vectors that satisfy the eigenvalue equation A(x) = λx."
"eigenvalue equation","The equation that defines eigenvalues and eigenvectors."
"codirected","Refers to vectors that point in the same direction."
"collinear","Vectors that lie along the same line."
"algebraic","Relating to symbols and the rules for manipulating those symbols in mathematical expressions."
"multiplicity","The number of times a particular value appears as a root of a polynomial."
"eigenspace","The set of all eigenvectors associated with a specific eigenvalue."
"eigenspectrum","The set of all eigenvalues of a matrix."
"spectrum","A collection of eigenvalues."
"orthonormal basis","an (ordered) basis B of R? where b/ b; = 1 if i = j and 0 otherwise."
"linear combination","any x € R® can be written as a sum of the basis vectors multiplied by suitable coordinates."
"lower-dimensional subspace","a subspace U C R” with dimension dim(U) = M."
"Euclidean distance","the distance ||a — &|| that we aim to minimize."
"Markov Chain","A mathematical system that undergoes transitions from one state to another on a state space, with probabilities that depend only on the current state and not on the sequence of events that preceded it."
"Monte Carlo","A statistical technique that allows for the estimation of an unknown quantity using random sampling."
"Strictly Proper Scoring Rules","A set of rules that ensure the best forecast is the one that maximizes the expected score."
"Linear Operators","Mappings between vector spaces that preserve the operations of vector addition and scalar multiplication."
"Matrix Computations","Algorithmic processes that manipulate matrices to perform various mathematical operations."
"Deep Learning","A subset of machine learning that uses neural networks with many layers to model complex patterns in data."
"inference","See Chapter 9 for details."
"projection","Find the projection my (a) € U."
"projection matrix","the projection matrix that solves P,,2 = wy (a) must be P,=B(B'B)\"B'."
"subspaces","The solution for projecting onto general subspaces includes the 1D case as a special case."
"dim(U)","If dim(U) = 1, then B'B € Risa scalar."
"basis","the generating set of U is a basis (linear independent)."
"coordinates","find the coordinates A of x in terms of the subspace U, the projection point 7y(x) and the projection matrix P,."
"matrix B","we compute the matrix B "B" and the vector B' x as B."
"linear independent","the generating set of U is a basis (linear independent)."
"dataset","A collection of data points, represented here as Y = {a, ax, € R®, is centered at 0, ... }."
"zero-mean","An assumption that the expected value E[Y'] = 0."
"optimal projection","The best representation of a vector in a lower-dimensional subspace."
"one-dimensional subspace","A subspace that can be represented by a single basis vector."
"distances","The measurement of the space between points, denoted as || — &|| for some e."
"orthogonal projection","The projection of a vector onto a subspace that minimizes the distance to that subspace."
"basis vectors","A set of vectors that define a subspace, represented as b1,...,by."
"principal subspace","The main subspace onto which the original data is projected, typically in PCA."
"Dimensionality Reduction","A process that transforms data from a high-dimensional space to a lower-dimensional space."
"minimizes the distance","Refers to finding a vector projection that results in the smallest distance to a subspace."
"span","The set of all possible linear combinations of a set of vectors."
"Constrained optimization","Optimization that takes into account certain restrictions or constraints on the variables."
"Unconstrained problem","An optimization problem that does not have any constraints."
"Minimum","The lowest point in the context of a mathematical function or optimization problem."
"Box constraints","Constraints that restrict variable values to lie within a specified box or range."
"Optimal solution","The best possible solution that satisfies all constraints of an optimization problem."
"Gradient","A vector that represents the direction and rate of fastest increase of a function."
"Step-size parameter","A scalar value that determines the step taken in the direction of the gradient during optimization."
"Batch gradient descent","An optimization method that computes the gradient using the entire dataset."
"Mini-batch gradient descent","An optimization method that computes the gradient using a randomly chosen subset of the entire dataset."
"Tnk","the probability that the kth mixture component generated the nth data point."
"mixture component","an individual part of a mixture model that represents a distribution."
"Gaussian Mixture Models","a probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions."
"means","the average values of the individual mixture components."
"covariance matrices","matrices that describe the variance and correlation of the mixture component distributions."
"mixture weights","the weights associated with each mixture component, indicating the proportion of data points drawn from each component."
"closed-form solution","an explicit solution expressed in terms of known functions and parameters."
"gradient","the vector of partial derivatives of a function, indicating the direction and rate of change."
"log-likelihood","the logarithm of the likelihood function, used in statistics to measure the fit of a model to data."
"partial derivative","the derivative of a function with respect to one variable, holding other variables constant."
"bijective","A function that is both injective and surjective, meaning each element of the range is mapped from a unique element of the domain."
"binary classification","A type of classification task that involves categorizing data into one of two distinct classes."
"Binomial distribution","A discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials."
"cumulative distribution function","A function that describes the probability that a random variable will take a value less than or equal to a certain value."
"blind-source separation","A process used to separate a set of source signals from a set of mixed signals without any prior knowledge about the sources."
"Borel c-algebra","A σ-algebra generated by the open sets of a topological space, used in the context of measure theory."
"d-separation","A criterion for deciding whether a set of variables is independent of another set, given a third set of variables in a graphical model."
"random forest","A machine learning method used for classification and regression tasks that operates by constructing multiple decision trees and outputting the mode or mean prediction of the individual trees."
"SVMs","Support Vector Machines, a supervised learning model used for classification and regression."
"kernels","Functions used in SVMs to transform data into a higher dimensional space to make it easier to classify."
"empirical risk minimization","A principle in statistical learning that focuses on minimizing the average of the loss function over a given dataset."
"theoretical properties","Inherent characteristics of a mathematical model or algorithm that can be described and analyzed theoretically."
"loss function","A mathematical function that quantifies the difference between the predicted output and the actual output."
"likelihood","A statistical measure of how well a particular model explains observed data."
"logistic regression","A statistical method for modeling the relationship between a binary dependent variable and one or more independent variables using a logistic function."
"generalized linear models","A flexible generalization of ordinary linear regression that allows for response variables to have error distribution models other than a normal distribution."
"vector", "A mathematical object that has both a magnitude and a direction."
"inner products", "A mathematical operation that calculates the angle and length between two vectors."
"orthogonality", "A geometric concept referring to vectors that are perpendicular to each other."
"projections", "The representation of one vector in the direction of another."
"classification algorithms", "Methods used to categorize data into different classes or groups."
"R?", "The notation for n-dimensional real number space."
"binary classification", "A type of classification problem where there are two distinct classes."
"hyperplane", "A flat, affine subspace of one dimension less than its ambient space, used to separate different classes."
"affine subspaces", "A set of points that can be defined as a linear transformation followed by a translation."
"f:R? oR", "A function mapping from an n-dimensional real number space to the real numbers."
"w € R?", "The weight vector that parameterizes the function in the context of classification."
"b € R", "A bias term in the function used for classification."
"{a ER?: f(x) =0}", "The mathematical definition of the hyperplane that separates two classes."
"Gram matrix","The matrix K € R^n resulting from the inner products or the application of k(-,-) to a dataset, and is often just referred to as the kernel matrix."
"kernel matrix","A matrix K that is formed from the inner products of a dataset."
"kernel","Functions that must be symmetric and positive semidefinite so that every kernel matrix K is symmetric and positive semidefinite."
"symmetric","A property of a matrix where it is equal to its transpose."
"positive semidefinite","A property of a matrix such that for any vector z, z'Kz > 0."
"polynomial kernel","A popular example of a kernel for multivariate real-valued data."
"Gaussian radial basis function kernel","A popular example of a kernel for multivariate real-valued data."
"rational quadratic kernel","A popular example of a kernel for multivariate real-valued data."
"canonical feature map","A transformation that allows the data to be expressed in a higher-dimensional space."
"kernel trick","A technique that allows the implicit mapping of data into a higher-dimensional space without ever computing the coordinates of the data in that space."
"conjugate models","special cases in which we can solve it."
"conjugate parameter prior","a chosen prior p(@) that allows computation of marginal likelihood in closed form."
"marginal likelihood","the likelihood of observing the data under a specified model, computed in closed form when using conjugate priors."
"linear regression","a statistical method to model the relationship between a dependent variable and one or more independent variables."
"posterior odds","the odds of a hypothesis after observing data."
"prior odds","the odds of a hypothesis before observing data."
"Bayes factor","a ratio that quantifies the evidence against a null hypothesis based on prior and posterior probabilities."
"Jeffreys-Lindley paradox","a situation in Bayesian statistics where overwhelming evidence against a null hypothesis does not lead to conclusive evidence supporting an alternative hypothesis."
"function","a mapping from inputs to corresponding values."
"training inputs","a set of inputs used to train a model."
"noisy observations","observations that include randomness or error."
"general versions","has more general versions (Schélkopf et al., 2001), and necessary and sufficient conditions on its existence can be found in Yu et al. (2013)."
"representer theorem","The representer theorem (12.38) also provides an explanation of the name “support vector machine.”"
"support vector machine","The examples x,,, for which the corresponding parameters a, = 0, do not contribute to the solution w at all."
"support vectors","The other examples, where a, > 0, are called support vectors since they “support” the hyperplane."
"Lagrangian","By substituting the expression for w into the Lagrangian (12.34), we obtain the dual."
"dual","By substituting the expression for w into the Lagrangian (12.34), we obtain the dual."
"primal variable","Note that there are no longer any terms involving the primal variable w."
"inner products","Recall that inner products are symmetric."
"convex hulls","Figure 12.9 Convex hulls."
"eigenvalues","The maximum amount of variance PCA can capture with the first principal components."
"variance","A measure of how much information can be retained in the PCA."
"principal components","The components derived from PCA to capture variance."
"data covariance matrix","A matrix that represents the covariance between data dimensions."
"data compression","The process of reducing the amount of data by eliminating less important information."
"relative variance","The variance captured relative to the total variance."
"reconstruction error","The difference between the original data and its approximation after compression."
"linear auto-encoder","A type of neural network used to learn efficient representations of data."
"GMM","Gaussian Mixture Model, a probabilistic model that represents a distribution as a combination of multiple Gaussian distributions."
"K","the number of components in the Gaussian Mixture Model."
"N(a| — 4, 1)","a normal distribution with mean -4 and variance 1."
"N(a| 0, 0.2)","a normal distribution with mean 0 and variance 0.2."
"N(a|8, 3)","a normal distribution with mean 8 and variance 3."
"maximum likelihood estimate","a method for estimating the parameters of a statistical model that maximizes the likelihood function."
"likelihood","the predictive distribution of the training data given the parameters."
"i.i.d.","independent and identically distributed, a property of a set of random variables."
"log-likelihood","the logarithm of the likelihood function, commonly used in statistical estimation."
"Gaussian mixture density","a probability density function that represents a mixture of multiple Gaussian distributions."
"equation form","n equation form involves representing mathematical relationships using symbols and numbers."
"min","min refers to the minimum value of a function or set."
"max","max refers to the maximum value of a function or set."
"loss function","a loss function quantifies the difference between predicted and actual outcomes."
"soft margin SVM","soft margin SVM is a Support Vector Machine technique that allows some misclassification to achieve a more generalized model."
"maximum likelihood estimators","maximum likelihood estimators are statistical methods used to estimate parameters of a probability distribution."
"hyperplanes","hyperplanes are flat affine subspaces of one dimension less than their ambient space, used in classification and regression tasks."
"margin","margin refers to the distance between the separating hyperplane and the nearest data point in classification problems."
"regularization term","regularization term is an added component in an optimization problem that discourages overly complex models." 
"binary classification problems","binary classification problems are tasks that involve categorizing data into one of two distinct classes."
"tr(A + B)","tr(A) + tr(B) for A, B ∈ R^n"
"tr(aA)","atr(A), a ∈ R for A ∈ R^n"
"tr(I_n)","n"
"tr(AB)","tr(BA) for A ∈ R^m, B ∈ R^n"
"trace","function satisfying specific properties of matrices"
"cyclic permutations","tr(AKL) = tr(KLA) for matrices A ∈ R^n, K ∈ R^m, L ∈ R^p"
"linear mapping","® : V → V, where V is a vector space"
"transformation matrix","matrix representation of the linear mapping®"
"trace of a map","trace of the transformation matrix A"
"degree of a polynomial","The number of variables in a polynomial expression in a regression setting."
"mixture model","A probabilistic model that assumes that the data points are generated from a finite mixture of several distributions."
"network architecture","The structure and configuration of a neural network."
"type of kernel","The function used to map data into a higher dimensional space in support vector machines."
"dimensionality of the latent space","The number of hidden variables that represent underlying factors in PCA."
"learning rate","A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated."
"maximum likelihood estimate","A method of estimating the parameters of a statistical model that maximizes the likelihood function."
"heuristics for model selection","Rules or methods that guide the choice of the best model to avoid overfitting."
"Probabilistic Graphical Models","Models that use probability distributions to represent the conditional dependencies between random variables."
"Quantile Tomography","A method for analyzing multivariate data using quantiles."
"Linear Algebra","The branch of mathematics concerning linear equations, linear functions, and their representations through matrices and vector spaces."
"Probabilistic Non-Linear Principal Component Analysis","An approach that combines probabilistic models with non-linear dimensionality reduction techniques."
"Univariate Distribution Relationships","The study of relationships among distributions of a single random variable."
"Testing Statistical Hypotheses","A method in statistics to determine the validity of a hypothesis based on sample data."
"Theory of Point Estimation","The branch of statistics that deals with the estimation of parameters of a statistical model."
"Support Vector Machines","A supervised machine learning model used for classification and regression analysis."
"vector","A quantity with both magnitude and direction."
"kernel","The set of vectors that are mapped to the zero vector by a given linear transformation."
"null space","The set of all vectors that, when multiplied by a given matrix, result in the zero vector."
"invertible","A matrix is invertible if there exists another matrix such that their product is the identity matrix."
"determinant","A scalar value that is a function of the entries of a square matrix and encodes certain properties of the matrix."
"characteristic polynomial","A polynomial which is derived from a square matrix and whose roots give the eigenvalues of that matrix."
"eigenvalues","Scalars that characterize the behavior of a linear transformation represented by a matrix."
"eigenvectors","Non-zero vectors that only change by a scalar factor when a linear transformation is applied."
"eigenspaces","Subspaces formed by the set of eigenvectors associated with a particular eigenvalue."
"hinge loss","gives us the unconstrained optimization problem"
"unconstrained optimization problem","the first term is called the regularization term or the regularizer"
"regularization term","the second term is called the loss term or the error term"
"loss term","arises directly from the margin"
"error term","margin maximization can be interpreted as regularization"
"margin","can be directly solved with (sub-)gradient descent methods"
"sub-gradient descent methods","(12.31) and (12.26a) are equivalent"
"equivalent","hinge loss essentially consists of two linear parts"
"linear parts","replace minimization of the hinge loss over t with a minimization of a slack variable € with two constraints"
"similar","56"
"singular","24"
"singular value decomposition","119"
"singular value equation","124"
"singular value matrix","119"
"singular values","119"
"slack variable","379"
"soft margin SVM","379, 380"
"solution","20"
"span","44"
"special solution","27"
"spectral clustering","136"
"spectral norm","131"
"spectral theorem","111"
"spectrum","106"
"square matrix","25"
"standard basis","45"
"standard deviation","190"
"standard normal distribution","198"
"standardization","336"
"statistical independence","194"
"statistical learning theory","265"
"stochastic gradient descent","231"
"strong duality","236"
"sufficient statistics","210"
"sum rule","184"
"support point","61"
"support vector","384"
"supporting hyperplane","242"
"surjective","48"
"SVD","119"
"SVD theorem","119"
"symmetric","73, 76"
"symmetric matrix","25"
"symmetric, positive definite","74"
"symmetric, positive semidefinite","74"
"system of linear equations","20"
"target space","175"
"standard basis", "The standard basis is a set of vectors used to represent other vectors in a space."
"A", "A represents a matrix in the context."
"det(A)", "det(A) refers to the determinant of the matrix A."
"det(A) = 1.0", "This indicates that the determinant of matrix A equals one."
"mapping", "A transformation that relates points in one space to points in another."
"area preserving", "A property of a mapping where the area before and after the transformation remains unchanged."
"eigenvalue", "A scalar that indicates how much an eigenvector is stretched or compressed during a transformation."
"eigenvectors", "Vectors that only change by a scalar factor when a linear transformation is applied."
"rotation", "A transformation that turns points around a fixed center."
"volume preserving", "A property of a transformation where the volume remains unchanged."
"determinant", "A scalar value that can be computed from the elements of a square matrix."
"dimension","M."
"Eckart-Young theorem","Theorem 4.25 offers a direct way to estimate the low-dimensional representation."
"rank-M approximation","The best rank-M approximation minimizes the difference || X — All ||."
"spectral norm","Defined as ||-||, in (4.93)."
"SVD","The Eckart-Young theorem states that Xy, is given by truncating the SVD at the top-M singular value."
"singular value","The / largest singular values of X."
"eigenvalues","Important in fundamental machine learning methods that require matrix decompositions."
"eigenvectors","Important in fundamental machine learning methods that require matrix decompositions."
"characteristic polynomial","We can solve for the eigenvalues as roots of the characteristic polynomial."
"space",""
"auto-encoder",""
"column vector",""
"automatic differentiation",""
"completing the squares",""
"automorphism",""
"concave function",""
"condition number",""
"conditional probability",""
"conditionally independent",""
"conjugate",""
"conjugate prior",""
"convex conjugate",""
"Bayes’ rule",""
"convex function",""
"Bayes’ theorem",""
"convex hull",""
"Bayesian GP-LVM",""
"convex optimization problem",""
"convex set",""
"coordinate",""
"coordinate representation",""
"coordinate vector",""
"backpropagation",""
"basic variable",""
"basis",""
"basis vector",""
"Bayes factor",""
"Bayes’ law",""
"Bayesian inference",""
"Bayesian information criterion",""
"Bayesian linear regression",""
"Bayesian model selection",""
"Bayesian network",""
"correlation",""
"Bayesian PCA",""
"covariance",""
"Covariance matrix",""
"Bernoulli distribution",""
"Beta distribution",""
"bilinear mapping",""
"CP decomposition",""
"cross-covariance",""
"Dimensionality of PCA","A measure of the number of variables required to represent data in Principal Component Analysis."
"SVD","Singular Value Decomposition, a mathematical technique used to factor matrices."
"Latent-Variable Modeling","A statistical approach that involves variables that are not directly observed but are inferred from other variables."
"Bayesian Learning","A method of machine learning that applies Bayesian probability to update the probability for a hypothesis as more evidence or information becomes available."
"EM Algorithm","Expectation-Maximization Algorithm, an iterative method for finding maximum likelihood estimates in statistical models."
"Gradient","Defined as a row vector to generalize to vector-valued functions and apply the multi-variate chain rule."
"Multi-variate chain rule","A rule that allows differentiation of functions with multiple variables."
"Partial derivatives","Derivatives of a function with respect to one variable while keeping others constant."
"Matrix","A rectangular array of numbers or functions arranged in rows and columns."
"Basic differentiation rules","Rules known from school, such as sum rule and product rule."
"vectors","mathematical objects that have both a magnitude and a direction."
"displacement vector","the difference vector between the original data point and its projection."
"projection","the transformation that maps one vector onto another, often in a lower-dimensional space."
"projection matrix","a matrix that performs the projection of data points into a subspace."
"orthogonal complement","the set of all vectors that are orthogonal to a given subspace."
"principal subspace","the subspace that captures the most variance in the data."
"Low-Rank Approximation","a method that uses a low-rank matrix to approximate data, often in the context of data compression."
"sum over M and a sum over D — M terms","the process of breaking down a sum into two parts based on the number of terms."
"orthogonal","describing lines or vectors that meet at right angles (90 degrees) to each other."
"variable selection","263"
"variance","190"
"vector","37"
"vector addition","37"
"vector space","37"
"vector space homomorphism","48"
"vector space with inner product","73"
"vector subspace","39"
"weak duality","235"
"zero-one loss","381"
"outer product","38"
"overfitting","262"
"PageRank","114"
"parameters","61"
"parametric equation","61"
"partial derivative","146"
"particular solution","27"
"PCA","317"
"pdf","181"
"penalty term","263"
"pivot","30"
"plane","62"
"plate","281"
"population mean and covariance","191"
"positive definite","71"
"posterior","185"
"posterior odds","287"
"power iteration","334"
"power series representation","145"
"PPCA","340"
"preconditioner","230"
"predictor","12"
"primal problem","234"
"principal component","322"
"principal component analysis","136"
"principal subspace","327"
"prior","185"
"prior odds","287"
"probabilistic inverse","186"
"probabilistic PCA","340"
"likelihood","The likelihood is prone to overfitting."
"marginal likelihood","The marginal likelihood is typically not as the model parameters have been marginalized out."
"model parameters","The model parameters have been marginalized out."
"model complexity","The marginal likelihood automatically embodies a trade-off between model complexity and data fit."
"data fit","The marginal likelihood embodies a trade-off between model complexity and data fit."
"Occam’s razor","The marginal likelihood embodies a trade-off between model complexity and data fit (Occam’s razor)."
"parametric models","In parametric models, the number of parameters is often related to the complexity of the model class."
"Akaike information criterion","Akaike information criterion."
"Bayesian information criterion","Bayesian information criterion."
"regression","One of the four pillars of machine learning."
"dimensionality reduction","One of the four pillars of machine learning."
"density estimation","One of the four pillars of machine learning."
"class","One of the four pillars of machine learning."
"vector","a mathematical object with magnitude and direction."
"constrained optimization problem","a type of optimization problem where the solution is restricted to a specific set of conditions."
"unit vectors","vectors that have a magnitude of one."
"direction of maximum variance","the direction in which the data varies the most."
"lower-dimensional subspace","a simplified representation of data that retains as much original variance as possible."
"orthonormal basis vectors","a set of vectors that are both orthogonal and of unit length."
"variance","a measure of the spread of data points in a dataset."
"projected","the operation of mapping data onto a lower-dimensional space."
"magnitude","the length or size of a vector."
"matrix B","a rectangular array of numbers that can represent a collection of vectors."
"basis vectors","Vectors that span a vector space, which can be either orthogonal or non-orthogonal."
"orthogonal basis vectors","A set of basis vectors that are mutually perpendicular."
"Gram-Schmidt method","A process for orthogonalizing a set of vectors in an inner product space."
"dot product","An algebraic operation that takes two equal-length sequences of numbers and returns a single number, reflecting the cosine of the angle between the two vectors."
"inner product","A generalization of the dot product which defines an algebraic operation on vectors in a vector space that yields a scalar."
"orthogonal projection","The projection of a vector onto a subspace spanned by a set of vectors that are orthogonal."
"subspace","A vector space that is entirely contained within another vector space."
"Expectation Propagation","A method used in statistical models to approximate the distribution of the latent variables based on observed data."
"Gaussian Process Dynamical Systems","A statistical model that allows for the representation of dynamic systems using Gaussian processes."
"Gaussian Filtering","A method used to estimate the state of a dynamical system by applying a Gaussian function."
"Smoothing","A technique used to refine the estimate of the state of a system by taking into account past and future observations."
"Maximum Likelihood","A statistical method for estimating the parameters of a statistical model that maximizes the likelihood function."
"EM Algorithm","An iterative method used for finding maximum likelihood estimates from incomplete data." 
"Binary Coding","A method of representing data using two symbols, typically 0 and 1." 
"Speech Spectrograms","A visual representation of the spectrum of frequencies in a sound signal as they vary with time."
"Dimensionality Reduction","exploits structure and correlation and allows us to work with a more compact representation of the data, ideally without losing information."
"Principal Component Analysis","a technique used for dimensionality reduction that identifies the directions (principal components) along which the variance of the data is maximized."
"High-dimensional data","data that has a large number of features or dimensions, making it difficult to analyze and visualize."
"Overcomplete","many dimensions are redundant and can be explained by a combination of other dimensions."
"Correlation","a statistical relationship between two or more dimensions in the data."
"Intrinsic lower-dimensional structure","the underlying structure in high-dimensional data that can often be represented in fewer dimensions." 
"Compression technique","a method used to reduce the size of data without losing significant information."
"kernel matrix","The inputs ¥ of the kernel function can be very general and are not necessarily restricted to RY."
"kernel","The inputs ¥ of the kernel function can be very general and are not necessarily restricted to RY."
"constrained optimization problem","The objective function (12.48) and the constraint (12.50), along with the assumption that a > 0, give us a constrained (convex) optimization problem."
"convex optimization problem","The objective function (12.48) and the constraint (12.50), along with the assumption that a > 0, give us a constrained (convex) optimization problem."
"soft margin dual","To obtain the soft margin dual, we consider the reduced hull."
"reduced hull","The reduced hull is similar to the convex hull but has an upper bound to the size of the coefficients a."
"convex hull","The reduced hull is similar to the convex hull but has an upper bound to the size of the coefficients a."
"maximum possible value","The maximum possible value of the elements of @ restricts the size that the convex hull can take."
"bound on a@","In other words, the bound on a@ shrinks the convex hull to a smaller volume."
"segment","e segment 7y(a) — x from wy (x) to x is orthogonal to U"
"orthogonal","The orthogonality condition yields (7y (a) — x, b) = 0 since angles between vectors are defined via the inner product."
"projection","The projection zy (a) of a onto U must be an element of U and, therefore, a multiple of the basis vector b that spans U."
"basis vector","a multiple of the basis vector b that spans U."
"coordinate","Finding the coordinate \. The orthogonality condition yields mu (@)=b (a (a — my(a),b) = 0 Ab, b) =0."
"inner product","angles between vectors are defined via the inner product."
"bilinearity","We can now exploit the bilinearity of the inner product and arrive at"
"symmetric","we exploited the fact that inner products are symmetric."
"dot product","If we choose (-,-) to be the dot product, we obtain"
"projection matrix","the projection matrix P, that maps any x € R” onto U."
"norm","If ||b|| = 1, then the coordinate . of the projection is given by b' x."
"convex quadratic programming","tools."
"binary classification","a well-studied task in machine learning."
"discrimination","sometimes used in binary classification contexts."
"separation","sometimes used in binary classification contexts."
"decision","sometimes used in binary classification contexts."
"output of the linear function","often called the score, which can take any real value."
"ranking","used for ranking the examples in binary classification."
"threshold","picking a threshold on the ranked examples."
"non-linear function","to constrain its value to a bounded range."
"bounded range","for example in the interval {0, 1]."
"sigmoid function","a common non-linear function."
"well-calibrated probabilities","result of applying non-linearity in certain contexts."
"polynomial kernel","where the number of terms in the explicit expansion grows very quickly (even for polynomials of low degree) when the input dimension is large."
"kernel function","only requires one multiplication per input dimension, which can provide significant computational savings."
"Gaussian radial basis function kernel","where the corresponding feature space is infinite dimensional."
"feature space","cannot explicitly represent the feature space but can still compute similarities between a pair of examples using the kernel."
"kernel trick","there is no need for the original data to be already represented as multivariate real-valued data."
"inner product","is defined on the output of the function @(-), but does not restrict the input to real numbers."
"yperplanes","hyperplanes are a fundamental concept in geometry, representing flat affine subspaces of one dimension less than their ambient space."
"convex functions","a function is convex if its domain is a convex set and the line segment connecting any two points on the graph of the function lies above the graph itself."
"Legendre transform","a mathematical transformation applied to a function that relates the gradient of the function to its dual representation."
"Legendre—Fenchel transform","a specific type of Legendre transform that converts a convex differentiable function into its convex conjugate."
"convex conjugate","the convex conjugate of a function f is defined as the supremum of the difference between the linear functional and the original function."
"duality","a concept in mathematics that relates two seemingly different structures or problems, often providing deeper insights or simpler solutions."
"Lagrangian","The Lagrangian is a function that summarizes the dynamics of a system, typically formulated in terms of kinetic and potential energy."
"Lagrange multipliers","Lagrange multipliers are used in optimization to find the local maxima and minima of a function subject to equality constraints."
"dual optimization problem","The dual optimization problem is derived from the original problem by taking the Lagrangian and is expressed in terms of Lagrange multipliers."
"SVM","SVM (Support Vector Machine) is a supervised learning model used for classification and regression tasks that aims to find the hyperplane that best separates the categories."
"dual problem","The dual problem is an optimization problem derived from the primal problem that provides bounds on the solution of the primal."
"non-negative","Non-negative refers to a value that is either positive or zero, indicating that the Lagrange multipliers cannot be less than zero."
"equality constraint","An equality constraint is a condition that requires two expressions to be equal, often used in optimization problems."
"slack variables","Slack variables are used in optimization problems to allow for flexibility in meeting constraints."
"constraint","the condition that the optimization must satisfy."
"optimization","the process of making something as effective or functional as possible."
"soft margin SVM","a type of support vector machine that allows for some misclassifications in the training data."
"minimization","the process of finding the lowest value of a function."
"parameters","quantities that define the system or model in optimization problems."
"identity matrix","a square matrix in which all the elements of the principal diagonal are ones and all other elements are zeros."
"matrix of zeros","a matrix in which all the elements are zero."
"matrix of ones","a matrix in which all the elements are one."
"vector of labels","an ordered array of labels corresponding to each instance in the data."
"diag","a function that creates a diagonal matrix from a vector."
"g","278"
"probability","175"
"probability density function","181"
"probability distribution","172"
"probability integral transform","217"
"probability mass function","178"
"product rule","184"
"projection","82"
"projection error","88"
"projection matrix","82"
"pseudo-inverse","86"
"random variable","172"
"range","58"
"rank","47"
"rank deficient","47"
"rank-k approximation","130"
"rank-nullity theorem","60"
"raw-score formula for variance","193"
"recognition network","344"
"reconstruction error","88"
"reduced hull","388"
"reduced row-echelon form","31"
"reduced SVD","129"
"REF","30"
"regression","289"
"regular","24"
"regularization","262"
"regularization parameter","263"
"regularized least squares","302"
"regularizer","263"
"representer theorem","384"
"responsibility","352"
"reverse mode","161"
"right-singular vectors","119"
"RMSE","298"
"root mean square error","298"
"rotation","91"
"rotation matrix","92"
"row","22"
"row vector","22"
"row-echelon form","30"
"sample mean","192"
"sample space","175"
"scalar","37"
"scalar product","72"
"sigmoid","213"
"dataset","a collection of data points that may not be linearly separable."
"linearly separable","a property of a dataset where two classes can be separated by a straight line."
"inner products","a mathematical operation that takes two vectors and returns a scalar."
"non-linear feature map","a transformation that maps data into a higher-dimensional space to make it linearly separable."
"similarity function","a function k that measures similarity between two examples."
"kernels","functions k : Y x X% — R that define a non-linear feature map."
"Hilbert space","a complete and infinite-dimensional vector space used in functional analysis."
"feature map","a mapping @ : XY — H that relates input space to Hilbert space."
"nested cross-validation","a model validation technique that uses subsets of data to tune parameters effectively."
"ONB","An orthonormal basis (ONB) of a vector space."
"partial derivatives","Derivatives of a multivariable function with respect to each variable."
"orthogonal complement","The set of vectors that are orthogonal to a subspace."
"orthogonal projections","The projection of a vector onto a subspace, minimizing the distance to that subspace."
"orthonormal basis","A basis where all vectors are orthogonal and of unit length."
"coordinate of projection","The value representing a vector's projection onto a subspace defined by a basis vector."
"subspace","A subset of a vector space that is itself a vector space."
"Hilbertiens d’Espaces Vectoriels Topologiques","A concept referring to Hilbert spaces in the context of topological vector spaces."
"Estimating the Dimension of a Model","A statistical method to determine the number of parameters in a model."
"Bayesian Optimization","A probabilistic model-based optimization technique for optimizing objective functions."
"Machine Learning","A field of artificial intelligence that uses algorithms to enable computers to learn from data."
"Kernel Methods","A class of algorithms for pattern analysis in machine learning, using kernel functions."
"Optimization Methodologies","Various techniques used to enhance the performance of algorithms, particularly in support vector machines."
"Gaussian Belief Propagation","An algorithm used for performing inference on graphical models, particularly useful in linear equations."
"objective function","the sum of the losses L incurred by each example n."
"parameter","the vector of parameters of interest, i.e., we want to find 6 that minimizes L."
"negative log-likelihood","an example from regression represented as a sum over log-likelihoods of individual examples."
"log-likelihood","the likelihood of individual examples expressed in logarithmic form."
"gradient descent","a batch optimization method, performing optimization using the full training set."
"stochastic gradient descent","an optimization method that uses a stochastic process for training models."
"constrained optimization","an optimization method that involves constraints on the parameters."
"Lagrange multipliers","a strategy for finding the local maxima and minima of a function subject to equality constraints."
"Laplace expansion","A method for computing the determinant of a matrix by expressing it in terms of smaller matrices."
"determinant","A scalar value that is a function of a square matrix, providing important properties about the matrix."
"minor","The determinant of a smaller matrix obtained by deleting one row and one column from a larger matrix."
"cofactor","The signed minor of an element in a matrix, defined as (-1)^(i+j) times the determinant of the associated minor."
"algorithm","A step-by-step procedure or formula for solving a problem or accomplishing a task."
"matrix","A rectangular array of numbers or functions arranged in rows and columns."
"det(A)","The notation representing the determinant of matrix A."
"nxn matrix","A matrix that has n rows and n columns."
"recursively","A method of problem-solving where the solution depends on solutions to smaller instances of the same problem."
"2 x 2 matrices","Matrices that have 2 rows and 2 columns."
"Expansion along column j","A method of calculating the determinant based on a specific column of the matrix."
"Expansion along row j","A method of calculating the determinant based on a specific row of the matrix."
"Projection","the best linear projection of Y onto a lower-dimensional subspace U of R?"
"Subspace" ,"a lower-dimensional subspace U of R? with dim(U) = M"
"Orthonormal basis" ,"basis vectors b;,..., bj"
"Principal subspace" ,"we will call this subspace U the principal subspace"
"Projection of the data points" ,"the projections of the data points are denoted by Bn = > Znnbm"
"Coordinate vector" ,"where 2n := [Zin,---,2Mn|' € R™ is the coordinate vector of %, with respect to the basis (b;,..., 6,7)"
"Squared Euclidean norm" ,"the similarity measure we use is the squared Euclidean norm ||a — ||" between « and ."
"Objective" ,"we therefore define our objective as the minimizing the average squared Euclidean distance"
"Reconstruction error" ,"the minimizing the average squared Euclidean distance (reconstruction er- ror)"
"scale","we need to scale b in order to “reach” &."
"optimal coordinates","Setting this partial derivative to 0 yields immediately the optimal coordinates."
"orthogonal projection","This means that the optimal coordinates z;,, of the projection @, are the coordinates of the orthogonal projection."
"linear projection","The optimal linear projection %,, of x,, is an orthogonal projection."
"principal subspace","the orthogonal projection of x,, onto the principal sub-space."
"best linear mapping","An orthogonal projection is the best linear mapping given the objective (10.29)."
"plane in the n-dimensional space","two-dimensional subspace of R”"
"Givens Rotation","Let V be an n-dimensional Euclidean vector space and an automorphism with transformation"
"kernel methods","exploit the fact that many linear algorithms can be expressed purely by inner product computations"
"kernel trick","allows us to compute these inner products implicitly in a (potentially infinite-dimensional) feature space"
"non-linearization","enabled many algorithms used in machine learning, such as kernel-PCA for dimensionality reduction"
"Linear Classification","A method for assigning items to categories based on linear predictors."
"Gaussian Process Regression","A Bayesian regression technique that uses Gaussian processes to encode prior knowledge."
"Latent Variable Models","Statistical models that explain observed variables in terms of underlying unobserved variables."
"Optimal Hard Threshold","The best threshold level used to determine which singular values should be kept or discarded."
"Bayesian Data Analysis","A statistical approach that incorporates prior beliefs, updates them with evidence, and produces posterior distributions."
"Random Number Generation","The process of creating a sequence of numbers that lacks any predictable pattern."
"Monte Carlo Methods","Statistical techniques that utilize randomness to solve mathematical problems, often through simulation."
"Probabilistic Machine Learning","A branch of machine learning focused on modeling uncertainty using probabilistic principles."
"EM Algorithm","A statistical technique used for finding maximum likelihood estimates in models with latent variables."
"Deep Auto-Encoder","A type of artificial neural network used for unsupervised learning of efficient codings."
"Non-Uniform Random Variate Generation","A technique for generating random numbers with a specified non-uniform distribution."
"Hessian Eigenmaps","A method for dimensionality reduction using concepts from Riemannian geometry."
"Locally Linear Embedding","An algorithm for finding a low-dimensional representation of high-dimensional data."
"Optimal Quadratic Programming Algorithms","Algorithms designed to solve optimization problems that can be expressed in quadratic form."
"Variational Inequalities","Mathematical equations that involve finding a function that minimizes a given functional subject to certain constraints."
"Exploratory Data Analysis","An approach to analyzing data sets to summarize their main characteristics, often using visual methods."
"Numerical Solution of Variational Problems","Techniques for approximating solutions to variational problems, typically involving calculus of variations."
"Lineare Algebra","The branch of mathematics concerning linear equations, linear functions, and their representations through matrices and vector spaces."
"Real Analysis and Probability","A field of mathematics that deals with the real numbers and the analysis of real-valued functions, probability theory."
"2N(a| 1, 2) +0.3N (a) 4,1)" "A mathematical expression involving normal distributions and mixture components."
"Parameter Learning via Maximum Likelihood" "A statistical method for estimating the parameters of a model by maximizing the likelihood of the observed data."
"iid" "Independently and identically distributed, a property of random variables that indicates they all come from the same probability distribution and are mutually independent."
"unknown distribution p(a)" "A probability distribution that is not known or specified, from which data points are drawn."
"GMM" "Gaussian Mixture Model, a probabilistic model representing the presence of subpopulations within an overall population."
"mixture components" "The individual distributions that make up a Gaussian Mixture Model."
"means" "The average values of the individual distributions in a Gaussian Mixture Model."
"covariances" "A measure of how much two random variables change together, used in the context of Gaussian distributions within a GMM."
"mixture weights" "The weights that determine the contribution of each component in a Gaussian Mixture Model to the overall distribution."
"free parameters" "The parameters within a model that can be adjusted during training or learning."
"one-dimensional dataset" "A dataset that consists of observations or values along a single dimension."
"Gaussian" "A bell-shaped curve that represents the probability distribution of a continuous random variable, characterized by its mean and variance."
"standard basis","Consider the standard basis {e = | ,e2= \\ of IR?, which defines"
"rotation matrix","the rotation matrix that performs the basis change into the rotated coordinates R(6) is given as"
"rotated coordinates","the rotation matrix that performs the basis change into the rotated coordinates R(6) is given as"
"orthonormal","making sure these images Re), Re2, Re; are orthonormal to each other"
"rotation angle","To have a meaningful rotation angle, we have to define what “counterclockwise” means when we operate in more than two dimensions"
"counterclockwise rotation","a “counterclockwise” (planar) rotation about an axis refers to a rotation about an axis when we look at the ax"
"n","258, 284"
"neutral element","36"
"noninvertible","24"
"nonsingular","24"
"norm","71"
"normal distribution","197"
"normal equation","86"
"normal vector","80"
"null space","33, 47, 58"
"numerator layout","150"
"Occam’s razor","285"
"ONB","79"
"one-hot encoding","364"
"ordered basis","50"
"orthogonal","77"
"orthogonal basis","79"
"orthogonal complement","79"
"orthogonal matrix","78"
"orthonormal","77"
"orthonormal basis","79"
"Taylor polynomial","142, 166"
"Taylor series","142"
"test error","300"
"test set","262, 284"
"Tikhonov regularization","265"
"trace","103"
"training","12"
"training error","300"
"training set","260, 292"
"transfer function","315"
"transformation matrix","51"
"translation vector","63"
"transpose","25, 38"
"triangle inequality","71, 76"
"truncated SVD","129"
"Tucker decomposition","136"
"underfitting","271"
"undirected graphical model","283"
"uniform distribution","182"
"univariate","178"
"unscented transform","170"
"upper-triangular matrix","101"
"k-one matrices","Not explicitly defined but refers to matrices involving k dimensions in linear algebra."
"BB'","Represents the product of matrix B and its transpose, resulting in a symmetric matrix."
"rank","The dimension of the vector space generated by the rows or columns of a matrix."
"average squared reconstruction error","A measure of how close the approximated data is to the original data, averaged over all data points."
"orthonormal basis vectors","A set of vectors that are both orthogonal (perpendicular) and normalized (of unit length)."
"rank-M approximation","An approximation of a matrix that maintains a specified rank, reducing dimensionality."
"identity matrix I","A square matrix in which all the elements of the principal diagonal are ones and all other elements are zeros."
"loss function","A function that measures the difference between the predicted values and the actual values."
"squared norm","The sum of the squares of the elements of a vector, representing its magnitude."
"ONB","Orthonormal Basis, a basis consisting of orthonormal vectors."
"dot product","An algebraic operation that takes two equal-length sequences of numbers and returns a single number, reflecting the magnitude of one vector in the direction of another."
"eigenvector","A vector that satisfies the equation A*x = λ*x for a given eigenvalue λ."
"eigenvalue","A scalar λ such that there exists a non-zero vector x (the eigenvector) satisfying the equation A*x = λ*x."
"eigenspace","The set of all eigenvectors corresponding to a particular eigenvalue, along with the zero vector."
"geometric multiplicity","The number of linearly independent eigenvectors associated with a given eigenvalue."
"Cavalieri’s principle","A principle in geometry stating that if two regions in the same plane have the same height and cross-sectional area at every level, then they have equal areas."
"prior distribution","the probability distribution that represents knowledge or uncertainty about a variable before observing data."
"blind-source separation","the process of separating a set of signals into their original sources without prior information about the sources."
"maximum likelihood estimation","a method for estimating the parameters of a statistical model that maximizes the likelihood function."
"PCA","Principal Component Analysis, a technique that identifies the best lower-dimensional subspace for data representation."
"latent sources","the hidden variables or signals that are not directly observed but underlie the observed data."
"Gaussian","a type of continuous probability distribution for a real-valued random variable, often referred to as the normal distribution."
"Gaussian Mixture Models","a probabilistic model that assumes all data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters."
"reinforcement learning","A type of machine learning algorithm that focuses on how agents ought to take actions in an environment in order to maximize a cumulative reward."
"Gaussian process models","A probabilistic model used in machine learning and statistics for regression and classification tasks."
"Constrained Optimization","An optimization method where the solution must satisfy certain constraints while minimizing or maximizing an objective function."
"Lagrange Multipliers","A mathematical technique for finding the local maxima and minima of a function subject to equality constraints."
"min f(a)","An expression indicating the minimization of a function f with respect to the variable a."
"subject to gi(a@) <0 forall i=1,...,m","A condition that all functions gi evaluated at a must be less than zero for all i in the set of constraints." 
"non-convex","Refers to a type of function that does not have a clear global minimum or maximum due to its shape."
"convex","Describes a function where any line segment connecting two points on the graph does not lie below the graph." 
"indicator function","A function that indicates whether a certain condition is true, often used in optimization to incorporate constraints."
"subgradient","A generalization of the gradient that can be used at nonsmooth points."
"hinge loss","A loss function used in machine learning, particularly with support vector machines, that is differentiable almost everywhere except at one point."
"gradient","A vector that contains the partial derivatives of a function and indicates the direction of the steepest ascent."
"convex quadratic programming","A type of optimization problem where the objective function is quadratic and the feasible region is convex."
"optimization variables","Variables in an optimization problem that are adjusted to find the best solution."
"primal SVM","The original form of the Support Vector Machine problem, focusing on finding the optimal separating hyperplane."
"dual SVM","A reformulation of the SVM problem that focuses on maximizing a dual objective function."
"constrained optimization","An optimization problem where the solution must satisfy certain constraints."
"inner product","A mathematical operation that takes two equal-length sequences of numbers and returns a single number, often used in defining angles and distances in vector spaces."
"lata covariance matrix",""
"ata point",""
"lata-fit term",""
"lecoder",""
"leep auto-encoder",""
"efective",""
"lenominator layout",""
"erivative",""
"lesign matrix",""
"leterminant",""
"iagonal matrix",""
"iagonalizable",""
"iagonalization",""
"difference quotient",""
"imension",""
"dimensionality reduction",""
"irected graphical model",""
"irection",""
"irection space",""
"istance",""
"istribution",""
"istributivity",""
"lomain",""
"ot product",""
"ual SVM",""
"Eckart-Young theorem",""
"eigendecomposition",""
"eigenspace",""
"eigenspectrum",""
"eigenvalue",""
"eigenvalue equation",""
"eigenvector",""
"elementary transformations",""
"EM algorithm",""
"embarrassingly parallel",""
"Eckart-Young theorem","Theorem related to matrix approximations that states the best low-rank approximation of a matrix is given by the truncation of its singular value decomposition."
"Abel-Ruffini theorem","Theorem stating that there is no general solution in radicals to polynomial equations of degree five or higher."
"np.linalg.eigh","Function in NumPy that computes the eigenvalues and right eigenvectors of a square array."
"np.linalg.svd","Function in NumPy that computes the Singular Value Decomposition of a matrix."
"power iteration","An algorithm to find the dominant eigenvalue and corresponding eigenvector of a matrix."
"PCA","Principal Component Analysis, a statistical technique used for dimensionality reduction by transforming data to a new set of variables (principal components)."
"eigenvectors","Vectors that correspond to the eigenvalues of a matrix and represent directions in the data space along which variance is maximized."
"eigenvalues","Scalars that provide information about the magnitude of the corresponding eigenvectors in a matrix."
"data covariance matrix","Matrix representing the covariances between pairs of dimensions in a dataset, central to PCA."
"Bayes factor","the ratio of the marginal likelihoods p(D|M,) and p(D|Mz); used for model selection."
"marginal likelihood","plays an important role in model selection and requires computation of Bayes factors."
"posterior distributions","distributions over models computed using Bayes factors."
"integral","a mathematical concept required to compute the marginal likelihood."
"analytically intractable","describes a problem that cannot be solved with analytical methods."
"approximation techniques","methods resorted to when direct computation is not feasible."
"numerical integration","a method of approximating the integral."
"stochastic approximations","techniques that rely on randomness for approximating solutions."
"Monte Carlo","a computational algorithm that relies on repeated random sampling to obtain numerical results."
"Bayesian Monte Carlo techniques","refers to methods that incorporate Bayesian principles into Monte Carlo methods."
"optimization tools","a variety of methods used to improve the performance of algorithms or models."
"SVM","a machine learning model that focuses on finding the hyperplane that best separates data into classes."
"maximum likelihood","a statistical method for estimating the parameters of a probabilistic model that maximizes the likelihood of the observed data."
"probabilistic view","an approach that incorporates the probabilities of different outcomes based on data distribution."
"geometric intuitions","understanding and reasoning based on geometric concepts and principles."
"PCA","Principal Component Analysis; a dimensionality reduction technique that transforms data into a new coordinate system."
"loss function","a function that quantifies the difference between predicted and actual values; used to optimize models."
"empirical risk minimization","a principle in statistical learning theory that focuses on minimizing the average loss over a training set."
"hyperplane","a subspace of one dimension less than its ambient space, used in SVM to separate different classes of data." 
"binary classification","a type of classification task that involves categorizing data into one of two classes."
"Gram-Schmidt method","Constructively transform any basis of an n-dimensional vector space V into an orthogonal/orthonormal basis."
"Orthogonal basis","A set of vectors that are all orthogonal to each other."
"Orthonormal basis","An orthogonal basis where each vector has a length of 1."
"Span","The set of all linear combinations of a given set of vectors."
"Projection","The process of mapping a vector onto a subspace spanned by other vectors."
"Subspace","A vector space that is contained within another vector space."
"Normalize","To adjust a vector so that it has a length (or norm) of 1."
"ONB","Abbreviation for orthonormal basis."
"Iteratively constructs","Refers to the process of building a basis step by step."
"Hilbert Space Embeddings","A method used to represent probability measures in a Hilbert space, allowing for the application of geometric techniques to probability theory."
"Metrics on Probability Measures","Quantitative functions that define the distance between different probability measures."
"Loss Functions","Mathematical functions that quantify the difference between predicted values and actual values in statistical models."
"Risks","The expected loss associated with a particular model or prediction."
"Support Vector Machines","A supervised learning model that analyzes data for classification and regression analysis and finds the optimal hyperplane separating different classes."
"Fundamental Theorem of Linear Algebra","A theorem that relates the properties of linear transformations to the vector spaces and their dimensions."
"Numerical Analysis","A branch of mathematics that studies algorithms for approximating solutions to mathematical problems numerically."
"curved direction","the direction in which the geometry curves significantly in the context of optimization problems."
"least curved direction","the direction in which the geometry curves the least in optimization contexts."
"poorly conditioned problems","problems characterized by long, thin valleys that are very curved in one direction but flat in the other."
"preconditioner","a matrix or operator P designed to improve the condition number of another operator in mathematical problems."
"condition number","a measure of how the output value of a function can change for a small change in the input value."
"gradient descent","an iterative optimization algorithm for finding the minimum of a function."
"convergence","the property of an algorithm to reach a solution or a final value as iterations increase."
"curvature","a measure of how quickly a curve deviates from being a straight line; in optimization, it relates to the steepness and shape of the cost function landscape."
"gradient descent steps","the incremental adjustments made in the direction of the steepest descent on the optimization surface."
"Reproducing Kernel Hilbert Spaces","A mathematical framework where every function can be represented as a linear combination of kernel functions."
"Low Rank Approximation","A technique in linear algebra where a matrix is approximated by another matrix that has a lower rank."
"Stochastic Models","Mathematical models that incorporate random variables and processes."
"Estimation","The process of determining the values of parameters based on observed data."
"Control","In the context of systems, it refers to the methods used to manage the behavior of dynamic systems."
"Generalized Linear Models","A generalization of ordinary linear regression that allows for response variables to have error distribution models other than a normal distribution."
"Turbo Decoding","A decoding technique used in error correction coding for achieving reliable communication over noisy channels."
"Belief Propagation","An algorithm used in graphical models to compute marginal distributions."
"Fisher Discriminant Analysis","A statistical method used to find a linear combination of features that best separates two or more classes."
"Approximate Bayesian Inference","Methodologies for estimating the posterior distribution of parameters when exact inference is intractable."
"covariance matrix","a matrix whose elements represent the covariances between pairs of random variables."
"eigenvectors","non-zero vectors that only change by a scalar factor when a linear transformation is applied."
"PCA","a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of linearly uncorrelated variables."
"PPCA","a probabilistic version of PCA that includes a Gaussian noise model in its formulation."
"maximum likelihood estimate","a method of estimating the parameters of a statistical model that maximizes the likelihood function."
"expectation maximization (EM) algorithm","an iterative method used for finding maximum likelihood estimates in models with latent variables."
"dimensionality of latent variables","the number of hidden or unobserved variables that influence the observable data."
"jointly Gaussian distributed","2 = Bz of it are jointly Gaussian dis- distributed."
"marginals","We already know the marginals p(z) = NV(z|0, I) and p(x) = N (a |e, BB' + oI)."
"cross-covariance","The missing cross-covariance is given as Cov[«, z] = Cov,[Bz + pw] = BCov,[z,z] = B."
"probabilistic model","Therefore, the probabilistic model of PPCA, i.e., the joint distribution of latent and observed random variables is explicitly given by"
"posterior distribution","The joint Gaussian distribution p(x, z| B, u,o7) in (10.72) allows us to determine the posterior distribution p(z | x) immediately by applying the rules of Gaussian conditioning from Section 6.5.1."
"mean vector","with a mean vector of length D + M and a covariance matrix of size (D+M)x(D+M)."
"covariance matrix","with a mean vector of length D + M and a covariance matrix of size (D+M)x(D+M)."
"Gaussian conditioning","The joint Gaussian distribution p(x, z| B, u,o7) in (10.72) allows us to determine the posterior distribution p(z | x) immediately by applying the rules of Gaussian conditioning from Section 6.5.1."
"latent variable","The posterior distribution of the latent variable given an observation x is then p(z|x) =N(z|m, C)."
"Gaussian distribution","The posterior distribution of the latent variable given an observation x is then p(z|x) =N(z|m, C)."
"responsibility","represents the probability that x, has been generated by the kth mixture component."
"mixture components","the A mixture components are the parts that distribute probability mass among the data."
"soft assignment","refers to r,, as a method of assigning data points to mixture components based on their probabilities."
"probability mass","distributes probability among the mixture components."
"data point","refers to an individual entry in a dataset."
"responsibilities","are computed for each mixture component relative to the data point."
"sum of responsibilities","the total of all A responsibilities for a data point is 1."
"N,,","represents the sum of all entries of a column, giving an overview of responsibility across the mixture components."
"Eigenvector","The basis of the principal subspace obtained as the eigenvectors associated with the largest eigenvalues of the data covariance matrix."
"Eigenvalue","The values associated with the eigenvectors in the context of a data covariance matrix."
"Principal subspace","A subspace defined by the eigenvectors of a matrix, particularly highlighting its conceptual role in data analysis."
"Data covariance matrix","A matrix that contains the covariances between pairs of elements in a dataset."
"Eigendecomposition","A method to compute eigenvalues and eigenvectors of a matrix by decomposing it into parts."
"Singular value decomposition","A mathematical technique to factorize a matrix into its constituent parts, useful for finding eigenvalues and eigenvectors."
"Low-Rank Approximations","Approximations of a matrix that retain important features while reducing the number of dimensions."
"orthogonal","d y are orthogonal if and only if (x, y) = 0"
"unit vectors","if additionally ||a|| = 1 = |ly|l"
"orthonormal","then x and y are orthonormal"
"orthogonality","the generalization of the concept of perpendicularity to bilinear forms that do not have to be the dot product"
"inner product","geometrically, we can think of orthogonal vectors as having a right angle with respect to a specific inner product"
"angle","we are interested in determining the angle w between them using two different inner products"
"dot product","Using the dot product as the inner product yields an angle w between x and y of 90°"
"mean","a measure of central tendency, specifically the average of a set of values."
"covariance matrix","a matrix that represents the covariance between pairs of elements in a dataset."
"compressed representation","a low-dimensional representation or code that captures the essential information of a higher-dimensional dataset."
"projection matrix","a matrix that maps a vector onto a subspace."
"orthonormal","a property of vectors in which they are both orthogonal (perpendicular) and normalized (unit vectors)."
"subspace","a space that is a subset of a larger vector space, defined by certain dimensions."
"dimensionality","the number of coordinates needed to specify a point in a space, indicated as M < D."
"coordinates","numerical values that define a position within a given space with respect to a basis."
"basis vectors","a set of vectors that defines a coordinate system for a vector space." 
"loss due to compression","the difference or error between original data and its compressed form."
"epigraph","This resulting filled-in set, called the epigraph of the convex function, is a convex set."
"convex function","If a function f : R\" > R is differentiable, we can specify convexity in."
"linear program","Consider the linear program."
"unconstrained problem","The unconstrained problem (indicated by the contour lines) has a minimum on the right side."
"optimal value","The optimal value given the constraints are shown by the star."
"variables","Recall that d is the number of variables and m is the number of constraints in the primal linear program."
"constraints","Recall that d is the number of variables and m is the number of constraints in the primal linear program."
"objective function","The objective function is linear, resulting in linear contour lines."
"feasible region","The optimal value must lie in the shaded (feasible) region."
"least-squares solution","the solution of an overdetermined system, assuming the dot product as the inner product"
"overdetermined system","a system where there are more equations than unknowns"
"principal component analysis","a technique used to derive reconstruction errors"
"projections of vectors","the process of projecting a vector onto a subspace"
"subspace U","a subset of a vector space that is also a vector space"
"basis vectors","a set of vectors in a vector space that is linearly independent and spans the space"
"ONB","orthonormal basis, where basis vectors are orthogonal and of unit length"
"projection equation","an equation that gives the projection of a vector onto a subspace"
"computation time","the amount of time required to perform calculations"
"affine space","a geometric structure that generalizes the concepts of linear spaces"
"Machine Learning","A field of study that uses algorithms and statistical models to enable computers to perform tasks without explicit instructions."
"Geometry","Branch of mathematics concerned with questions of shape, size, relative position of figures, and properties of space."
"Reproducing Kernel Hilbert Spaces","A concept in functional analysis that allows the analysis of linear functionals and represents functions in a Hilbert space."
"Nonlinear Programming","A branch of mathematical optimization dealing with problems that are not linear in nature."
"Convex Optimization Theory","A theoretical framework addressing the optimization of convex functions over convex sets."
"Mathematical Statistics","The branch of mathematics dealing with the collection, analysis, interpretation, presentation, and organization of data."
"Linear Detection via Belief Propagation","A method in statistical inference that uses a message-passing algorithm to estimate the state of a probabilistic graphical model."
"Probability","A branch of mathematics concerned with the analysis of random phenomena."
"Neural Networks","Computational models inspired by human brain networks that are designed to recognize patterns."
"Bayesian PCA","A statistical approach to principal component analysis that incorporates Bayesian methods."
"Probabilistic Reasoning","A reasoning approach based on the principles of probability."
"Markov Random Fields","A collection of random variables having a Markov property described by a graph."
"Energy Minimization Methods","Techniques used to find the minimum energy configuration that corresponds to a certain problem."
"Smoothness-Based Priors","Prior assumptions in statistical modeling that promote smooth behaviors in the estimated functions."
"Change of Variable Theorem","A fundamental theorem in calculus for integrating functions with transformed variables."
"Fundamental Theorem of Calculus","Connects differentiation and integration, providing a way to evaluate definite integrals."
"Lebesgue Integral","A method of integration that generalizes the notion of area under a curve."
"Global Geometric Framework","A unified approach to understanding and visualizing geometric data structures."
"Nonlinear Dimensionality Reduction","Techniques for reducing the number of random variables while preserving relationships in data."
"Regression Selection and Shrinkage","Statistical methods for selecting variables and reducing their impact in a regression model."
"Lasso","A regression analysis method that performs both variable selection and regularization to enhance prediction accuracy."
"Probabilistic Principal Component Analysis","A statistical technique that combines principal component analysis and probabilistic modeling."
"angle" ,"the angle between x and b."
"unit circle" ,"if ||a|| = 1, then x lies on the unit circle."
"projection" ,"a projection is a linear mapping."
"projection matrix" ,"there exists a projection matrix P such that ty (a) = P,a."
"dot product" ,"with the dot product as inner product."
"inner product" ,"with the dot product as inner product."
"symmetric matrix" ,"bb' (and, consequently, P) is a symmetric matrix (of rank 1)."
"rank" ,"bb' (and, consequently, P) is a symmetric matrix (of rank 1)."
"scalar" ,"||b||² = (b, b) is a scalar."
"subspace" ,"the line through the origin with direction b (equivalently, the subspace U spanned by b)."
"kernel function","k(-,-) can be defined on any object, e.g., sets, sequences, strings, graphs, and distributions."
"SVM","Support Vector Machine; a supervised learning algorithm used for classification and regression."
"optimal solution","the best possible outcome determined by the algorithms applied to the SVM."
"loss function","a method used to evaluate how well a specific algorithm models the given data."
"unconstrained optimization problem","an optimization problem with no constraints on the variables."
"constrained versions","approaches that involve restrictions on the values that the variables can take."
"primal SVM","the original formulation of the Support Vector Machine optimization problem."
"dual SVM","a reformulation of the Support Vector Machine optimization problem that can sometimes be easier to solve."
"quadratic programs","mathematical optimization problems that can be expressed in a standard form involving quadratic functions."
"convex","a property of a function where any line segment joining two points on the graph of the function does not lie below the graph."
"hinge loss","a specific loss function predominantly used for "maximum-margin" classification, particularly in SVMs."
"model choice","a selection process regarding the type of model to use for regression analysis."
"Gaussian noise","a type of statistical noise characterized by a bell-shaped curve in the distribution."
"Linear Regression","a statistical method used to model the relationship between a dependent variable and one or more independent variables."
"time-series analysis","a method used to analyze time-ordered data points to identify trends or patterns."
"system identification","the process of developing or improving a mathematical representation of a physical system using measured data."
"reinforcement learning","a machine learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward."
"forward/inverse model learning","processes involved in predicting future states (forward) or deducing inputs that lead to a particular outcome (inverse)."
"optimization","the process of making something as effective or functional as possible."
"line searches","a method used in optimization to find a minimum or maximum of a function along a given direction."
"global optimization","a process that finds the best solution among all possible solutions for a given problem."
"deep learning","a subset of machine learning that uses neural networks with many layers to analyze various forms of data."
"classification algorithms","methods used to assign categories to data points based on their characteristics."
"regression function","a function used in regression analysis to model the relationship between variables."
"parametrization","the act of defining parameters for a function or model to constrain it in a specific way."
"polynomials","mathematical expressions involving variables raised to various powers combined using addition, subtraction, and multiplication."
"degree of the polynomial","the highest power of the variable in a polynomial expression."
"model selection","the procedure of comparing different models to determine which one best explains the data."
"Convex Analysis","A branch of mathematics that studies the properties of convex sets and convex functions."
"EM Algorithms","Algorithms used for finding maximum likelihood estimates in statistical models, especially when the model depends on unobserved variables."
"PCA","Principal Component Analysis, a technique used for dimensionality reduction by transforming to a new set of variables that are orthogonal."
"SPCA","Sparse Principal Component Analysis, an extension of PCA that seeks to find sparse representations."
"Linear Gaussian Models","Statistical models that assume a linear relationship between variables and that the noise is normally distributed."
"Linear Algebra","A branch of mathematics concerning linear equations, linear functions, and their representations through matrices and vector spaces."
"Matrix Analysis","The study of matrices and their algebraic properties, as well as their applications in various fields."
"Monte Carlo Method","A computational technique that uses random sampling to obtain numerical results for mathematical problems."
"Spline Models","Models used in statistical data analysis for smoothing data points by constructing a piecewise polynomial function."  
"Probability","A measure of the likelihood that an event will occur, expressed as a number between 0 and 1."  
"Nonparametric Statistics","A branch of statistics that does not assume a specific distribution for the population from which the samples are drawn."  
"Expectation","The expected value of a random variable, representing the average outcome if an experiment were repeated many times."  
"Tidy Data","A data structure format that organizes datasets to facilitate analysis, where each variable is in a column, each observation is in a row, and each type of observational unit forms a table."  
"Decision Trees","A flowchart-like structure used for decision-making and predictive modeling that maps observations about an item to conclusions about its target value."  
"Naive Bayesian Classifiers","A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features."  
"Representer Theorem","A result in statistical learning theory that provides conditions under which a solution to a regularized empirical risk minimization problem can be expressed as a linear combination of the training data."  
"Infinite Networks","Structures in computing that involve components with connections that may lead to indefinitely large or complex interactions."
"convex conjugates","calculates their convex conjugates (Rifkin and Lippert, 2007)."
"functional analysis","view (also the regularization methods view) of SVMs are referred to the work by Wahba (1990)."
"kernels","Theoretical exposition of kernels (Aronszajn, 1950; Schwartz, 1964; Saitoh, 1988; Manton and Amblard, 2015) requires a basic grounding in linear operators."
"linear operators","requires a basic grounding in linear operators (Akhiezer and Glazman, 1993)."
"Banach spaces","The idea of kernels have been generalized to Banach spaces (Zhang et al., 2009)."
"Krein spaces","The idea of kernels have been generalized to Krein spaces (Ong et al., 2004; Loosli et al., 2016)."
"hinge loss","the hinge loss has three equivalent representations, as shown in (12.28) and (12.29)."
"constrained optimization problem","as well as the constrained optimization problem in (12.33)."
"SVM loss function","comparing the SVM loss function with other loss functions (Steinwart, 2007)."
"subgradients","convenient for computing subgradients, as each piece is linear."
"International Symposium on Information Theory","A conference focused on the principles and applications of information theory."
"Conjugate Gradient Method","An algorithm for solving systems of linear equations, particularly useful for large systems."
"Normalized Cuts","A graph partitioning method used for image segmentation."
"Image Segmentation","The process of partitioning a digital image into multiple segments to simplify representation."
"Hash Kernels","A method used in machine learning to measure the similarity between data structures in a high-dimensional space."
"Structured Data","Data that is organized in a defined manner, often in tables or databases."
"Probability","A branch of mathematics dealing with the likelihood of an event's occurrence."
"Minimization Methods","Techniques used to find the minimum value of a function, especially for non-differentiable functions."
"Texton","A basic element or unit in the appearance of a texture, used in image processing."
"Object Recognition","The ability of a system to identify objects in images or video."
"Bayes Factor","A statistic used in Bayesian analysis to compare the predictive power of two hypotheses."
"Line and Planes of Closest Fit","Mathematical concepts used to determine the best-fitting line or plane for a given set of data points in space."
"Causal Inference","A field of study that involves determining the cause-effect relationships from data, often using models and algorithms."
"Matrix Cookbook","A comprehensive resource that provides techniques and formulas related to matrices and their applications."
"Probabilistic Outputs","Outputs generated by models like Support Vector Machines, which estimate probabilities of class memberships instead of binary outcomes."
"Measure Theoretic Probability","A mathematical framework for probability that utilizes measures and integrates with measure theory principles."
"Legendre Transformation","A mathematical operation used in optimization that transforms one set of variables into another, often used to change the perspective of a problem."
"Numerical Recipes","A collection of algorithms and techniques for numerical computing and problem-solving in scientific computing."
"standard basis","The standard basis refers to a set of vectors that are used as the foundation for a vector space, typically represented as e1 and e2."
"determinant","The determinant is a function that measures the signed volume formed by column vectors composed in a matrix."
"volume","Volume is the measure of space occupied by a three-dimensional object, calculated as the absolute value of the determinant of a matrix formed by vectors spanning the object."
"parallelepiped","A parallelepiped is a solid with faces that are parallel parallelograms."
"linearly independent vectors","Linearly independent vectors are vectors that do not lie in the same plane and cannot be expressed as a linear combination of each other."
"matrix","A matrix is a rectangular array of numbers or functions arranged in rows and columns."
"upper-triangular matrix","An upper-triangular matrix is a type of matrix where all the entries below the main diagonal are zero."
"lower-triangular matrix","A lower-triangular matrix is a type of matrix where all the entries above the main diagonal are zero."
"signed volume","Signed volume is the value that represents the volume of a geometric object, with a sign that indicates its orientation."
"kernel trick","A technique used in machine learning to transform data into a higher-dimensional space to make it easier to perform linear separation."
"kernel PCA","A variant of Principal Component Analysis that uses kernel methods to find principal components in a higher-dimensional feature space."
"deep auto-encoder","A type of artificial neural network used for unsupervised learning that aims to learn efficient representations of data."
"Gaussian process","A collection of random variables, any finite number of which have a joint Gaussian distribution, used in Bayesian statistics."
"latent-variable","An unobserved variable that influences observed variables in a statistical model."
"model","A mathematical representation of a system or process."
"GP-LVM","Gaussian Process Latent Variable Model; a generative model that combines Gaussian processes with latent variables."
"Bayesian GP-LVM","A Bayesian approach to Gaussian Process Latent Variable Models, incorporating uncertainty in the estimates."
"Gaussian Mixture Model","A probabilistic model that assumes that all data points are generated from a mixture of several Gaussian distributions."
"mixture models","A statistical model that represents the presence of subpopulations within an overall population as a combination of multiple distributions."
"convex combination","A linear combination of items where the coefficients are non-negative and sum up to one."
"mixture weights","Coefficients in a mixture model that determine the contribution of each base distribution to the overall model."
"boundary","The limit or dividing line of a set."
"convex hull","The smallest convex set that contains all the points in a given set."
"positive examples","Data points that belong to the class of interest."
"negative examples","Data points that do not belong to the class of interest."
"distance","The length of the difference vector between two points."
"convex set","A set in which, for any two points within the set, the line segment connecting them also lies entirely within the set."
"convex combination","A linear combination of points where the coefficients are non-negative and sum to one."
"weights","Non-negative values assigned to points in a convex combination."
"generative process",""
"generator",""
"geometric multiplicity",""
"Givens rotation",""
"global minimum",""
"GP-LVM",""
"gradient",""
"Gram matrix",""
"Gram-Schmidt orthogonalization",""
"graphical model",""
"group","a mathematical structure consisting of a set equipped with an operation that combines any two elements to form a third element."
"Hadamard product",""
"hard margin SVM",""
"Hessian",""
"Hessian eigenmaps",""
"Hessian matrix",""
"hinge loss",""
"histogram",""
"hyperparameter",""
"hyperplane",""
"hyperprior",""
"Lid.",""
"CA",""
"identity automorphism",""
"identity mapping",""
"identity matrix",""
"image",""
"independent and identically distributed",""
"independent component analysis",""
"inference network",""
"injective",""
"inner product","an operation that takes two sequences of numbers and produces a single number."
"inner product space",""
"intermediate variables",""
"inverse",""
"inverse element",""
"invertible",""
"Isomap",""
"isomorphism",""
"Jacobian",""
"Jacobian determinant",""
"Jeffreys-Lindley paradox",""
"Jensen’s",""
"kernel","comes from the idea of the reproducing kernel Hilbert space (RKHS)"
"hyperplane","solving for hyperplanes, that is, the hypothesis class of functions are still linear"
"non-linear surfaces","due to the kernel function"
"kernel in linear algebra","the kernel is another word for the null space"
"smoothing kernel","the smoothing kernel in kernel density estimation"
"kernel representation","mathematically equivalent to the kernel representation k(a;,a,;)"
"inner product","computed more efficiently than the inner product"
"Computational Thinking","Foundations of Data Science."
"Inferential Thinking","Foundations of Data Science."
"Geometric View","Conjugate Priors."
"Categorical Data Analysis","A statistical methodology."
"Statistical Model Identification","A method for identifying statistical models."
"Linear Operators","Operators acting on a vector space."
"Hilbert Space","A complete inner product space."
"Machine Learning","A field of study that gives computers the ability to learn."
"Information Geometry","A field studying geometric structures on probability distributions."
"Representer Theorems","Theorems providing conditions under which certain estimates can be expressed."
"Reproducing Kernels","Kernels associated with reproducing kernel Hilbert spaces."
"Linear Algebra","The branch of mathematics concerning linear equations."
"average squared reconstruction error","is equivalent to minimizing the projection of the data covariance matrix onto the orthogonal complement of the principal subspace."
"data covariance matrix","the matrix representing how much the dimensions vary from the mean with respect to each other."
"orthogonal complement","the set of vectors that are perpendicular to a given subspace."
"principal subspace","the subspace that contains the most variance of the data."
"variance","a measure of the dispersion of a set of data points."
"PCA","Principal Component Analysis, a statistical procedure that transforms data to a new coordinate system."
"dimensionality reduction","the process of reducing the number of random variables under consideration, obtaining a set of principal variables."
"projection matrix","a matrix that transforms a vector onto a subspace while preserving its direction within that subspace."
"eigenvector","a non-zero vector that changes at most by a scalar factor when a linear transformation is applied."
"eigenvalue","a scalar associated with an eigenvector that represents how much the eigenvector is stretched or compressed during a linear transformation."
"orthogonal projections","a type of projection where the vector being projected is perpendicular to the subspace onto which it is projected."
"lower-dimensional subspaces","subsets of a higher-dimensional space that have fewer dimensions."
"ordered basis","a sequence of vectors in a vector space that are linearly independent and span the space."
"linear combination","a mathematical expression constructed from a set of terms by multiplying each term by a constant and adding the results."
"displacement vector","the vector that represents the difference between the original vector and its projection onto a subspace."
"span","the set of all linear combinations of a given set of vectors."
"inequality","239"
"joint probability","178"
"Karhunen-Loéve transform","318"
"kernel","33, 47, 58, 254, 388"
"kernel density estimation","369"
"kernel matrix","389"
"kernel PCA","347"
"kernel trick","316, 347, 389"
"label","253"
"Lagrange multiplier","234"
"Lagrangian","234"
"Lagrangian dual problem","234"
"Laplace approximation","170"
"Laplace expansion","102"
"Laplacian eigenmaps","136"
"LASSO","303, 316"
"latent variable","275"
"law","177, 181"
"law of total variance","203"
"leading coefficient","30"
"least-squares loss","154"
"least-squares problem","261"
"least-squares solution","88"
"left-singular vectors","119"
"Legendre transform","242"
"Legendre-Fenchel transform","242"
"length","71"
"likelihood","185, 265, 269, 291"
"line","61, 82"
"linear combination","40"
"linear manifold","61"
"linear mapping","48"
"linear program","239"
"linear subspace","39"
"linear transformation","48"
"linearly dependent","40"
"linearly independent","40"
"link function","272"
"loading","322"
"local minimum","225"
"log-partition function","211"
"logistic regression","315"
"logistic sigmoid","315"
"loss function",""
"dot product","the inner product on Euclidean vector space."
"matrix","an N by N matrix where the elements of the diagonal are from y."
"kernel matrix","a matrix such that each entry is K;; = k(x;, x;)."
"feature representation","an explicit feature representation «; then we define K;; = (x;,a;)."
"diag","a matrix with zeros everywhere except on the diagonal, where we store the labels, that is, Y = diag(y)."
"dual SVM","can be written as min 30 YKYa —1)\,e@ "
"inequality constraints","the standard forms of the constraints to be inequality constraints."
"equality constraints","will express the dual SVM’s equality constraints."
"optimize","an objective function (likelihood or posterior)"
"maximum likelihood","a method for estimating the parameters of a statistical model"
"MAP estimation","Maximum A Posteriori estimation; a method for estimating parameters by incorporating prior knowledge"
"overfitting","a modeling error that occurs when a model is too complex and captures noise instead of the underlying distribution"
"Bayesian linear regression","a statistical method using Bayesian inference to estimate the posterior distribution of model parameters"
"Bayesian inference","a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available"
"posterior distribution","the probability distribution that represents the updated beliefs about the unknown parameters after observing the data"
"point estimate","a single value estimate of a parameter"
"prior","prior distribution; a probability distribution that represents the uncertainty of a parameter before observing data"
"regularizer","a technique in statistical modeling that is applied to discourage overly complex models"
"Machine Learning","A field of study that gives computers the ability to learn without being explicitly programmed."
"System Identification","The process of developing or improving a mathematical representation of a physical system."
"Learning SVM","Support Vector Machine is a supervised learning model used for classification and regression analysis."
"Optimization","The action of making the best or most effective use of a situation or resource."
"Bayesian Interpolation","A statistical method used to estimate unknown values based on known data, incorporating Bayesian principles."
"Gaussian Processes","A collection of random variables, any finite number of which have a joint Gaussian distribution."
"Information Theory","The study of quantification, storage, and communication of information."
"Matrix Differential Calculus","A branch of mathematics involving the differentiation of matrix functions."
"Statistics","The science of collecting, analyzing, interpreting, presenting, and organizing data."
"Econometrics","The application of statistical methods to economic data for testing hypotheses and forecasting."
"convex functions","functions for which the line segment joining any two points on the graph lies above the graph itself."
"Jensen’s inequality","an inequality for nonnegative weighted sums of convex functions."
"closure property","the property that the sum or weighted sum of convex functions is also convex."
"nonnegative weighted sums","sums where each function in the sum is multiplied by a nonnegative weight."
"mixture model","A model that represents a distribution as a combination of multiple individual distributions."
"mixture weight","The coefficients used to determine the contribution of each component in a mixture model."
"Gaussian mixture model","A probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions."
"Gaussian mixture distribution","A distribution that is composed of a convex combination of Gaussian distributions, providing more expressiveness."
"weighted Gaussian components","The individual Gaussian distributions in a mixture model, each multiplied by a mixture weight."
"eigenvalues","the eigenvalues of S are the squared singular values of X."
"singular values","the only nonzero entries are the singular values."
"SVD","the SVD of X is given by X = U Ev V."
"orthogonal matrices","U and V are orthogonal matrices."
"eigenvectors","the columns of U are the eigenvectors of S."
"variance view","provides the connection between the maximum variance view and the singular value decomposition."
"normalized probability vector","a probability vector that sums to 1."
"mean values","the average of a set of values."
"GMM density","Gaussian Mixture Model density, representing a probability distribution."
"updating the mean values","the process of adjusting the mean values based on new data."
"mixture component","an individual Gaussian distribution within a mixture model."
"regime of the data","the characteristic distribution or pattern exhibited by the data."
"ear model", "y = 0(Az + b), where A is a weight matrix and b a bias vector, we identify this generalized linear model as a single-layer neural network with activation function o(-)."
"generalized linear model", "Generalized linear models are the building blocks of deep neural networks."
"logistic regression", "A method used in statistical classification."
"logistic sigmoid", "A type of activation function used in neural networks."
"transfer function", "A function that maps the input of a system to its output."
"activation function", "A mathematical function applied to a node in a neural network that determines the output."
"canonical link function", "A function that relates the mean of the response variable to the linear predictors."
"posterior","used to obtain an approximate posterior (Bishop, 2006)"
"binary classification","A task of predicting one of two outcomes."
"class","Input example x, may also be referred to as inputs, data points, features, or instances."
"probabilistic models","Mathematically convenient to use {0, 1} as a binary representation."
"discrete outcomes","A small number of outcomes without additional structure."
"principal subspace","a subspace formed by the principal components that captures the structure of the data."
"principal components (PCs)","the directions in the feature space along which the data varies the most."
"reconstruction","the process of approximating the original data from a compressed form."
"average squared reconstruction error","a measure of the average error between the original data and its reconstruction, calculated by squaring the differences."
"compression loss","the loss of original data information when compressing data into a lower-dimensional space."
"Lagrange multipliers","> 0 corresponding to each inequality constraint respectively."
"Lagrangian duality","The idea of converting an optimization problem in one set of variables x into another optimization problem in a different set of variables X."
"primal variables","The set of variables x in the optimization problem."
"dual variables","The different set of variables X in the dual optimization problem."
"primal problem","The problem min f(x) subject to g(a) <0 for all i=1,...,m."
"Lagrangian dual problem","The associated problem derived from the primal problem."
"PCA","a linear relationship between the original data and its low-dimensional code z for data compression."
"B","a suitable matrix used in the linear relationship in PCA."
"z","the lower-dimensional representation of the compressed data in PCA."
"x","the original data in the PCA context."
"encoder","an operation that encodes the original data a as a low-dimensional code z."
"decoder","an operation that maps the low-dimensional code z back into the original data space R²."
"MNIST digits dataset","the dataset used throughout the chapter for demonstration in PCA."
"low-dimensional code","a compressed representation of the original data a in PCA."
"PCA","an algorithm for linear dimensionality reduction."
"dimensionality reduction","a technique used to reduce the number of features or dimensions in a dataset."
"data compression","the process of encoding information using fewer bits."
"data visualization","the representation of data in a visual context to make information more accessible."
"latent factors","underlying variables that can explain patterns in observed data."
"high-dimensional data","data that has a large number of features or variables."
"principal components","The importance of the principal components drops off rapidly."
"variance","The most of the variance of the projected data is captured by only a few principal components."
"maximum-variance","We derived PCA without any notion of a probabilistic model using the maximum-variance."
"projection perspective","We derived PCA... using the projection perspectives."
"probabilistic model","A probabilistic model would offer us more flexibility and useful insights."
"Empirical Mean","The arithmetic average of the observations for each variable."
"Empirical Covariance Matrix","A D x D matrix defined to compute the covariance of observations."
"Symmetric Matrix","A matrix that is equal to its transpose."
"Positive Semidefinite","A property of matrices where all eigenvalues are non-negative."
"Variance","The expectation of the squared deviation of a random variable."
"Population Variance","Variance calculated using integrals rather than empirical observations."
"dual" "the idea of duality from Section 7.2, without considering constraints."
"convex set" "a set that can be equivalently described by its supporting hyperplanes."
"hyperplane" "a supporting hyperplane of a convex set if it intersects the convex set, and the convex set is contained on just one side of it."
"epigraph" "the convex function filled up to obtain a convex set."
"tangent" "the supporting hyperplane just touches the convex function, and is in fact the tangent to the function at that point."
"gradient" "the evaluation of the gradient of that function at that point."
"auto-encoder","An auto-encoder encodes the data x, € R” to a code z, € R“ and decodes it to a x, similar to x,,."
"code","The code is a compressed version of the original data."
"encoder","The mapping from the data to the code is called the encoder."
"decoder","The mapping from the code back to the original data space is called the decoder."
"principal subspace","The decoded vector is the orthogonal projection of the original data onto the M-dimensional principal subspace."
"dimensionality reduction","PCA can be viewed as a linear auto-encoder that encodes high-dimensional data into a lower-dimensional representation."
"GMM","Gaussian Mixture Model, a probabilistic model used for density estimation and clustering.",
"Density Estimation","The process of constructing an estimate of the probability density function of a random variable.",
"Negative log-likelihood","A measure used in statistical modeling, representing the logarithm of the probability of the observed data given a statistical model.",
"EM algorithm","Expectation-Maximization algorithm, a statistical technique used for finding maximum likelihood estimates in models with latent variables.",
"mixture components","The individual probability distributions that make up a Gaussian Mixture Model.",
"iterations","Repetitions of a process in algorithms, specifically indicating the steps taken in the EM algorithm to refine estimates."
"Latent Variable" ,"A variable that is not directly observed but is inferred from other variables that are observed. "
"PPCA Model" ,"Probabilistic Principal Component Analysis model that assumes a lower-dimensional latent cause for high-dimensional observations."
"Bayesian Inference" ,"A statistical method that applies Bayes' theorem for updating the probability of a hypothesis as more evidence or information becomes available."
"PCA" ,"Principal Component Analysis, a technique used to reduce the dimensionality of data while preserving as much variance as possible."
"Latent Coordinates" ,"Coordinates in a lower-dimensional latent space that represent the underlying structure of high-dimensional data."
"Generated Data" ,"Data created from latent variables that replicate or resemble observed data."
"Gaussian mixture model","A density model where we combine a finite number of K Gaussian distributions."
"likelihood","A statistical measure of how well a model explains the observed data."
"maximum likelihood","A method used to estimate the parameters of a statistical model."
"dependent simultaneous equations","A set of equations that need to be solved together due to their interrelated variables."
"iteratively","A process of solving equations by repeating steps until a desired level of accuracy is achieved."
"Gaussian distributions","Probability distributions that are symmetric and defined by their mean and variance."
"K","The finite number of Gaussian distributions in a Gaussian mixture model."
"parameters","The variables that define the specific form of a probability distribution."
"convex optimization problem","A problem that seeks to minimize a convex function subject to convex constraints."
"linear programming","A special case of convex optimization where all functions are linear."
"linear program","An optimization problem with linear objective function and linear constraints."
"Lagrangian","A function that combines the objective function and the constraints via Lagrange multipliers."
"Lagrange multipliers","Values that scale the constraints in optimization, used in the Lagrangian."
"dual Lagrangian","The Lagrangian function expressed in terms of the dual variables."
"variables","The quantities in an optimization problem whose values are to be determined."
"constraints","Conditions that the solutions to the optimization problem must satisfy."
"mean function","the posterior mean function is identical to the MAP estimate."
"Bayesian linear regression","a statistical method that incorporates prior beliefs and updates them with observed data."
"posterior distributions","distributions that represent the probabilities of an unknown quantity after taking into account the observed data."
"maximum likelihood estimate (MLE)","a statistical estimate that maximizes the likelihood function."
"MAP estimate (MAP)","the mode of the posterior distribution, representative of the most probable value of an unknown parameter."
"predictive confidence bounds","intervals that contain a specified percentage of the predictive distribution for future observations."
"marginal uncertainties","the uncertainties in predictions that account for the variability in the model parameters."
"product of two Gaussian random variables","an (unnormalized) Gaussian distribution"
"linear transformation of a Gaussian random variable","Gaussian distributed"
"mean","the expected value of a random variable"
"covariance matrix","a matrix representing the covariances between pairs of random variables"
"marginal likelihood","the likelihood of the observed data under a statistical model, marginalizing over latent variables"
"affine transformations of random variables","a transformation that involves a linear mapping followed by a translation"
"i.i.d. random variables","independent and identically distributed random variables"
"covariance","a measure of how much two random variables change together"
"normalizing constant","a constant used to ensure that a probability distribution sums to one"
"standardization","undo the standardization and multiply by the standard deviation before adding the mean"
"PCA","apply PCA to the MNIST digits dataset"
"MNIST digits","dataset contains 60,000 examples of handwritten digits 0 through 9"
"image","each digit is an image of size 28 x 28"
"pixels","it contains 784 pixels"
"vector","interpret every image in this dataset as a vector x € IR”**"
"principal subspace","determine the principal subspace as detailed in this chapter"
"projection matrix","used the learned projection matrix to reconstruct a set of test images"
"hot encoding","at random according to p(z) = 7;"
"probabilistic model","defined by the joint distribution of the data and the latent variables"
"joint distribution","p(x, Ze = 1) = p(w | ze = 1)p(ze = 1)"
"latent variables","samples of random variables depend on samples from the variable’s parents in the graphical model"
"ancestral sampling","sampling, where samples of random variables depend on samples from the variable’s parents in the graphical model"
"prior distribution","we share the same prior distribution 7 across all latent variables z,."
"latent variable","A variable that is not observed directly but is inferred from the model and is used to describe the data-generating process."
"model parameters","Parameters that define the model explicitly, influencing its structure and outcomes."
"random variable","A variable that can take on different values, each with an associated probability."
"maximum likelihood","A statistical method used to estimate parameters of a model by maximizing the likelihood function."
"eigenvector","converges to the eigenvector associated with the largest eigenvalue of S."
"eigenvalue","the largest eigenvalue of S."
"Google PageRank algorithm","algorithm for ranking web pages based on their hyperlinks."
"PCA","Principal Component Analysis, requires computation of the data covariance matrix."
"covariance matrix","a D x D matrix used in PCA."
"eigenvalues","values associated with the eigenvectors of a matrix, computationally expensive to compute."
"eigenvectors","vectors that correspond to the eigenvalues of a matrix."
"eigendecomposition","process of decomposing a matrix into its eigenvalues and eigenvectors."
"dimensions","the number of distinct coordinates needed to specify a point in a mathematical space." 
"data points","observations in the dataset, represented as a centered dataset."
"variable","indicates whether the ‘th mixture component generated that data point"
"mixture component","a component in a probabilistic model that represents a distribution of data points"
"likelihood","the probability of the observed data given a model"
"latent variable","a variable that is not directly observed but is inferred from the observed data"
"marginalize","to sum out a variable from a joint distribution"
"probabilistic model","a model that incorporates randomness and uncertainty in its predictions"
"joint distribution","the probability distribution that models two or more random variables simultaneously"
"posterior covariance","does not depend on the observed data"
"posterior distribution","distribution of the corresponding latent variable z,."
"covariance matrix","allows us to assess how confident the embedding is."
"determinant","measures volumes"
"latent embedding","z, is fairly certain."
"posterior distribution","p(z. | a,.) with much variance"
"outlier","we may be faced with an outlier."
"generative process","underlying PPCA, which allows us to explore the posterior distribution"
"latent variable","z,.~ p(z |x.) from the posterior distribution"
"parameters","based on training data."
"predictor","a function."
"probabilistic models","a section described in Section 8.3."
"empirical risk minimization","the idea originally popularized by the support vector machine."
"support vector machine","described in Chapter 12."
"learning","the question being asked without explicitly constructing probabilistic models."
"design choices","four main choices covered in following subsections."
"training data","data used to measure how well the predictor performs."
"unseen test data","data on which predictors perform well."
"posterior distribution","a distribution that reflects the updated beliefs about a parameter after observing evidence."
"parameters","quantities that define the characteristics or behavior of a model."
"low-order polynomials","polynomials of lower degree, which typically have simpler structures."
"higher-order polynomial","a polynomial of higher degree, which can model more complex relationships."
"MAP estimate","the maximum a posteriori estimate, which provides the mode of the posterior distribution."
"Bayesian linear regression","a statistical method that applies Bayesian principles to linear regression modeling."
"marginal likelihood","the probability of the observed data under a specific statistical model, integrated over all possible values of the parameters."
"T","Contains eigenvectors of the data covariance matrix"
"A","Diagonal matrix with the eigenvalues associated with the principal axes on its diagonal"
"R","An arbitrary orthogonal matrix"
"Maximum likelihood solution","Unique up to an arbitrary orthogonal transformation"
"Singular value decomposition","A mathematical technique described in Section 4.5"
"Maximum likelihood estimate","Sample mean of the data"
"Maximum likelihood estimator","Average variance in the orthogonal complement of the principal subspace"
"Observation noise variance","Average leftover variance not captured by the first / principal components"
"Gaussian distribution","A statistical distribution characterized by its bell-shaped curve, defined by its mean and standard deviation."
"independent and identically distributed (i.i.d.)","A condition where a set of random variables are all drawn from the same probability distribution and are mutually independent."
"likelihood","The probability of a particular outcome, given a statistical model and its parameters."
"maximum likelihood estimate","A method of estimating the parameters of a statistical model by maximizing the likelihood function."
"MAP estimate","Maximum a posteriori estimate; a method in which the mode of the posterior distribution is used as an estimate for the parameter."
"N","Partial Derivative"
"The derivative of a function with respect to one variable while keeping other variables constant."

"Optimality Condition","A condition that must be satisfied for a solution to be considered optimal."

"N","Probability Vector"
"A vector with entries that represent probabilities and sum to one."

"Theorem 11.2","The theorem referenced in the text, likely related to the update rule derived."

"Update Rule","A method for adjusting parameters based on new information or calculations."

"Importance-Weighted Expected Value","An expectation where outcomes are weighted by their importance, in this case related to variance updates."

"Variance","A measure of how much values in a dataset differ from the mean."
"expected value","The expected value of a function g : R > R of a univariate continuous random variable X ~ p(x) is given by Ex[g(z)| = [ ate\pla\ae."
"discrete random variable","The expected value of a function g of a discrete random variable X ~ p(x) is given by Ex(9(z)] = > g(@)p(e)."
"random variable","A variable whose possible values are outcomes of a random phenomenon."
"exponential family","A useful family of distributions where the statistics of the random variable capture all possible information."
"probability distributions","To describe properties of probability distributions (expected values and spread)."
"Negative log-likelihood","as a function of the algorithm converges and returns this GMM."
"EM algorithm","applied to the GMM from."
"GMM","Gaussian Mixture Models."
"mixture weights","updated in a GMM."
"Density Estimation","with Gaussian Mixture Models."
"weight parameter updates","the mixture weights are updated as follows."
"mixture weights","prior to updating the mixture weights."
"mixture weights","after updating the mixture weights."
"means and variances","retaining the means and variances."
"Multivariate random variable","A random variable that has multiple dimensions or components."
"Variance","Describes the relation between individual dimensions of a random variable."
"Covariance","A measure of how much two random variables change together."
"Covariance matrix","A matrix that contains the covariances between pairs of random variables."
"Symmetric matrix","A matrix that is equal to its transpose."
"Positive semidefinite","A property of a matrix that indicates it does not have negative eigenvalues."
"Marginals","The variances of the individual components within a multivariate distribution."
"orthogonal projection","the projection of a vector onto a subspace such that the difference between the vector and its projection is orthogonal to that subspace."
"subspace","a mathematical space that is a subset of a larger space, which follows the same properties."
"coordinates","a set of values that define a point in a given coordinate system."
"orthonormal basis","a basis consisting of vectors that are both orthogonal and normalized to unit length."
"principal subspace","the subspace that contains the most significant features of the data, typically found using techniques like PCA."
"dimensional","relating to the number of dimensions in a space or a subspace."
"basis vectors","vectors that define a vector space through linear combinations."
"Bayes’ theorem","allows us to invert the relationship between x and y given by the likelihood."
"posterior distribution","the quantity of interest as it encapsulates all available information from the prior and the data."
"statistic of the posterior","such as the maximum of the posterior."
"loss of information","focusing on some statistic of the posterior leads to loss of information."
"decision-making system","the posterior can be used within a decision-making system."
"model-based reinforcement learning","the context in which Deisenroth et al. (2015) show that using."
"multivariate random variables","a finite vector of univariate random variables [X),...,X pli."
"expected value","the integral with respect to the probability density (for continuous distributions) or the sum over all states (for discrete distributions)."
"mean","a special case of the expected value, obtained by choosing g to be the identity function."
"t-variable","latent variable z can attain only a finite set of values."
"latent variable","PCA, where the latent variables were continuous-valued numbers in R™."
"probabilistic perspective","justifies some ad hoc decisions made in the previous sections."
"posterior probabilities","concrete interpretation of the responsibilities."
"iterative algorithm","updating the model parameters derived in a principled manner as the EM algorithm for maximum likelihood parameter estimation."
"EM algorithm","maximum likelihood parameter estimation in latent-variable models."
"generative process","the process that allows us to generate data, using a probabilistic model."
"mixture model","assume a mixture model with kK components."
"binary indicator","used to signify that a data point can be generated by exactly one mixture component."
"jective function","a function that summarizes the performance of a model on the training set by calculating the total loss for all examples."
"convex conjugate","a function derived from another function that provides insights into its properties, particularly in optimization."
"dot product","an algebraic operation that takes two equal-length sequences of numbers and returns a single number, often used in the context of vectors."
"Legendre transform","a mathematical transformation relating two sets of variables, typically used in physics to switch between energy representations."
"Legendre-Fenchel transform","a specific case of the Legendre transform that applies to convex functions."
"dual optimization problem","an optimization problem derived from the primal problem that seeks to find its maximum or minimum in a different variable space."
"quadratic programming","a type of mathematical optimization problem where the objective function is quadratic and the constraints are linear."
"multipliers","corresponding to the inequality constraints in (7.28) to be non-negative"
"La-Grange multipliers","corresponding to the equality constraints unconstrained"
"Convex Optimization","a particularly useful class of optimization problems, where we can guarantee global optimality"
"convex function","when f(-) is a convex function"
"convex sets","when the constraints involving g(-) and h(-) are convex sets"
"strong duality","The optimal solution of the dual problem is the same as the optimal solution of the primal problem"
"convex set","a set C is a convex set if for any x, y € C and for any scalar 6 with 0 < 6 < 1"
"straight line","connecting any two points in a convex set"
"variance","the variance of the measurement noise: When we predict noisy function values, we need to include o? as a source of uncertainty, but this term is not needed for noise-free predictions."
"parameter posterior","the only remaining uncertainty stems from the parameter posterior."
"distribution over functions","the fact that we integrate out the parameters @ induces a distribution over functions."
"mean function","the mean function, i.e., the set of all expected function values Eo[f(-)|0,%,¥], of this distribution over functions is m}-¢(-)."
"variance of the function","the (marginal) variance, i.e., the variance of the function f(-), is given by $'()Sv(-)-."
"Bayesian linear regression","Let us revisit the Bayesian linear regression problem with polynomials of degree 5."
"parameter prior","We choose a parameter prior p(@) = \’(0, +1)."
"orthogonal projection","the projection of a vector onto a subspace that is perpendicular to that subspace."
"margin","the distance of the separating hyperplane to the closest examples in the dataset, assuming the dataset is linearly separable."
"separating hyperplane","a hyperplane used to separate different classes in a dataset."
"linearly separable","a term describing a dataset that can be perfectly separated by a hyperplane."
"scale","a method or system of measurement, relevant here to how distance is measured."
"hyperplane","a flat affine subspace of one dimension less than its ambient space, commonly used in classification tasks."
"example","a specific instance or data point in the dataset, often denoted by x."
"hypothesis class of functions","a set of functions used to represent the mapping from input data to output in supervised learning."
"supervised learning setting","a machine learning framework where a model is trained on labeled data."
"predictor","a function that generates outputs based on input data and model parameters."
"parameter","a variable that defines a model's structure and behavior in mathematical functions."
"empirical risk minimization","a strategy in statistical learning to minimize the difference between predicted and actual outcomes."
"loss function","a mathematical function that quantifies the cost associated with a prediction error."
"ordinary least-squares regression","a statistical method used to estimate the parameters of a linear regression model by minimizing the sum of the squares of the residuals."
"PCA","A method involving linear dimensionality reduction and data representation via orthogonal transformation."
"Bayesian treatment","A statistical approach for inference that incorporates prior beliefs and updates them with evidence."
"Marginalizing","The process of integrating out certain variables in a probabilistic model."
"Latent variable","A variable that is not directly observed but is inferred from other variables."
"Probabilistic PCA (PPCA)","A version of PCA that incorporates probabilistic principles, introduced by Tipping and Bishop."
"Maximum likelihood estimation","A method for estimating the parameters of a statistical model that maximizes the likelihood function."
"Standard-normal prior","A prior probability distribution for a latent variable that follows a normal distribution with mean 0 and variance 1."
"Linear relationship","A relationship in which a change in one variable results in a proportional change in another."
"Backpropagation","A method used in machine learning to compute the gradient of the loss function with respect to the weights of the network."
"Automatic Differentiation","A technique to evaluate the derivative of a function specified by a computer program."
"Computation Graph","A representation of the computations performed, where nodes represent operations and edges represent the flow of data."
"Chain Rule","A formula in calculus for computing the derivative of the composition of two or more functions."
"Derivative","A measure of how a function changes as its input changes, indicating the rate of change of the function."
"Elementary Functions","Basic functions such as polynomials, exponentials, and trigonometric functions that are used in calculus."
"Gradient","A vector containing the partial derivatives of a function with respect to its variables, indicating the direction of steepest ascent."
"maximum likelihood estimation","A method for estimating the parameters of a probabilistic model, which aims to maximize the likelihood function."
"probabilistic models","Statistical models that incorporate randomness and are used to describe uncertain events and outcomes."
"method of moments","A technique for estimating parameters by equating sample moments to theoretical moments."
"parameter estimation","The process of determining the parameters of a statistical model based on observed data."
"overfitting","A modeling error that occurs when a model is excessively complex, capturing noise instead of the underlying pattern."
"underfitting","A modeling error that occurs when a model is too simple to capture the underlying structure of the data."
"generative process","The underlying process that is believed to have generated the observed data in a probabilistic model."
"latent space","the space in which latent variables exist and influence generated images, leading to variations in shape, rotation, and size."
"intrinsic dimensionality","a measure of the minimum number of parameters needed to represent a dataset, here indicated as two for the generated images."
"probabilistic PCA","a statistical method that models observations based on latent variables and assumes a Gaussian distribution."
"latent variables","unobserved variables that underpin and influence the observed data in a probabilistic model."
"model parameters","the parameters, denoted as B, which define the model structure and are consistent across the dataset."
"likelihood parameter","a parameter, denoted as o, that represents the likelihood of the observations given the model parameters in the dataset."
"generate new data","the process of creating new data points from the latent variables, expressed as & = Bz."
"training data","the dataset used to train a model, influencing the realism of the generated data."
"parameter vector","a vector indicating direction, considered as a geometric vector."
"example vector","a vector representing a data point, described by its coordinates."
"hyperplane","a mathematical concept that represents a flat affine subspace of one dimension less than its ambient space."
"classify","to assign categories (like positive or negative) to elements based on certain criteria."
"positive examples","examples that lie above the hyperplane during classification."
"negative examples","examples that lie below the hyperplane during classification."
"heads","parameter of a Bernoulli distribution"
"Bernoulli distribution","p(x | j) = Ber(j)"
"dataset","specific dataset VY"
"coin-flip experiments","observed outcomes of coin-flip experiments"
"probabilistic modeling","used for learning from observed outcomes"
"probability distributions","uncertain aspects of an experiment represented"
"probability theory","set of tools for modeling, inference, prediction, and model selection"
"joint distribution","p(a, 0) of the observed variables x and the hidden parameters @"
"expected likelihood","under the prior, i.e., Eo (p()’ |, 8)"
"Linear Regression","a statistical method for modeling the relationship between a dependent variable and one or more independent variables."
"marginal likelihood","the likelihood of the observed data averaged over the possible values of the model parameters."
"Bayesian model selection","a method for selecting models based on their posterior probabilities, integrating over prior beliefs and data evidence."
"generative process","a stochastic process that describes how data is generated in a model."
"conjugate Gaussian prior","a prior distribution that is Gaussian, which is mathematically convenient because it allows for easy updating with new evidence."
"marginal likelihood is Gaussian","indicates that the marginal likelihood can be expressed as a Gaussian distribution in terms of the response variable."
"mean and covariance","statistical measures that describe the central tendency and dispersion of a probability distribution."
"mean","to calculate the mean"
"variance","a sum of pairwise differences between all pairs of observations"
"bias—variance decomposition","a concept in machine learning related to the variance"
"observations","realizations of random variable X"
"empirical variance","the sum of pairwise differences is the empirical variance of the observations"
"pairwise differences","squared difference between pairs of x; and x;"
"deviations from the mean","a sum of deviations from the mean"
"geometrically","a mean related explanation in geometric terms"
"directed graphical model","A probabilistic model that specifies dependencies between random variables visually."
"graphical model","A model that visually captures the decomposition of the joint distribution over random variables into a product of factors."
"joint distribution","The distribution that represents the probability of a set of random variables simultaneously."
"prior","Information about a random variable before observing any data."
"likelihood","The probability of observing the data given a set of parameters in a probabilistic model."
"posterior","The updated probability of a random variable after observing data."
"independence relations","The relationships that indicate when two random variables do not influence each other."
"conditional independence","A property where a random variable is independent of another random variable given a third random variable."
"one-hot encoding","a binary representation of categorical variables where only one category is represented by a 1 and all others by 0."
"K","number of possible configurations or settings of the variable z."
"configuration","a specific arrangement of values for a variable, in this case, the nonzero entry of the z-vector."
"marginal distribution","the probability distribution of a subset of a collection of random variables."
"GMM model","Gaussian Mixture Model; a probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions."
"likelihood","the probability of the observed data under a statistical model."
"positive side of the hyperplane","itive labels are on the positive side of the hyperplane, i.e., (w,a,)+b>0 when y, = +1"
"negative side of the hyperplane","examples with negative labels are on the negative side, i.e., (w,a,)+b<0 when y, =—1"
"separating hyperplane","Equation (12.7) is equivalent to (12.5) and (12.6) when we multiply both sides of (12.5) and (12.6) with y,, = 1 and y,, = —1, respectively."
"linear classifiers","There are many linear classifiers (green lines) that separate orange crosses from blue discs."
"orthogonal","w is orthogonal to any vector on the hyperplane."
"margins","A classifier with large margi"
"marginal likelihood","p(Y|X) = (2m) det(XS)X? + 02I)-2 -exp (— 3(y— Xmo)'(XS)X' +07D)'(y — Xmo)"
"Maximum Likelihood","The maximum likelihood solution to a linear regression problem finds a subspace onto which the overall projection error of the observations is minimized."
"Orthogonal Projection","The orange dots are the projections of the noisy observations onto the line 0.2."
"Regression dataset","A dataset consisting of noisy observations y of function values at input locations xp."
"automatic differentiation","Different from symbolic differentiation and numerical approximations of the gradient, e.g., by using finite differences."
"Jacobian","Can be vectors, matrices, or tensors."
"reverse mode","A method of automatic differentiation that is often used in the context of neural networks."
"forward mode","A method of automatic differentiation that is computationally more expensive than the reverse mode."
"intermediate variables","Variables like a, b, c, d, e used in a computation graph."
"computation graph","Graph that includes inputs, function values, and intermediate variables."
"backpropagation","The reverse mode of automatic differentiation used in neural networks."
"input dimensionality","Often much higher than the dimensionality of the labels in neural networks."
"label dimensionality","Often lower than the input dimensionality in neural networks."
"function","An expression that takes an input and produces an output, represented here as f(x) = \/a? + exp(x?) + cos (a? + exp(x”))."
"random variables","variables whose values are subject to randomness."
"factorization","the expression of a joint distribution as a product of conditional distributions."
"directed graphical model","a graphical representation of a set of random variables and their conditional dependencies via directed edges."
"conditional distribution","the probability distribution of a subset of random variables conditioned on the values of other variables."
"nodes","the points in a graph that represent random variables."
"directed link","an arrow in a graphical model indicating a dependency from one variable to another."
"joint distribution","a probability distribution that defines the likelihood of different outcomes for multiple random variables."
"predictor","a function that, when given a particular input example, produces an output."
"output","the result produced by the predictor, considered to be a single number (a real-valued scalar output)."
"input example","the data (in this case, a vector of features) fed into the predictor."
"vector","a mathematical representation that can contain multiple values or features."
"real-valued scalar output","a numerical output that is a single real number."
"D-dimensional","refers to a vector that has D features."  
"function","a mathematical relation that maps input(s) to an output."
"convex","if and only if for any two points «, y it holds that fy) > fw) + Vela)" (y- 2)."
"twice differentiable","a function f(a) is twice differentiable, that is, the Hessian exists for all values in the domain of «."
"Hessian","the matrix of second derivatives used to determine the local curvature of a function."
"positive semidefinite","the function f(a) is convex if and only if V2 f(a) is positive semidefinite."
"negative entropy","The negative entropy f(x) = x log, x is convex for x > 0."
"convexity","to illustrate the previous definitions of convexity, let us check the calculations for two points x = 2 and x = 4."
"differentiable","Since f(x) is differentiable."
"maximize D(A)","Maximizing the function D(A) subject to various constraints."
"derivative","The rate of change of a function with respect to a variable."
"dual optimization problem","An optimization problem that is derived from the primal problem, often providing bounds on the value of the solution."
"max —b'X (7.43)","Expresses the maximization objective for the dual problem."
"linear program","A mathematical optimization problem in which the objective function and constraints are linear."
"primal","The original optimization problem before forming its dual."
"dual","The optimization problem derived from the primal by switching the roles of the objective function and constraints."
"Jensen’s inequality","A fundamental inequality in convex analysis related to expectations of convex functions."
"convex optimization problem","An optimization problem where the objective function is convex and the feasible region is a convex set."
"Quadratic Programming","A special type of mathematical optimization problem where the objective function is quadratic and the constraints are linear."
"objective function","The function that is being minimized or maximized in an optimization problem."
"affine","A function that is a linear transformation followed by a translation."
"positive definite","A property of a matrix indicating that all its eigenvalues are positive, ensuring a unique minimum in optimization."
"square symmetric matrix","A matrix that is equal to its transpose and has the same number of rows and columns."
"Rix(MxN)","A matrix of dimension MxN."
"Gradient","The direction and rate of fastest increase of a function."
"Tensor","A mathematical object that generalizes scalars, vectors, and matrices."
"Dot product","An algebraic operation that takes two equal-length sequences of numbers and returns a single number."
"Partial derivative","The derivative of a function with respect to one variable while holding the others constant."
"Dimension","The number of independent parameters or coordinates needed to specify a point in a space."
"Entry","An individual element within a matrix."
"Matrix","A rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined."
"maximin","is less than the minimax, i-e., max min y(a, y) < minmax y(a, y)."
"minimax inequality","says that for any function with two arguments (a, y), maximin is less than minimax."
"Weak duality","uses (7.23) to show that primal values."  
"Constrained Optimization","refers to optimization problems with constraints on the variables."
"Lagrange Multipliers","a method for finding the local maxima and minima of a function subject to equality constraints."
"responsibilities","are a function of 7j, Mj, 44 for all j = 1,...,4"
"GMM","Gaussian Mixture Model"
"linear regression","a statistical method for modeling the relationship between a dependent variable and one or more independent variables"
"PCA","Principal Component Analysis"
"mean parameter","the average value of mixture component in a GMM"
"weights","the values given by corresponding responsibilities"
"maximum likelihood","a method for estimating the parameters of a statistical model"
"variances","a measure of the dispersion indicating how much the values in a set differ from the mean"
"mixture weights","the proportions of each component in a mixture model"
"PCA","We derived PCA from two perspectives: (a) maximizing the variance in the projected space; (b) minimizing the average reconstruction error."
"latent variables","Variables that are not directly observed but are inferred from other variables that are observed."
"posterior distribution","The distribution of an unknown quantity, updated to include new data."
"high-dimensional data","Data that exists in a space with many dimensions."
"lower-dimensional representation","A simplified version of high-dimensional data that retains essential features."
"covariance matrix","A matrix that provides a measure of how much variables change together."
"eigenvectors","Vectors that do not change direction during a linear transformation."
"eigenvalues","Scalars that indicate the magnitude of the eigenvectors."
"projection matrix","A matrix that transforms a higher dimension into a lower dimension while retaining structure."
"Theorem 11.3","Identifies the mixture weight as the ratio of the total responsibility of the kth cluster and the number of data points."
"mixture weight","The ratio of the total responsibility of the kth cluster and the number of data points."
"N","Represents the total number of data points."
"responsibility","The total weight assigned to a specific cluster in a mixture model."
"mixture components","The distinct clusters or groups in a mixture model."
"relative importance","The significance of a kth mixture component for the dataset."
"update equation","An equation used to adjust mixture weights based on cluster responsibilities."
"EM Algorithm","A general iterative scheme for learning parameters in mixture models and latent-variable models."
"Gaussian mixture model","A model that assumes all data points are generated from a mixture of several Gaussian distributions."
"GMM","Gaussian mixture model with N data points."
"Posterior Distribution","Distribution of latent variable z according to Bayes’ theorem."
"Latent Variable","A variable that is not directly observed but is inferred from other variables."
"Bayes’ Theorem","A mathematical formula used to determine conditional probabilities."
"Responsibility","The contribution of the kth mixture component for a specific data point."
"plane","a flat, two-dimensional surface that extends infinitely in all directions."
"orthogonal projection" ,"the process of dropping a perpendicular from a point to a line or plane."
"hyperplane" ,"a subspace of one dimension less than its ambient space, typically used in classification."
"linear separability" ,"a condition where data points can be separated by a straight line or hyperplane."
"margin" ,"the distance between the hyperplane and the closest data points from either class."
"distance" ,"a measure of the space between two points."
"Support Vector Machines" ,"a supervised machine learning model used for classification and regression."
"r" ,"a value representing the distance of the closest example to the hyperplane."
"scale" ,"to adjust the size or magnitude of data points in relation to the hyperplane."
"loss function","The loss corresponding to the hard margin SVM is defined as a function that evaluates the error in predictions."
"zero-one loss","A type of loss function that assigns a loss of 0 for correct predictions and 1 for incorrect predictions."
"hinge loss","A specific loss function used in SVMs that calculates the error based on the margin between predicted and actual values."
"negative log-likelihood","A function used to estimate parameters in statistical models that is minimized to find the best fit."
"squared error function","The loss function that is minimized when looking for the maximum likelihood solution, representing the average squared difference between predicted and actual values."
"primal SVM","The description of the SVM in terms of the variables w and b." 
"optimization problem","A mathematical problem that seeks to maximize or minimize a function by selecting input values from a allowed set."
"eigenvector","we recover Xc,,, aS an eigenvector of S."
"PCA" ,"we want to apply the PCA algorithm that we discussed in Section 10.6"
"covariance matrix" ,"we recover the data covariance matrix again."
"norm" ,"we need to normalize the eigenvectors Xc,,, of S so that they have norm 1."
"mean subtraction" ,"We start by centering the data by computing the mean y of the dataset and subtracting it from every single data point."
"standard deviation" ,"Standardization _ Divide the data points by the standard deviati"
"£(x, A)","overall problem is easy to solve"
"maximization","maximum over a set of affine functions"
"concave function"," The maximum of a concave function can be efficiently computed"
"Lagrange dual problem","found by differentiating the Lagrangian with respect to x"
"differentiating","setting the differential to zero"
"optimal value","solving for the optimal value"
"convex","where f(-) and g;(-) are convex"
"equality constraints","subject to g;(a) <0 for all h(a) =0_ for all"
"weak duality",""
"gradient","terms of its gradient V, f(a)"
"expected value","The expected value is represented as Vx [a] := Ex[(x — p)*]."
"mean","The mean yp is defined as yp = Ex(a)."
"random variable","X is referred to as a discrete or continuous random variable."
"variance","The variance is the mean of a new random variable Z := (X — )?."
"two-pass algorithm","A two-pass algorithm involves one pass to calculate the mean and a second pass to compute the variance."
"raw-score formula for variance","The raw-score formula for variance is given by Vx [2] = Ex[x] — (Ex([2])’. "
"mean of the square minus the square of the mean","The expression in (6.44) is referred to as 'the mean of the square minus the square of the mean'."
"Gaussian distributions","A type of continuous probability distribution for a real-valued random variable characterized by a symmetric bell-shaped curve defined by its mean and variance."
"constrained optimization","A mathematical optimization problem where the solution is subject to constraints or conditions."
"dimensionality reduction","A process of reducing the number of random variables under consideration, obtaining a set of principal variables."
"high-dimensional data","Data that has a large number of input variables or features."
"low-dimensional subspace","A subspace with fewer dimensions compared to the original space in which the data resides."
"one-dimensional subspace","A subspace that contains points that can be represented using a single coordinate."
"PCA","Principal Component Analysis, a statistical method used to reduce the dimensionality of a dataset while preserving its variance."
"empirical risk minimization","the process of minimizing the expected loss of a predictive model by optimizing its parameters."
"affine function","a function that is a linear polynomial; a combination of linear transformations and translations."
"non-linear functions","functions that do not increase or decrease at a constant rate, allowing for more complex relationships between variables."
"neural networks","computational models inspired by human brain networks, used to approximate complex functions and patterns."
"loss function","a method for quantifying how well a model's predictions match the actual data by producing a non-negative error value."
"linear models","A framework for modeling relationships between variables using linear equations."
"dimensionality reduction","The process of reducing the number of random variables under consideration by obtaining a set of principal variables."
"PCA","Principal Component Analysis, a technique used for reducing the dimensionality of data while preserving as much variance as possible."
"D","Represents input dimensionality in the context of dimensionality reduction."
"N","Represents the number of data points in the context of dimensionality reduction."
"inner products","A mathematical operation that combines two vectors to produce a scalar, often used in the context of machine learning and PCA."
"kernel trick","A method used in machine learning to enable algorithms to operate in higher-dimensional spaces by computing inner products of transformed data without explicitly transforming it."
"kernel PCA","A version of Principal Component Analysis that uses the kernel trick for nonlinear dimensionality reduction."
"nonlinear dimensionality reduction","Techniques for reducing dimensions in data that do not assume a linear relationship among variables."
"auto-encoder","A type of artificial neural network used for unsupervised learning of efficient codings." 
"deep auto-encoder","An advanced form of auto-encoder that consists of multiple layers for more complex representations."
"GMM","Gaussian Mixture Model (GMM) is a probabilistic model that assumes that the data is generated from a mixture of several Gaussian distributions with unknown parameters."
"mixture weights","Mixture weights are the coefficients that determine the contribution of each component in a mixture model to the overall density."
"means","Means refer to the average values of each Gaussian component in a mixture model."
"variances","Variances measure the spread or dispersion of each Gaussian component in a mixture model."
"log-likelihood","Log-likelihood is a statistical measure that evaluates how well a model explains the observed data; higher values indicate a better fit."
"dataset","a collection of data points or observations."
"dimension","a measurable extent of a particular kind, such as length, breadth, depth, or height."
"variance","a measure of how much values in a dataset differ from the mean value."
"covariance matrix","a matrix that contains the covariances between pairs of variables in the dataset."
"eigenvalues","values that indicate the scale of the eigenvectors associated with a matrix, particularly in transformation."
"eigenvectors","vectors that remain in the same direction after a linear transformation."
"symmetric matrix","a square matrix that is equal to its transpose."
"spectral theorem","a theorem stating properties of symmetric matrices and their eigenvalues and eigenvectors."
"ONB","orthonormal basis, a set of vectors that are orthogonal and each of unit length."
"principal components","the directions in the feature space along which the data varies the most."
"l independence","as described in Section 6.4.5."
"graphical model","nodes are random variables."
"random variables","The nodes represent the random variables a, b, c."
"edges","represent probabilistic relations between variables, e.g., conditional probabilities."
"conditional probabilities","Relations represented by edges in a graphical model."
"distribution","Not every distribution can be represented in a particular choice of graphical model."
"Probabilistic graphical models","have some convenient properties."
"visualize","a simple way to visualize the structure of a probabilistic model."
"statistical models","can be used to design or motivate new kinds of statistical models."
"conditional independence","Inspection of the graph alone gives us insight into properties, e.g., conditional independence."
"inference and learning","Complex computations for inference and learning in statistical models can be expressed in terms of graphical manipulations."
"Directed graphical models/Bayesian networks","are a method for representing conditional dependencies in a probabilistic model."
"GMM","Gaussian Mixture Model; a probabilistic model for representing the presence of sub-populations within an overall population."
"variances","Measures of how much values in a distribution differ from the mean; in GMM, it refers to the variability of each component."
"density","A function that describes the probability of a random variable taking on a particular value."
"updating","The process of revising model parameters, such as variances, to improve accuracy."
"mixture weights","Coefficients that determine the contribution of each component in a mixture model."
"components","The individual distributions within a Gaussian Mixture Model that combine to form the overall model."
"higher-dimensional representation","using a higher-dimensional representation ¢(a,,) to construct new features as non-linear combinations of the original features."
"feature map","refers to the process discussed in Section 9.2 that leads to a kernel in Section 12.4."
"kernel","related to the feature map and discussed in Section 12.4."
"deep learning methods","methods that learn new good features from the data itself and have been successful in areas such as computer vision, speech recognition, and natural language processing."
"neural networks","a key concept in machine learning that will not be covered in this part of the book."
"backpropagation","a key concept for training neural networks discussed in Section 5.6."
"sample space","the set of all possible outcomes of a random variable."
"target space","the set of possible values that a random variable can take."
"random variable","a variable whose possible values are numerical outcomes of a random phenomenon."
"p(x)","the probability that random variable X has the outcome x."
"discrete random variables","random variables that have a countable number of possible values."
"P(X = x)","the notation representing the probability mass function for discrete random variables."
"probability mass function","the function that gives the probability of discrete outcomes."
"distribution","another term often used to refer to the probability mass function."
"probability density function","the function used for continuous random variables representing the probability per unit."
"cumulative distribution function","the function that gives the probability that a random variable is less than or equal to a certain value."
"probability distribution","a term used for both discrete probability mass functions and continuous probability density functions."
"gradient","A vector that represents the direction and rate of the steepest ascent of a function."
"chain rule","A formula to compute the derivative of a composite function."
"differentiation","The process of finding the derivative of a function."
"linear regression model","A statistical method to model the relationship between a dependent variable and one or more independent variables using a linear equation."
"backpropagation algorithm","An efficient method for computing the gradient of the loss function with respect to the weights of the network in deep learning."
"tational methods","Sampling and variational inference techniques for statistical analysis."
"sampling","A method used to obtain a subset of data from a larger dataset to make inferences."
"variational inference","A technique in Bayesian statistics that approximates probability densities through optimization."
"conditional probabilities","Probabilities that express the likelihood of an event given the occurrence of another event."
"interdependence","A relationship between random variables where the outcome of one affects the outcome of another."
"directed links","Arrows between nodes that indicate the direction of the relationship in a graphical model."
"random variables","Variables whose possible values are outcomes of a random phenomenon."
"conditional probability","The probability of an event given that another event has occurred."
"joint distribution","A probability distribution that describes two or more random variables simultaneously."
"factorization","The process of breaking down a joint distribution into simpler distributions that describe the dependencies among the variables."
"SVM" "The SVM (Support Vector Machine) allows for a geometric way to think about supervised machine learning."
"supervised machine learning" "A type of machine learning that uses labeled data to train models."
"probabilistic models" "Models in which the outputs are uncertain and are described using probability distributions."
"maximum likelihood estimation" "A method used in statistics for estimating the parameters of a model by maximizing the likelihood function."
"Bayesian inference" "A statistical method that applies Bayes' theorem for updating the probability of a hypothesis based on new evidence."
"inner products" "A mathematical operation that combines two vectors to produce a scalar."
"projections" "The process of mapping a vector onto another vector or space."
"optimization problem" "A mathematical problem that seeks to find the best solution from a set of feasible solutions."
"analytic solution" "A solution to a mathematical problem that can be expressed in a closed-form expression."
"sum rule","relates the joint distribution to a marginal distribution."
"joint distribution","contains more than two random variables."
"marginal distribution","resulting in a marginal distribution of potentially more than one random variable."
"random variables","random quantities that can take on different values."
"high-dimensional sum","performing a high-dimensional sum or integral."
"high-dimensional integral","generally computationally hard, in the sense that there is no known polynomial-time algorithm to calculate them exactly."
"independent and identically distributed","iid."
"conditionally independent",""
"inner products","between multivariate random variables can be treated in a similar fashion"
"probability mass","is positive and needs to add up to 1"
"statistical manifold","the space of probability distributions"
"information geometry","the study of the space of probability distributions"
"Kullback-Leibler divergence","a generalization of distances that account for properties of the statistical manifold"
"Euclidean distance","a special case of a metric"
"code","given by z, = B ‘ry, € R” and we are interested in minimizing the average squared error between the data z,, and its reconstruction fy,"
"average squared error","the difference between the actual data and its reconstruction"
"objective function","the same objective function as in (10.29) that we discussed in Section 10.3"
"PCA","we obtain the PCA solution when we minimize the squared auto-encoding loss"
"nonlinear auto-encoder","when we replace the linear mapping of PCA with a nonlinear mapping"
"deep auto-encoder","a prominent example of this is a deep auto-encoder where the linear functions are replaced with deep neural networks"
"encoder","also known as a recognition network or inference network"
"decoder","also called a generator"
"information theory","another interpretation of PCA is related to information theory"
"compressed version","the code as a smaller or compressed version of the original data point"
"likelihood","the likelihood of this probabilistic model by integrating out the latent variable z"
"probabilistic model","a model that incorporates randomness through the use of probability distributions"
"latent variable","a variable that is not directly observed but is inferred from other variables"
"Gaussian distribution","the solution to this integral is a Gaussian distribution with mean"
"mean","expected value of the distribution, calculated as E, [a] [Bz + pw) +E,[e] =p"
"covariance matrix","describes the variance and covariance of multiple random variables, represented as V[a] = V.[Bz+ pw] + V.[e]"
"maximum likelihood estimation","a method used for estimating the parameters of a statistical model"
"MAP estimation","Maximum A Posteriori estimation, another method for estimating the parameters incorporating prior information"
"conditional distribution","the distribution of a random variable given certain conditions or the values of other variables"
"covariance","a measure of how much two random variables change together"
"Gaussian random variable","a random variable that follows a Gaussian (normal) distribution"
"linear transformation","a mathematical operation that maps a vector space onto itself using addition and scalar multiplication"
"regularization","This trade-off is achieved using regularization."
"abduction","Abduction is the process of inference to the best explanation."
"hyperparameter","The choice of the number of components is an example of a hyperparameter."
"model selection","The problem of choosing among different models is called model selection."
"nested cross-validation","Model selection is often done using nested cross-validation."
"Partial Derivative","The partial derivatives of L with respect to the parameters 0; = {.A;, b;} of each layer."
"Chain Rule","The chain rule allows us to determine the partial derivatives."
"Output","The orange terms are partial derivatives of the output of a layer with respect to its inputs."
"Parameters","The blue terms are partial derivatives of the output of a layer with respect to its parameters."
"Backpropagation","Assuming we have already computed the partial derivatives 0L/06;,1."
"Automatic Differentiation","5.6 Backpropagation and Automatic Differentiation."
"Random variables","Two random variables X,Y are statistical objects that can take on different values, governed by probability distributions."
"Statistical independence","Two random variables X,Y are statistically independent if p(x, y) = p(x)p(y)."
"Covariance","Covariance measures the degree to which two random variables change together."
"Covariance zero","Two random variables can have covariance zero but are not statistically independent."
"Linearly dependent","Random variables that have a linear relationship will show dependence measured by covariance."
"Nonlinearly dependent","Random variables that have a non-linear relationship could have covariance zero."
"Mean","The mean of a random variable is its expected value, represented as E[X]."
"Expected value","The expected value E[X] is the average or mean of all possible values of a random variable weighted by their probabilities."
"quadratic","a term that includes the square of a variable."
"linear","a term that is directly proportional to a variable."
"constant term","a term that does not change with the variable."
"matching","the process of finding equivalence between different expressions."
"predictive distribution","a distribution that predicts the outcome based on the model parameters."
"expectation","a mathematical concept to calculate the average or mean value of a random variable."
"marginal likelihood","the likelihood of the observed data under a specific statistical model, integrating over all possible parameter values."
"convex","A function is convex."
"quadratic program","A mathematical program that contains a quadratic objective function and linear constraints."
"variables","Quantities represented in the program that can change."
"linear constraints","Conditions in the program that are linear equations or inequalities."
"objective function","The function that is being optimized in the program."
"positive semidefinite matrix","A matrix Q that ensures the objective function is convex and results in elliptical contour lines."
"contour lines","Lines in a graph representing levels of the objective function."
"optimal value","The best achievable value of the objective function within the feasible region."
"feasible region","The set of all possible points that satisfy the constraints of the program."
"Lagrangian","A function that combines the objective function and constraints, used in optimization."
"derivative","A measure of how a function changes as its input changes."
"invertible","A property indicating that a matrix can be reversed or has an inverse." 
"dual Lagrangian","A form of the Lagrangian associated with the dual optimization problem."
"Covariance" ,"The covariance of a variable with itself Cov[, x] is called the variance and is denoted by V x [x]."
"Variance" ,"The covariance of a variable with itself Cov[, x] is called the variance and is denoted by V x [x]."
"Standard deviation" ,"The square root of the variance is called the standard deviation and is often denoted by o(x)."
"Expectation" ,"By using the linearity of expectations, the expression in Definition 6.5 can be rewritten as the expected value of the product minus the product of the expected values."
"Multivariate random variables" ,"The notion of covariance can be generalized to multivariate random variables." 
"Linearity of expectations" ,"By using the linearity of expectations, the expression in Definition 6.5 can be rewritten as the expected value of the product minus the product of the expected values."
"Covariance (Multivariate)" ,"If we consider two multivariate random variables X and Y with states x € R? and y € R® respectively, the covariance between X and Y is defined as..."
"latent-variable models","models that include variables that are not directly observed but are inferred from other variables."
"conditional distribution","the distribution of a random variable given the value of another variable."
"generative process","a statistical model that describes how data is generated from a set of parameters."
"prior","the probability distribution that represents the uncertainty about a variable before observing data."
"parameter learning","the process of estimating the parameters of a statistical model using data."
"inference","the process of drawing conclusions about unknown values or distributions based on observed data."
"maximum likelihood estimation","a method for estimating the parameters of a statistical model that maximizes the likelihood function."
"Bayesian inference","a statistical method that updates the probability for a hypothesis as more evidence or information becomes available."
"likelihood","the probability of the observed data under a specific statistical model."
"M algorithm","Algorithm to assign data points to clusters."
"GMM","Gaussian Mixture Model, treats the means as cluster centers."
"k-means","A clustering algorithm that ignores covariances."
"MacKay","Reference describing the difference between A-means and GMM."
"A-means","Makes a 'hard' assignment of data points to cluster centers."
"soft assignment","Method of assigning data points via responsibilities in GMM."
"latent-variable perspective","Perspective considering hidden variables in models."
"EM algorithm","Expectation-Maximization algorithm used for parameter learning."
"maximum likelihood estimation","Statistical method for estimating parameters of a model."
"multi-noulli","a generalization of the Bernoulli distribution to more than two values."
"one-hot encoding","z is a one-hot encoding (also: 1-of-K representation)."
"prior distribution","we place a prior distribution on the latent variable z."
"probability vector","the kth entry describes the probability that the Ath mixture component generated data point x."
"GMM","the construction of this latent-variable model lends itself to a sampling procedure (generative process) to generate data."
"sampling procedure","to generate data: 1. Sample 2“ ~ p(z). 2. Sample xv ~ p(a| 2 = 1)."
"binary classification","This is a machine learning task where the output can only take on two possible values."
"predictors","Functions or models that output binary values for a given input."
"labels","The set of possible outcomes or classes associated with a particular task, denoted as {+1,—1} in this context."
"feature vector","A representation of an example or data point consisting of D real numbers."
"positive and negative classes","The labels used to categorize outcomes in binary classification, often denoted as +1 and -1."
"intuitive attributes","Assumptions made about the characteristics of the classes, such as associating +1 with positive results."
"cancer detection task","An example of a binary classification problem where patients are labeled as +1 if they have cancer."
"distinct values","Different sets of representations for binary outcomes, such as {True, False} or {0, 1}."
"margin condition","Formulation does not allow for any violations of the margin condition."
"projection error","The distance as the projection error that incurs when projecting x onto the hyperplane."
"squared norm","The squared norm results in a convex quadratic programming problem for the SVM."
"convex quadratic programming","Results in a convex quadratic programming problem for the SVM."
"hard margin SVM","Refers to a support vector machine that enforces a strict margin."
"linearly separable data","Linearly separable data, with a large margin."
"non-linearly separable data","Non-linearly separable data."
"maximizing","Maximizing wr yields the same solution as minimizing 4 ||w”||." 
"minimizing","Maximizing wr yields the same solution as minimizing 4 ||w”||." 
"parameters","Renaming the parameters to w” and b”."
"substituting","Substituting this result into (12.23) yields the final result."
"observation","The final step is to observe that maximizing wr yields the same solution."
"Gaussian-shaped kernel","a function used in density estimation that has a bell curve shape characteristic of the Gaussian distribution."
"Density Estimation","the process of estimating the probability density function of a random variable."
"Gaussian Mixture Model (GMM)","a probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions with unknown parameters."
"Ancestral sampling","a method of sampling from a probability distribution by first selecting an index from a categorical distribution and then drawing samples from a conditional distribution."
"Index k","a representation of an individual component in the mixture model, sampled from a probability vector."
"Probability vector","a vector that contains probabilities that sum up to one, representing the likelihood of choosing each component in a mixture model."
"N (µ, σ²)","denotes a normal distribution with mean µ (mu) and variance σ² (sigma squared)."
"Nested cross-validation","a technique used to find the model parameters by performing cross-validation within a cross-validation setup."
"K-means clustering","a clustering algorithm that partitions data into K distinct clusters based on the mean of the points in each cluster."
"Gaussian Distribution","A statistical distribution characterized by its bell-shaped curve, determined by its mean and variance."
"Joint Distribution","The probability distribution that describes two or more random variables simultaneously."
"Marginalizing","The process of integrating out variables that are not of interest in order to focus on specific variables."
"Gaussian Likelihood","A probability function that describes the likelihood of data under a Gaussian distribution."
"Bayes’ Theorem","A mathematical formula used to update the probability of a hypothesis based on new evidence."
"Posterior","The updated probability of a hypothesis after considering evidence, as computed using Bayes' Theorem."
"Gaussian Prior","A prior probability distribution that assumes the parameters follow a Gaussian distribution."
"Product of Two Gaussians","A multiplication of two Gaussian density functions resulting in another Gaussian distribution."
"Variance","A measure of the dispersion of a set of values, indicating how much the values deviate from the mean."
"Mean","The average or central value of a set of numbers."
"Cross-validation","can be used to determine a good bin size."
"kernel density estimation","a nonparametric way for density estimation."
"N i.i.d. samples","samples that are independent and identically distributed."
"kernel function","a nonnegative function that integrates to 1."
"smoothing/bandwidth parameter","plays a similar role as the bin size in histograms."
"uniform distribution","a commonly used kernel function."
"Gaussian distribution","a commonly used kernel function."
"histograms","closely related to kernel density estimates." 
"smoothness of the density estimate","can be guaranteed by choosing a suitable kernel."
"roots","the solutions of a polynomial equation."
"polynomial","an algebraic expression consisting of variables raised to non-negative integer powers."
"degree","the highest power of the variable in a polynomial."
"Abel-Ruffini theorem","states that there exists no algebraic solution to the problem of finding roots for polynomials of degree 5 or more."
"algebraic solution","a solution that can be expressed in terms of algebraic operations (addition, subtraction, multiplication, division, and root extraction)."
"eigenvalues","the scalars associated with a linear transformation that, when multiplied by their eigenvectors, yield the same result as applying the transformation."
"singular values","the non-negative values derived from the singular value decomposition (SVD) of a matrix, representing the scaling factors along the principal components."
"iterative methods","computational algorithms that obtain solutions through successive approximations."
"PCA","Principal Component Analysis, a technique for reducing the dimensionality of data while preserving as much variance as possible."
"eigenvectors","the vectors that correspond to eigenvalues and indicate the directions along which a linear transformation acts." 
"eigendecomposition","the process of decomposing a matrix into its eigenvalues and eigenvectors."
"SVD","Singular Value Decomposition, a method for decomposing a matrix into its constituent parts for analysis."
"power iteration","a simple iterative method used to find the dominant eigenvector of a matrix."
"Gaussian distribution","Defined in Section 6.5, characterized by a mean vector and covariance matrix."
"marginal distribution","The distribution of a subset of variables, obtained by integrating over the other variables."
"bimodal","A distribution that has two modes."
"negatively correlated","A statistical relationship where an increase in one variable corresponds to a decrease in another."
"positively correlated","A statistical relationship where an increase in one variable corresponds to an increase in another."
"cross-covariance","Covariance between different variables; denoted as Cov[xi,xj] for i,j =1,...,D, i≠j."
"covariance matrix","A square matrix giving the covariances between each pair of elements in a random vector."
"positive definite","A property of a matrix where all its eigenvalues are positive, ensuring unique solutions in optimization problems."
"positive semidefinite","A property of a matrix where all its eigenvalues are non-negative, but may be low-rank."
"expected log-likelihood","The expectation of log p(x, z | 0) is taken with respect to the posterior p(z | 2, 0) of the latent variables."
"EM iteration","An EM iteration does increase the log-likelihood."
"maximum likelihood solution","There are no guarantees that EM converges to the maximum likelihood solution."
"local maximum","It is possible that the EM algorithm converges to a local maximum of the log-likelihood."
"initializations of the parameters","Different initializations of the parameters could be used in multiple EM runs to reduce the risk of ending up in a bad local optimum."
"Regression","A statistical process for estimating the relationships among variables."
"Completing the Squares","A mathematical technique used to transform a quadratic equation into a perfect square form."
"Symmetric and Positive Definite Matrix","A matrix that is equal to its transpose and has all positive eigenvalues."
"Quadratic Term","A term in a polynomial that is of the second degree."
"Linear Terms","Terms in a polynomial that are of the first degree."
"Constant","A value that does not change."
"Predictive Distribution","A probability distribution of a random variable that provides predictions based on observed data."
"Parameter Prior","The prior distribution representing beliefs about parameters before observing data."
"Parameter Posterior","The updated distribution of parameters after observing data."  
"Conjugate Model","A model in which the prior and posterior distributions are in the same family."
"Data Histogram","A graphical representation using bars to show the frequency of data points in specified ranges (bins)."
"KDE","Kernel density estimation; a technique for estimating the probability density function of a random variable."
"kernel density estimator","Produces a smooth estimate of the underlying density of a dataset."
"unsmoothed count","A raw measure of how many data points fall into a single bin."
"density estimation techniques","Various methods used to estimate the probability density function of a random variable."
"nonparametric way","A method that does not assume a predetermined form for the distribution of the data."
"binning","The process of dividing the data space into intervals (bins) for the construction of a histogram."
"bar","A graphic representation in a histogram that shows the frequency of data points within a bin."
"hyperparameter","A parameter whose value is set before the learning process begins and can impact model performance." 
"overfitting","A modeling error that occurs when a model is too complex and captures noise instead of the underlying pattern."
"underfitting","A modeling error that occurs when a model is too simple to capture the underlying pattern in the data."
"regression","One of the fundamental problems in machine learning used to predict a continuous outcome based on one or more predictor variables."
"dimensionality reduction","A technique in machine learning used to reduce the number of features or variables in a dataset while preserving its essential information."
"density estimation","A third pillar of machine learning that refers to the process of modeling the probability distribution of a dataset."
"expectation maximization (EM) algorithm","An iterative algorithm used in statistics to find maximum likelihood estimates of parameters in probabilistic models, often involving latent variables."
"latent variable","A variable that is not directly observed but is inferred from the observed variables in a statistical model."
"mixture models","Statistical models that assume the presence of multiple underlying probability distributions within a dataset." 
"parametric family","A group of probability distributions that are defined by a finite number of parameters, such as Gaussian or Beta distributions."
"mean","The average value of a set of numbers, calculated by summing the numbers and dividing by the count of numbers."
"variance","A measure of the dispersion or spread of a set of values, indicating how much the values differ from the mean."
"Linear regression","a method for solving systems of linear equations."
"Maximum likelihood linear regression","performs an orthogonal projection."
"least-squares error","the minimum least-squares error between y and X96."
"linear algebra","concepts from linear algebra related to linear regression."
"analytic geometry","concepts from analytic geometry related to linear regression."
"maximum likelihood estimator","effectively does an orthogonal projection of y onto the one-dimensional subspace spanned by X."
"orthogonal projection","the projection of y onto the one-dimensional subspace spanned by X."
"orthogonal projections","results on orthogonal projections from Section 3."
"prior","concept that represents the initial beliefs about parameters before observing data."
"posterior","concept that represents the updated beliefs about parameters after observing data."
"latent variable","a variable that is not directly observed but is inferred from the model's structure and is used in calculating probabilities."
"GMM","Gaussian Mixture Model, a probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions."
"one-hot encoding","a representation of categorical variables as binary vectors where only one element is 'hot' (1) and all others are 'cold' (0)."
"conditional probability","the probability of an event given the occurrence of another event."
"probability vector","a vector consisting of probabilities that sum to one, typically used to represent categorical outcomes."
"mixture component","one of the Gaussian distributions in a Gaussian Mixture Model that contributes to the overall probability distribution."
"a" ,"The observed data where a= Bztpt+eecR?"
"Gaussian observation noise","A type of noise characterized by its statistical properties, following a Gaussian distribution."
"linear/affine mapping","A function that describes the relationship between latent and observed variables."
"latent variables","Variables that are not directly observed but are inferred from observed variables."
"observed variables","Variables that are measured or recorded in a study."
"PPCA","Probabilistic Principal Component Analysis, a method linking latent and observed variables."
"generative process","A statistical process that allows for the creation of data points from specified model parameters."
"Ancestral sampling scheme","A method of generating samples from a probabilistic model by sampling from the prior distribution before the conditional distributions."
"joint distribution","The probability distribution of two or more random variables considered together."
"graphical model","A representation of the probabilistic relationships among a set of variables, often using graphs."
"probabilistic model","A model that incorporates randomness and uncertainty in the predictions."
"model selection","choosing hyperparameters of our model."
"parameters","explicit parameters of a probabilistic model."
"hyperparameters","higher-level parameters that control the distribution of explicit parameters."
"empirical risk minimization","a machine learning approach discussed in Section 8.2."
"principle of maximum likelihood","a concept in machine learning discussed in Section 8.3."
"probabilistic modeling","a method in machine learning discussed in Section 8.4."
"learning","the process of estimating in machine learning."
"underfit","a model that is insufficiently flexible and has few parameters."
"overfit","a model that has low training risk but high test risk."
"cross validation","a method to detect overfitting by comparing training risk and test risk."
"maximum likelihood","a fitting method used for different model classes to a regression dataset."
"parametrized model class","a model class that describes a dataset with a certain number of parameters."
"training risk","the error or risk assessed on the training dataset."
"test risk","the error or risk assessed on the test dataset."
"probability","the measure of the likelihood of an event occurring, illustrated by an outcome like a coin flip."
"Theorem 11.2","The update of the covariance parameters of the GMM is given by specific mathematical expressions."
"Covariance","A measure of how much two random variables change together."
"Gaussian Mixture Models (GMM)","A probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions."
"log-likelihood","A measure of how well a statistical model explains the observed data, using logarithms."
"Partial derivatives","The derivatives of functions with respect to one variable while keeping others constant."
"Gaussian distribution","A continuous probability distribution characterized by its bell-shaped curve, defined by its mean and variance."
"det","Determinant, a scalar value that is a function of the entries of a square matrix."
"Cov[zx, y]","the correlation between two random variables"
"correlation","the cosine of the angle between two random variables when considered geometrically"
"orthogonal","X and Y are orthogonal if and only if Cov[z, y| = 0, i.e., they are uncorrelated"
"Euclidean distance","a distance measure constructed in a Euclidean space"
"independent and identically distributed (i.i.d.)","random variables that are modeled as independent and identically distributed"
"mutually independent","refers to random variables where all subsets are independent"
"Gaussian","The special case of the Gaussian with zero mean and identity covariance is referred to as the standard normal distribution."
"Standard normal distribution","A special case of the Gaussian with zero mean and identity covariance."
"Statistical estimation","The process of estimating the parameters of a statistical model."
"Machine learning","A field of artificial intelligence that uses statistical techniques to give computer systems the ability to learn from data."
"Closed-form expressions","Expressions that can be computed in a finite number of standard operations."
"Marginal distributions","The probability distribution of a subset of a collection of random variables."
"Conditional distributions","The probability distribution of a random variable given the value of one or more other random variables."
"Variable transformations","The process of applying a mathematical operation to change the values of a variable."
"Multivariate random variables","Random variables that have more than one dimension or component."
"polynomials","mathematical expressions consisting of variables and coefficients that include terms of the form ax^n, where a is a coefficient, x is a variable, and n is a non-negative integer."
"posterior uncertainty","the degree of uncertainty in predictions made based on a posterior distribution, which updates initial beliefs based on new evidence."
"decision-making system","a system or model that utilizes predictions to make choices that can affect outcomes."
"reinforcement learning","a type of machine learning where an agent learns to make decisions by receiving rewards or penalties based on its actions."
"Bayesian linear regression","a statistical method that estimates the distribution of parameters in a linear model using Bayes' theorem."
"posterior distribution","the probability distribution that represents the updated beliefs about a parameter after observing data."
"maximum likelihood","a statistical method for estimating the parameters of a model that maximizes the likelihood function, ensuring the observed data is most probable."
"MAP estimation","maximum a posteriori estimation, a technique in Bayesian inference that finds the mode of the posterior distribution."
"machine learning","learn from empirical observations of data."
"random variable","a variable whose possible values are numerical outcomes of a random phenomenon."
"covariance","the covariance of multivariate random variables Cov(z, y) is sometimes referred to as cross-covariance."
"cross-covariance","covariance of multivariate random variables."
"population mean","the average of a set of values in a population."
"correlation","statistical measure that describes the extent to which two variables are linearly related."
"variance","a measure of how far a set of numbers are spread out from their average value."
"standard deviation","a measure of the amount of variation or dispersion of a set of values."
"covariance matrix","a matrix whose entries are covariances between pairs of random variables."
"marginal","the probability distribution of a subset of a collection of random variables."
"dependent random variables","random variables that have some relationship to each other."
"univariate random variables","random variables involving only one variable."
"Gaussian","A type of statistical distribution characterized by its bell-shaped curve, defined by its mean and variance."
"posterior predictive distribution","A distribution that represents the predictions of unobserved data based on observed data and prior distributions."
"posterior uncertainty","The uncertainty associated with the parameters after observing the data."
"MAP estimate","The Maximum A Posteriori estimate; a type of estimation that incorporates prior information."
"predictive mean","The expected value of the predictive distribution."
"factorize","To express a mathematical expression as a product of its factors."
"constant","A fixed value that does not change."
"parameter estimation","The process of determining the values of parameters in a statistical model."
"maximum likelihood","An estimation method that identifies the parameter values that maximize the likelihood function."
"MAP estimation","Maximum A Posteriori estimation, a method to estimate parameters by incorporating prior knowledge along with the known data."
"posterior distribution","The distribution of the parameter after observing the data, as described by Bayes' theorem."
"optimization","The process of making a system as effective or functional as possible, often involving finding the maximum or minimum of a function."
"integration","A mathematical process of finding the integral of a function, often associated with computing areas under curves."
"Bayesian inference","A method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available."
"prior knowledge","Information that is assumed to be known before current data is observed, used in Bayesian analysis."
"plausible parameter values","Parameter values that are considered likely or reasonable based on the prior distribution."
"parameter distribution","A representation of the potential values of parameters in a statistical model along with their associated probabilities."
"nonzero pdf (or pmf)","A probability density function (pdf) or probability mass function (pmf) that has nonzero values on all plausible x."
"likelihood p(y|a)","Describes how a and y are related; the probability of the data y if we know the latent variable x."
"posterior p(a|y)","The quantity of interest in Bayesian statistics, expressing knowledge about x after observing y."
"Bayes’ theorem","A fundamental theorem in probability theory relating current evidence to prior beliefs."
"Bayes’ rule","A rule that describes how to update the probability of a hypothesis based on new evidence."
"Bayes’ law","Another term for Bayes' theorem, relating the conditional and marginal probabilities."
"prior","The initial belief or probability distribution before observing data."
"likelihood","The probability of observing the data given a certain model or parameters."
"measurement model","An alternate term for likelihood, used to describe how measurements relate to the latent variables."
"Mean","A measure of central tendency, calculated as the average of a set of values."
"(co)variance","A measure of how much two random variables vary together; variance refers to the variability of a single variable."
"total responsibility of","the kth mixture component."
"N,","2.058"
"No","2.008"
"N3","2.934"
"model parameters","Hy, Ux, 7"
"responsibilities","depend on the responsibilities"
"closed-form solution","maximum likelihood estimation problem impossible"
"updating one model parameter","keeping the others fixed"
"recompute the responsibilities","iterating"
"EM algorithm","specific instantiation"
"Theorem 11.1","Update of the GMM Means"
"mean parameters","jt,,k =1,..., KX"
"responsibilities r,,;","defined in (11.17)"
"equivalence","an equivalence between the pairwise distances and the distances from the center of the set of points."
"mean","by computing the mean (VV terms in the summation)."
"variance","then computing the variance (again N terms in the summation)."
"summation","terms in the summation."
"random variables","simple manipulations of random variables (such as adding two random variables)."
"states","with states x, y € R”."
"expectation","Ele + y] = Ela] + Ely)"
"covariance","Cov[a, y] + Cov[y, x]"
"variance formula","V(x + y] = V(x] + V[y] + Cov[a, y] + Cov[y, x]." 
"variance subtraction","V[a — y] = Via] + V[y] — Cov[a, y] — Cov[y, x]."
"raw-score formula","raw-score formula for variance."
"parameters","describe the family of affine functions, i.e., straight lines with slope a, which are offset from 0 by b."
"affine functions","straight lines with slope a, which are offset from 0 by b."
"Bayes’ theorem","a principled tool to update our probability distributions of random variables."
"posterior distribution","p(@| x) (the more specific knowledge) on the parameters @ from general prior statements (prior distribution) p(@)."
"prior distribution","general prior statements p(@) before observing data x."
"likelihood","function p(a | @) that links the parameters 0 and the observed data x."
"maximize","finding the parameter 6 that maximizes the posterior."
"estimating","applying the predictor on test data."
"validation set","a sub-set of the available training data that we keep aside."
"predictive performance","the effectiveness of a model's predictions based on the validation set."
"cross-validation","a method used to estimate the performance of a model by splitting data into training and validation sets."
"K-fold cross-validation","partitions the data into K chunks, with one chunk serving as the validation set and the rest as the training set."
"training set","the subset of data used to train the model."
"noise","the deviation in an estimate that can arise from using a small validation set, often resulting in high variance."
"likelihood","a function that measures how well a parameter explains observed data."
"squared-error loss","a loss function that measures the average of the squares of the errors."
"mis-classification error","the rate at which a classification model incorrectly predicts the outcomes."
"posterior distribution","the updated probability distribution of a parameter after observing data."
"Bayesian inference","the process of updating the probability estimate for a hypothesis as more evidence becomes available."
"Bayes’ theorem","a mathematical formula for updating probabilities based on new evidence."
"parameters","variables that define a model and are estimated from the data."
"data","observations or measurements collected for analysis."
"propagate uncertainty","the process of incorporating uncertainty from one aspect of a model into predictions or conclusions."
"marginalize","the process of integrating out a variable to focus on other variables."
"labeled data","Data that includes both input features and the corresponding correct output values."
"training data","The subset of data used to train a model."
"Test data","The dataset used to evaluate the performance of a model after it has been trained."
"performance","A measure of how well a model makes predictions."
"parameters","Variables in a model that are learned during training."
"maximum likelihood estimation","A method for estimating the parameters of a statistical model."
"overfitting","When a model learns the noise in the training data instead of the actual signal, leading to poor generalization."
"test set","A set of data not used during training, used to assess model performance."
"generalizes","The ability of a model to perform well on unseen data."
"model selection","The process of choosing between different models based on their performance."
"Nested Cross-Validation","A technique for model selection that involves cross-validation within a cross-validation framework."
"cross-validation","A method for estimating the generalization error by dividing the dataset into training and validation sets."
"generalization error","The error rate of a model when making predictions on unseen data."
"validation sets","Subsets of data used to tune model parameters and assess performance during training."
"Bregman divergences","Classes of divergences that have applications in information geometry."
"f-divergences","Another class of divergences used in information geometry."
"Gaussian distribution","The most well-studied probability distribution for continuous-valued random variables."
"normal distribution","Another term for Gaussian distribution."
"likelihood","A function used to define statistical models, particularly in regression."
"prior","A prior distribution used in Bayesian statistics."
"mixture of Gaussians","A probabilistic model that represents a distribution as a combination of multiple Gaussian distributions."
"density estimation","A statistical method for estimating the probability distribution of a random variable."
"Gaussian processes","A collection of random variables, any finite number of which have a joint Gaussian distribution."
"variational inference","A method in machine learning that approximates probability distributions through optimization."
"reinforcement learning","An area of machine learning focused on how agents ought to take actions in an environment to maximize cumulative reward."
"Gaussian distribution","A probability distribution characterized by its mean and variance."
"mean","The average value in a set of numbers, often used to describe the center of a Gaussian distribution."
"variance","A measure of the dispersion or spread of a set of numbers, indicating how much the values deviate from the mean."
"maximum likelihood","A method for estimating the parameters of a statistical model, maximizing the likelihood function."
"maximum a posteriori estimation","A statistical technique that computes the mode of the posterior distribution to estimate parameters."
"dimensionality reduction","The process of reducing the number of random variables under consideration, often used in PCA and ICA."
"PCA","Principal Component Analysis, a method used to reduce dimensionality while preserving as much variance as possible."
"factor analysis","A statistical method used to describe variability among observed variables in terms of fewer unobserved variables."
"ICA","Independent Component Analysis, a computational method for separating a multivariate signal into additive, independent components."
"convex sets","sets where a line segment connecting any two points in the set lies entirely within the set."
"nonconvex sets","sets that do not have the property that a line segment connecting any two points in the set lies entirely within the set."
"convex functions","functions such that a straight line between any two points of the function lies above the function."
"concave function","a function that is the negative of a convex function."
"domain","the set of all possible input values for a function."
"scalar","a single number used to multiply a vector or function."
"function","a relation that assigns exactly one output to each input from a specified set." 
"convex function","a function defined on a convex set that satisfies a specific inequality involving points in its domain."
"p(z,,)" "Represents a probabilistic model for observations affected by noise."
"N(0, I)" "Denotes a normal distribution with mean 0 and identity covariance matrix."
"factor analysis (FA)" "A statistical method that allows each observation dimension to have a different variance."
"likelihood" "The probability of the observed data given the model parameters."
"PPCA" "Probabilistic Principal Component Analysis, a method that assumes a closed-form maximum likelihood solution."
"closed-form maximum likelihood solution" "A solution that can be expressed analytically in terms of the model parameters."
"iterative scheme" "A process that involves repeated approximation to find an estimate."
"expectation maximization algorithm" "A statistical technique used to find maximum likelihood estimates in the presence of latent variables."
"stationary points" "Points in a mathematical optimization where the gradient is zero."
"global optima" "The best possible solution across the entire parameter space."
"scaling the data" "Transforming the data uniformly across all dimensions."
"rotating the data" "Changing the orientation of the data in the feature space."
"independent component analysis (ICA)" "A computational method for separating a multivariate signal into additive, independent components."
"derivative","Calculating the derivative of f(a), we obtain"
"logarithm","log, «+4 l :"
"convex function","Example of a convex function."
"concave function","Example of a nonconvex set."
"convex optimization problem","convex optimization problem"
"strong duality","strong duality"
"convex set","Example of a convex set."
"epigraph","epigraph"
"Lagrange multipliers","the resulting Lagrange multipliers are then unconstrained" 
"equality constraints","for each equality constraint h;(a) = 0"
"inequality constraints","replace it by two constraints h;(a) < 0 and h;(a) > 0"
"numerical precision","loss of numerical precision in floating-point arithmetic."
"mean","a value obtained by dividing the sum of several values by the number of values."
"covariance","a measure of the degree to which two random variables change together."
"affine transformation","a linear mapping method that preserves points, straight lines, and planes."
"random variable","a variable whose values are determined by the outcome of a random phenomenon."
"mean vector","a vector that contains the means of random variables."
"covariance matrix","a square matrix giving the covariances between each pair of elements in random variables."
"Cov[x, y]","the covariance between random variables x and y."
"statistical independence","a condition where two random variables are independent if the occurrence of one does not affect the other."
"GMM","Gaussian Mixture Model, a probabilistic model used for representing the presence of sub-populations within an overall population."
"responsibilities","In the context of Gaussian Mixture Models, responsibilities define the probability that a given data point belongs to a particular mixture component."
"EM","Expectation-Maximization, an iterative algorithm used for finding maximum likelihood estimates in models with latent variables."
"mixture components","The individual distributions or clusters in a mixture model, representing sub-populations within the overall dataset."
"converges","Refers to the process in which the EM algorithm reaches a stable set of parameters for the mixture model."
"first-order Taylor series expansion","a series expansion that approximates a function using its derivatives at a single point."
"Hessian matrix","a square matrix of second-order partial derivatives of a scalar-valued function."
"outer products","a mathematical operation that takes two vectors and produces a matrix."
"multivariate Taylor series","an extension of the Taylor series that approximates multivariable functions."
"Taylor polynomial","a polynomial used to approximate a function, derived from the Taylor series."
"third-order tensor","an array with three indexes, extending the concept of matrices to higher dimensions."
"one-dimensional array","a linear arrangement of elements, representing a vector."
"two-dimensional array","a grid-like arrangement of elements, representing a matrix."
"law of the unconscious statistician","A principle in statistics relating to how expected values are calculated using the probability distribution of a random variable."
"mean","The average of a set of numbers, calculated by dividing the sum of the values by the number of values."
"marginal likelihood/evidence","The probability of the observed data under a specific model, integrating over all possible parameter values."
"Bayes’ theorem","A formula that describes how to update the probability of a hypothesis based on new evidence."
"probabilistic inverse","An alternative name for Bayes’ theorem, highlighting the process of updating beliefs in light of evidence."
"expectation operator","A mathematical operator that calculates the expected value of a random variable, usually denoted as E."
"posterior","The updated probability of a hypothesis after considering new evidence, calculated using Bayes' theorem."
"prior","The initial belief about the probability of a hypothesis before observing new evidence."
"Bayesian model selection","A method for choosing between different statistical models based on their marginal likelihoods."
"data point","a slightly distorted or noisy version of the original data"
"lossy","a type of compression that results in loss of data"
"correlation","the measure of the relationship between the original data and the lower-dimensional code"
"mutual information","a core concept in information theory related to the similarity of the original data and its representation"
"PCA","Principal Component Analysis, a statistical procedure that uses an orthogonal transformation to convert correlated variables into uncorrelated variables"
"maximum likelihood estimates","the parameters of the model derived to maximize the likelihood of the observed data"
"likelihood parameter","a parameter that defines the likelihood function in the context of statistical models"
"D-dimensional data","data represented in D dimensions, often referring to the original data space"
"/-dimensional subspace","a lower-dimensional space onto which the original data is projected"
"parameters","values that define the model being used in the analysis"
"x2-coordinate","the second coordinate in a dataset, referenced in the context of data compression and similarity to original data"
"x1-coordinate","the first coordinate in a dataset, mentioned as being ignored in the context of data compression"
"compressed data","data that has been reduced in size while attempting to retain as much relevant information as possible"
"original data","the initial uncompressed dataset prior to any reductions or transformations applied"
"information content","a measure of how well the dataset occupies space, indicating the richness of the data structure"
"spread of the data","the extent to which data points are dispersed or distributed across the dataset"
"variance","a statistical measure that indicates the degree of spread in a dataset, serving as an indicator of its distribution characteristics"
"PCA","Principal Component Analysis, a dimensionality reduction algorithm that seeks to maximize variance in a lower-dimensional space to preserve information"
"dimensionality reduction","the process of reducing the number of variables under consideration, focusing on retaining essential information"
"matrix B","a specific matrix used in the context of data compression and projection to retain information"
"random variables","uncorrelated variables that can be considered as vectors in a vector space."
"orthogonal vectors","vectors that are uncorrelated in a vector space."
"Pythagorean theorem","a relation in Euclidean space that applies to the lengths of the sides of a right triangle."
"Cov[x, y]","covariance of random variables x and y, measuring the degree to which they change together."
"inner products","mathematical operation that reflects the geometric properties of vectors."
"zero mean random variables","random variables with an expected value of zero."
"covariance","a measure of the joint variability of two random variables."
"symmetric covariance","covariance that has the property that Cov[x, y] = Cov[y, x]."
"positive definite covariance","covariance that indicates that the variance is non-negative."
"linear covariance","covariance that satisfies the linearity in either argument."
"length of a random variable","the standard deviation of a random variable."
"standard deviation","a measure of the amount of variation or dispersion in a set of values."
"deterministic random variable","a random variable with a length of 0, meaning it has a fixed value."
"angle between random variables","the geometric measure of the separation between two random variables."
"PCA","PCA returns the coordinates (10.60), not the projections x..."
"eigenvectors","B is the matrix that contains the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix as columns."
"eigenvalues","B is the matrix that contains the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix as columns."
"data covariance matrix","B is the matrix that contains the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix as columns."
"projection","we obtain the projection as &, = BB'x, (10.59)"
"null space","the null space of S and follows the iteration"
"normalization","this means the vector a; is multiplied by S in every iteration and then normalized, i.e., we always have ||2;,|| = 1."
"iteration","this means the vector a; is multiplied by S in every iteration and then normalized, i.e., we always have ||2;,|| = 1."
"univariate Gaussian","A Gaussian distribution characterized by one variable, which includes mean and variance parameters."
"conditional Gaussian","A Gaussian distribution conditioned on a specific value of a variable, using mean and variance based on this condition."
"marginal distribution","The probability distribution of a subset of variables, obtained by integrating out the other variables."
"mean vector","A vector that contains the means of each variable in a multivariate distribution."
"covariance matrix","A matrix that describes the covariance between pairs of variables in a multivariate distribution."
"multivariate Gaussian distribution","A generalization of the Gaussian distribution to multiple variables, defining their joint behavior."
"standard normal distribution","A special case of the normal distribution with a mean of 0 and a variance of 1."
"dimension","A measurement of the extent of a space, represented by the number of coordinates needed to specify a point within it."
"hyperplane","An affine subspace of dimension D — 1, in the context of a vector space of dimension D, commonly used in machine learning for classification."
"class","A category or group that examples belong to, typically distinguished by labels, such as orange cross or blue disc."
"label","A symbol or identifier that indicates the class of an example, such as the two different symbols used in classification tasks."
"affine subspace","A subset of a vector space that is closed under affine combinations, often referred to in defining hyperplanes."
"linear separator","A straight line (or hyperplane in higher dimensions) used to classify different classes in a dataset."
"overfitting","The phenomenon where a model learns the details of the training data to the extent that it negatively impacts performance on new data."
"empirical risk","The average loss (or error) of a predictive model on a given dataset."
"minimizer","A point in the input space that minimizes a given function or criterion."
"empirical risk minimization","The statistical method of minimizing the empirical risk in order to find the best model."
"penalty term","A term added to an optimization problem to discourage certain solutions, thus influencing the model's behavior."
"regularization","A technique in machine learning used to prevent overfitting by adding information or constraints to a model."
"least-squares problem","An optimization problem that minimizes the sum of the squares of the differences between observed and predicted values."
"regularized problem","An optimization problem that includes a penalty term to discourage complex solutions."
"complexity","A measure of how intricate or complicated a solution or model is."
"port vector regression","a Bayesian neural network with a single hidden layer where the number of units tends to infinity"
"Gaussian processes","statistical models where observations are assumed to be sampled from a multivariate Gaussian distribution"
"Gaussian parameter priors","allow for closed-form inference in linear regression models"
"non-Gaussian prior","a prior distribution that is not Gaussian"
"underdetermined","a regression problem where the training set size is smaller than the dimensionality of the input"
"sparsity","a parameter prior that tries to set as many parameters to 0 as possible"
"variable selection","the process of choosing a subset of relevant features for use in model construction"
"regularizer","a technique used to prevent overfitting by adding a penalty for more complex models" 
"prediction accuracy","the measure of how often the predicted values match the actual values"
"posterior","can be used for predictions within a Bayesian inference framework"
"likelihood","requires the marginalization of the latent variables"
"marginalization","the process of summing or integrating out latent variables"
"conjugate prior","a specific prior distribution that simplifies the analysis of posterior distributions"
"analytically tractable","refers to when a solution can be explicitly calculated in a closed form"
"approximations","methods used when exact solutions are not available"
"stochastic approximations","approaches that incorporate randomness in the approximation process"
"Markov chain Monte Carlo (MCMC)","a class of algorithms for sampling from probability distributions based on constructing a Markov chain"
"deterministic approximations","approaches that do not involve randomness and yield consistent results" 
"Laplace approximation","a method used to approximate integrals in Bayesian inference"
"d. To apply hill-climbing approaches we use the gradients described in Chapter 5 and implement numerical optimization approaches from Chapter 7."
"hill-climbing approaches","A method for optimizing a function by iteratively making small adjustments based on the slope or gradient."
"empirical risk minimization","A principle in machine learning that aims to minimize the error of a model on the training data."
"maximum likelihood","A statistical method for estimating the parameters of a model that maximizes the likelihood of the observed data."
"optimization","The process of making a system or design as effective or functional as possible."
"cross-validation","A technique for assessing how the results of a statistical analysis will generalize to an independent data set."
"abduction","A form of reasoning which constructs or considers the best explanation for the available evidence."
"hyperparameter","A parameter whose value is set before the learning process begins and controls the behavior of the learning algorithm."
"model selection","The process of choosing between different models or algorithms for a given problem."
"nested cross-validation","A technique in which cross-validation is used inside another cross-validation process to estimate the performance of a model."
"gradient","A vector of partial derivatives of a function, representing the direction and rate of steepest ascent."
"hinge loss","A loss function used by the support vector machine, defined as L(a) = max{0,1—a}."
"gradient methods","A set of optimization algorithms that use the gradient of the function to find minimum values."
"L-BFGS","Limited-memory Broyden-Fletcher-Goldfarb-Shanno, an optimization algorithm that approximates the inverse Hessian matrix."
"subgradient methods","Optimization techniques that extend gradient methods to non-differentiable functions."
"convex conjugate","A function that transforms a convex function into another function, often used in optimization."
"proximal term","An additional regularization term added to an optimization problem to promote certain properties, such as smoothness or sparsity."
"hyperparameter","A parameter whose value is set before the learning process begins and is not learned from the data."
"convex optimization","A subclass of optimization problems where the objective function is convex."
"convex analysis","The study of convex sets and convex functions."
"Legendre—Fenchel transforms","A specific type of transformation used in convex analysis to relate a function to its convex conjugate."
"det(X)","determinant of matrix X."
"exp","exponential function."
"partial derivative","a derivative of a function with respect to one variable while keeping other variables constant."
"mean","the average value of a set of numbers."
"likelihood","the probability of a given set of parameters producing the observed data."
"maximum likelihood","a method for estimating the parameters of a statistical model that maximizes the likelihood function."
"mixture component","a component in a mixture model representing a distinct distribution."
"responsibility","the degree to which a mixture component is responsible for a given data point."
"expected value","the weighted average of all possible values of a random variable."
"conditional distribution","The conditional distribution p(a1,...,ay | 21,...,2y) factorizes over the data points."
"Bayes’ theorem","To obtain the posterior distribution p(z,,, = 1|x,,), we follow the same reasoning as in Section 11.4.3 and apply Bayes’ theorem."
"posterior distribution","This means that p(z;, = 1|2,,) is the (posterior) probability that the kth mixture component generated data point x,,."
"responsibilities","Now the responsibilities also have not only an intuitive but also a mathematically justified interpretation as posterior probabilities."
"EM algorithm","The EM algorithm that we introduced as an iterative scheme for maximum likelihood estimation can be derived in a principled way from the latent-variable perspective."
"encoder","both the encoder and the decoder are represented by multilayer feedforward neural networks, which themselves are nonlinear mappings."
"multilayer feedforward neural networks","both the encoder and the decoder are represented by multilayer feedforward neural networks"
"nonlinear mappings","both the encoder and the decoder are represented by multilayer feedforward neural networks, which themselves are nonlinear mappings."
"PCA","If we set the activation functions in these neural networks to be the identity, the model becomes equivalent to PCA."
"nonlinear dimensionality reduction","A different approach to nonlinear dimensionality reduction is the Gaussian process latent-variable model (GP-LVM) proposed by Lawrence (2005)."
"Gaussian process latent-variable model (GP-LVM)","A different approach to nonlinear dimensionality reduction is the Gaussian process latent-variable model (GP-LVM) proposed by Lawrence (2005)."
"latent-variable perspective","The GP-LVM starts off with the latent-variable perspective that we used to derive PPCA"
"PPCA","The GP-LVM starts off with the latent-variable perspective that we used to derive PPCA"
"Gaussian process (GP)","the GP-LVM marginalizes out the model parameters and makes point estimates of the latent variables z."
"model parameters","the GP-LVM marginalizes out the model parameters and makes point estimates of the latent variables z."
"point estimates","the GP-LVM marginalizes out the model parameters and makes point estimates of the latent variables z."
"Bayesian PCA","Similar to Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence (2010) maintains a distribution on the latent variables z."
"Bayesian GP-LVM","the Bayesian GP-LVM proposed by Titsias and Lawrence (2010) maintains a distribution on the latent variables z."
"distribution on the latent variables","the Bayesian GP-LVM proposed by Titsias and Lawrence (2010) maintains a distribution on the latent variables z."
"approximate inference","uses approximate inference to integrate them out as well."
"yx*! eR?X? (10.53)","Equation notation representing a mathematical expression involving variables y, x, and some matrix operations."
"X","A D x N matrix whose columns are the data points."
"V < D","Indicates that the number of data points is smaller than the dimensionality of the data."
"rank of the covariance matrix S","The rank is N, meaning the maximum number of linearly independent column vectors in S is N."
"eigenvalues that are 0","Indicates that the covariance matrix has D — N + 1 eigenvalues equal to zero."
"redundancies","Suggests there is some repetition or unnecessary information in the data."
"covariance matrix","A matrix representing the covariance (how much two random variables change together) between the variables."
"PCA","Principal Component Analysis, a method used to reduce the dimensionality of a dataset while preserving as much variance as possible."
"eigenvector equation","An equation in the form Sbm = Ambm, where S is a covariance matrix and m indexes the principal components."
"basis vector of the principal subspace","A vector that serves as a basis for the space spanned by principal components in PCA."
"S defined in (10.53)","Refers to the covariance matrix as defined in the formula given in the text."
"X' € R*?","Notation indicating that the transpose of X belongs to a certain set of real numbers." 
"eigenvalues","Values that provide insight into the variance represented by the eigenvectors of a matrix."
"subspace","A space that is a smaller dimensionality within a higher-dimensional space."
"XTX","Matrix multiplication involving the transpose of X and itself."
"cm","Notation typically representing a column vector in matrix equations."
"y","individual components prior to updating the variances."
"variances","after updating the variances."
"Monte Carlo estimate","as a Monte Carlo estimate of the weighted covariance of data points."
"weighted covariance","weighted covariance of data points x,, associated with the ‘th mixture component."
"weights","where the weights are the responsibilities r,,;,. "
"responsibilities","through the responsibilities T'nk, which prohibits a closed-form solution."
"mixture weights","The mixture weights of the GMM are updated as."
"GMM","mixture weights of the GMM."
"data points","where N is the number of data points."
"partial derivative","To find the partial derivative of the log-likelihood with respect to the weight parameters."
"log-likelihood","partial derivative of the log-likelihood with respect to the weight parameters."
"weight parameters","with respect to the weight parameters 7,, k = 1,...,K."
"Lagrange multipliers","by using Lagrange multipliers (see Section 7.2)."
"Lagrangian","The Lagrangian is."
"Parameter Learning","The process of adjusting parameters in a model to maximize the likelihood of the data."
"Maximum Likelihood" ,"A statistical method for estimating the parameters of a model by maximizing the likelihood function."
"log-likelihood","A logarithmic transformation of the likelihood function, often used for simplification in optimization problems."
"Partial Derivative","A derivative taken with respect to one variable while holding others constant, indicating the rate of change of a function."
"Equality Constraint","A condition that specifies that a certain equation must hold true, in this case that mixture weights sum to 1."
"Lagrange Multiplier","A strategy used in optimization to incorporate constraints into the optimization problem."
"Optimum","The best or most favorable condition, typically referring to maximum or minimum values in optimization."
"System of Equations","A set of equations with multiple variables that are solved together to find values satisfying all equations."
"Weight Parameters","Coefficients in a model that are adjusted during training to achieve better predictions."
"gradient","The negative gradient at a points north and east, leading to a specific coordinate."
"asymptotic rate of convergence","The rate at which a sequence approaches a limit, noted to be inferior to many other methods."
"ball rolling down the hill analogy","A metaphor used to explain how optimization methods behave in relation to the landscape of a function."
"poorly conditioned","A situation where a problem can cause challenges in convergence, especially noted in convex problems."
"zigzags","Refers to the erratic movement of gradient descent when gradients point nearly orthogonally to the shortest direction to a minimum point."
"step-size","A parameter in gradient descent that affects the speed and stability of convergence, with implications for being too small or too large."
"momentum","A technique in gradient descent to help improve convergence behavior."
"Convex Conjugates","Application of convex conjugates to illustrate examples in machine learning."
"Quadratic Function","A function based on a positive definite matrix K."
"Positive Definite Matrix","A matrix K belonging to R**n x n that has specific properties leading to certain desirable characteristics in optimization."
"Primal Variable","Denoted as y belonging to R**n."
"Dual Variable","Denoted as a belonging to R**n."
"Supremum","The least upper bound of a set, represented here as sup (y,a)."
"Differentiable Function","A function that has a derivative at each point in its domain."
"Maximum","The highest value of a function, obtained by finding where the derivative equals zero."
"Gradient","A multi-variable generalization of the derivative, giving the direction and rate of fastest increase of a function."
"Substituting","The process of replacing one variable with another in an equation."
"Sums of Functions","The combination of multiple functions, often used in optimization scenarios."
"ower iteration","chooses a random vector that is not in."
"PCA","Key Steps of PCA in Practice."
"centering","Subtracting the mean from each data point."
"eigenvalues","Compute eigenvalues and eigenvectors of the data covariance matrix."
"eigenvectors","Compute eigenvalues and eigenvectors of the data covariance matrix."
"data covariance matrix","The ellipse represents the data covariance matrix."
"project","We can project any data point onto the principal subspace."
"standardize","We need to standardize x, using the mean and standard deviation." 
"variance","Data has variance 1 along each axis."
"principal subspace","The longer vector spans the principal subspace." 
"data point","We can project any data point x onto the principal subspace."
"mean","Subtracting the mean from each data point." 
"standard deviation","Using the mean and standard deviation of the data."
"Quasi-Newton methods","use cheaper computational methods to approximate the Hessian."
"Hessian","a square matrix of second-order partial derivatives of a scalar-valued function."
"Mirror descent","an approach for computing descent directions."
"Natural gradient","a method for optimizing parameters by taking into account the geometry of the parameter space."
"Non-differentiable functions","functions that have kinks and are not well-defined for gradient methods."
"Subgradient methods","techniques used for optimizing non-differentiable functions."
"Continuous optimization problems","problems that involve optimizing a continuous objective function."
"Constrained optimization problems","problems that impose restrictions or constraints on the optimization process."
"dimensionality","the dimensionality of the lower-dimensional subspace onto which we project the data"
"noise variance","estimate the noise variance o? of the data"
"positive semidefinite","the matrix A — o?I is guaranteed to be positive semidefinite"
"eigenvalue","the smallest eigenvalue of the data covariance matrix is bounded from below by the noise variance o?"
"Bayesian PCA","Bayesian PCA"
"factor analysis","factor analysis"
"independent component analysis","independent component analysis ICA"
"blind-source separation","blind-source separation"
"singular values","discard all singular values smaller than sefP"
"cross-validation","use (nested) cross-validation to determine a good estimate of the intrinsic dimensionality of the data"
"Bayesian model selection criteria","Bayesian model selection criteria to determine a good estimate of the intrinsic dimensionality of the data" 
"intrinsic dimensionality","a good estimate of the intrinsic dimensionality of the data"
"prior","knowledge about where good parameters lie in machine learning"
"regularization","introduces an additional term that biases the resulting parameters to be close to the origin"
"maximum a posteriori estimation","bridges the non-probabilistic and probabilistic worlds by acknowledging the need for a prior distribution"
"maximum likelihood estimate","possesses properties such as asymptotic consistency"
"asymptotic consistency","the MLE converges to the true value in the limit as the sample size goes to infinity"
"maximum a posteriori","another term for maximum a posteriori estimation"
"link function","a function used in generalized linear models to relate linear predictor to the mean of the distribution function"
"generalized linear model","a framework for modeling the relationship between a response variable and one or more explanatory variables"
"generative process","the underlying process that generates the data being modeled"
"linear regression","a statistical method for modeling the relationship between a dependent variable and one or more independent variables."
"prior distribution","a probability distribution that represents one's beliefs about a parameter before observing the data."
"parameters","quantifiable characteristics of a statistical model that are estimated from data."
"dimensionality","the number of dimensions in a mathematical space, often referring to the number of features or variables in data."
"latent space","a representation of underlying factors that are not directly observed but are inferred from observed data."
"Bayesian PCA","a probabilistic approach to principal component analysis that incorporates prior distributions on model parameters."
"generative process","a model that describes how data is generated in a probabilistic framework."
"overfitting","a modeling error that occurs when a model is too complex and captures noise instead of the underlying pattern in the data."
"analytically intractable","a term referring to problems that cannot be solved exactly using analytical methods."
"approximate inference methods","techniques used to estimate probabilistic model parameters that are difficult to compute exactly."
"MCMC","Markov Chain Monte Carlo, a class of algorithms for sampling from probability distributions."
"variational inference","a technique in Bayesian inference that approximates intractable integrals by transforming them into optimization problems."
"PPCA","Probabilistic Principal Component Analysis, which incorporates probabilistic models into principal component analysis."
"linear model","a mathematical model that assumes a linear relationship between inputs and outputs."
"N(a@n|Bzn + 1, 0°)","a notation describing a normal distribution with mean and variance parameters, commonly used in statistical models."
"partial derivatives","quantities computed from a function that give the rate of change of that function in relation to its variables."
"responsibilities","a quantity defined as the responsibility of a mixture component for a data point."
"likelihood","the probability of a mixture component given the data point."
"mixture component","a part of a mixture model that represents a probability distribution of a subset of the data."
"normalized probability vector","a vector whose components are non-negative and sum to one."
"probability","a measure of the likelihood that an event will occur."
"Integrating out","parameters induces a distribution over functions."
"mean function","The mean of the Bayesian linear regression model."
"Bayesian Linear Regression","A statistical method that models the relationship between a dependent variable and one or more independent variables using Bayesian principles."
"Posterior distribution","The distribution of an unknown quantity, updated with new evidence from data."
"MAP estimate","Maximum a posteriori estimate, the mode of the posterior distribution."
"predictive uncertainty","The sum of the noise term and the posterior parameter uncertainty."
"Training data","Data used to train a model."
"polynomials of degree","Mathematical expressions involving variables raised to a whole number exponent."
"posterior over functions","The distribution of functions as inferred from the posterior distribution."
"model class","the model class we would want to work with since it has good generalization properties."
"generalization","the property of a model that indicates its ability to perform well on unseen data."
"overfitting","a modeling error that occurs when a model is too complex and captures noise instead of the underlying pattern."
"regularization","a technique used to mitigate overfitting by adding a penalty on the size of coefficients."
"priors","information or beliefs about parameters that can be incorporated into a probabilistic model."
"maximum likelihood estimation","a principle that generalizes the idea of least-squares regression for linear models."
"least-squares regression","a statistical method used to determine the line of best fit by minimizing the sum of the squares of the residuals."
"predictor","a variable used to predict the value of another variable."
"nonlinear function","a function that does not form a straight line when graphed."
"binary classification","a classification task that involves categorizing data into one of two groups."
"count data","data that represent counts of occurrences."
"likelihoods","probability measures used to evaluate how well a statistical model explains observed data."
"exponential family","a set of probability distributions that have certain mathematical properties."
"n turns"",""to generalize well (Steinwart and Christmann, 2008)."""  
"margin","the distance that separates positive and negative examples."  
"closest examples","two or more examples that are nearest to a hyperplane."  
"hyperplane","a decision boundary that separates different classes in the data."  
"linearly separable","a type of dataset where classes can be separated by a linear boundary."  
"classifiers","models that assign a label to input data based on training."  
"separating hyperplane","a hyperplane that divides space into different class regions."  
"maximizes the margin","the goal of finding a hyperplane that creates the largest possible gap between classes."  
"distance between an example and a hyperplane","a measure used to compute how far a point is from the decision boundary."  
"closest point on the hyperplane","the nearest point on the hyperplane to a given data point."
"E-step","Evaluate the responsibilities r,,,, (posterior probability of data point n belonging to mixture component k)."
"M-step","Use the updated responsibilities to reestimate the parameters My, Uk, Te."
"log-likelihood function","Every step in the EM algorithm increases the log-likelihood function."
"convergence","For convergence, we can check the log-likelihood or the parameters directly."
"EM algorithm","A concrete instantiation of the EM algorithm for estimating the parameters of a GMM."
"responsibilities","Evaluate responsibilities r,,,, for every data point x, using current parameters."
"GMM","Example 11.6 (GMM Fit) describes the GMM fit after iterations."
"The prior","A component in Bayesian inference which represents the initial beliefs before observing data."
"The likelihood" ,"A function that measures how well a model explains the observed data."
"Product rule","A principle used in probability that states the joint probability of two events is the product of the probability of one event and the conditional probability of the other given the first."
"Marginal likelihood p(a)","The likelihood of the observed data averaged over all possible values of the parameters."
"Joint distribution","A probability distribution that represents the likelihood of multiple random variables occurring simultaneously."
"Integrating out the parameters","The process of summing or averaging a function over the values of a variable to obtain a marginal distribution."
"Sum rule","A principle in probability that states the total probability of an outcome is the sum of the probabilities of all the ways that outcome can occur."
"Posterior","The updated probability distribution of a parameter given the observed data."
"Probabilistic model","A mathematical representation of a system that incorporates random variables."
"Model parameters","Values that define the behavior of a model."
"Maximum likelihood","A method for estimating the parameters of a statistical model that maximizes the likelihood function."
"Maximum a posteriori estimation","A method that combines prior beliefs and observed data to find the most likely parameter estimates."
"Parameter estimation","The process of using data to infer the values of parameters of a model."
"Optimization problem","A mathematical problem of finding the best solution from all feasible solutions."
"EM Algorithm","Expectation maximization algorithm (EM algorithm) is a simple iterative scheme for finding a solution to the parameters estimation problem via maximum likelihood."
"maximum likelihood","A statistical method used for estimating the parameters of a mix model."
"Gaussian mixture model","A probabilistic model that assumes all data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters."
"negative log-likelihood","A function that is minimized during the EM algorithm iterations; lower values indicate a better fit of the model to the data."
"mixture model","A model that represents the presence of subpopulations within an overall population, exemplified by a Gaussian mixture model."
"Laplace prior","a prior distribution used in regression that encourages parameters to be 0."
"linear regression","a statistical method for modeling the relationship between a dependent variable and one or more independent variables."
"L1 regularization","a technique used in regression that involves adding a penalty equal to the absolute value of the magnitude of coefficients."
"LASSO","an acronym for Least Absolute Shrinkage and Selection Operator, which is a type of linear regression that includes L1 regularization."
"Laplace distribution","a probability distribution that is sharply peaked at zero and concentrates its probability mass closer to zero than the Gaussian distribution."
"Gaussian distribution","a probability distribution characterized by its bell-shaped curve, also known as normal distribution."
"variable selection","the process of selecting a subset of relevant features for use in model construction."
"linearly independent","a set of vectors in which no vector can be expressed as a linear combination of the others."
"orthogonal basis","a basis in which all vectors are orthogonal to each other, meaning they are at right angles."
"Gram-Schmidt process","an algorithm for converting a set of linearly independent vectors into an orthogonal set."
"identically distributed","All the random variables are from the same distribution."
"conditional independence","Two random variables X and Y are conditionally independent given Z if and only if p(x, y|z) =p(a|z)p(y|z) forall ze Z."
"p(x, y|z)","The joint probability of random variables X and Y given Z."
"p(a|z)","The conditional probability of random variable A given Z."
"p(y|z)","The conditional probability of random variable Y given Z."
"X IL Y | Z","Denotes that X is conditionally independent of Y given Z."
"product rule of probability","A rule that allows the expansion of joint probabilities."
"monotonic convergence","ristic guarantees monotonic convergence."
"linear equations","When we solve linear equations of the form Ag = 5."
"approximately","by finding x, that minimizes the squared error."
"squared error","|| Ax — b||? = (Aw — b)'(Aa — b)."
"Euclidean norm","if we use the Euclidean norm."
"gradient","The gradient of (7.9) with respect to x is Va =2(Ax—b)'A."
"gradient descent","We can use this gradient directly in a gradient descent algorithm."
"analytic solution","it turns out that there is an analytic solution."
"condition number","The speed of convergence of gradient descent is dependent on the condition number k = aoe."
"singular value","the ratio of the maximum to the minimum singular value."
"signal processing"",""a field that deals with the analysis, interpretation, and manipulation of signals."  
"Kalman filter"",""an algorithm that uses a series of measurements observed over time to produce estimates of unknown variables."  
"control"",""a field in engineering that deals with the behavior of dynamic systems."  
"linear quadratic regulator"",""a method to design a controller that regulates the behavior of a dynamic system to minimize a cost function."  
"statistics"",""the science of collecting, analyzing, and interpreting data."  
"hypothesis testing"",""a statistical method used to make decisions based on the analysis of data."  
"Gaussian distribution"",""a continuous probability distribution characterized by its bell-shaped curve."  
"normal distribution"",""a specific case of the Gaussian distribution where the mean is zero and the standard deviation is one."  
"central limit theorem"",""a theorem that states that the sum of a large number of independent random variables will be approximately normally distributed."  
"bivariate Gaussian"",""a Gaussian distribution involving two variables."  
"marginal distribution"",""the probability distribution of a subset of a collection of random variables."  
"joint Gaussian distribution"",""the probability distribution of two or more random variables considered simultaneously."  
"conditional distribution"",""the probability distribution of one random variable given the value of another."
"probabilistic model","A model that comes with a likelihood function and can explicitly deal with noisy observations."
"likelihood function","A function that measures the probability of observing the data given a set of parameters."
"Bayesian model comparison","A method to compare different models based on their marginal likelihood."
"marginal likelihood","The probability of the observed data under a specific model, integrating over all unknown parameters."
"PCA","Principal Component Analysis, a technique for dimensionality reduction."
"generative model","A model that allows us to simulate new data based on the learned structure."
"average squared reconstruction error","The sum of the eigenvalues in the orthogonal complement of the principal subspace."
"eigenvalues","Values that characterize the magnitude of the principal axes in a data set."
"orthogonal complement","The space formed by all vectors that are orthogonal to a given subspace."
"Bayes' theorem","A mathematical formula that describes how to update the probabilities of hypotheses when given evidence."
"novelty of a new data point","A measurement of how different a new data point is compared to existing data points."
"A640 x 480 pixel" is a data point in a million-dimensional space" A640 x 480 pixel" is a data point in a million-dimensional space, where every pixel responds to three dimensions, one for each color channel (red, green, blue).  
"principal component analysis" A method used in statistics to reduce the dimensionality of data while preserving as much variance as possible.  
"PCA" Abbreviation for principal component analysis.  
"dimensionality reduction" The process of reducing the number of random variables under consideration, obtaining a set of principal variables.  
"data covariance matrix" A matrix that contains the covariances between pairs of elements of a dataset.  
"basis" A set of vectors in a vector space that, in a linear combination, can represent every element of that space.  
"basis change" The process of converting coordinates from one basis to another.  
"projections" The process of mapping a higher-dimensional space to a lower-dimensional space.  
"eigenvalues" Scalars that provide information about the stretch of the transformation represented by a matrix in eigenvalue problems.  
"Karhunen-Loéve transform" A mathematical procedure used in data analysis that is synonymous with principal component analysis.
