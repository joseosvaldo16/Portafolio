{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Azure Open AI + MS SQL with Langchain for Larger Databases\n",
    "\n",
    "When working with databases that have numerous tables and rows, inputting the entire schema into a Language Model (LLM) prompt can become cumbersome and impractical. To address this issue, the Langchain library offers a solution designed to retrieve the schema information only for the tables relevant to the user's query. This enables efficient interaction with the database using Azure Open AI and MS SQL.\n",
    "\n",
    "## How Langchain Works\n",
    "\n",
    "1. **User Input Analysis**: Langchain begins by analyzing the user's input along with the database table names provided.\n",
    "\n",
    "2. **Schema Extraction**: After understanding the user query, Langchain identifies the specific tables that are relevant to the query. It then extracts the schema information for these relevant tables.\n",
    "\n",
    "3. **Context Generation**: Langchain constructs a context that includes the extracted schema information. This context is then combined with a system message, which instructs the Language Model (LLM) to convert natural language into SQL.\n",
    "\n",
    "4. **SQL Query Translation**: With the schema information and the system message, a standard LLM (GPT4) is now equipped to translate the user's natural language query into SQL code.\n",
    "\n",
    "5. **Execution on the Server**: The generated SQL code is executed on the server-side, allowing for efficient and accurate database interactions.\n",
    "\n",
    "6. **Result Retrieval and Display**: Finally, the results of the SQL query are retrieved from the database server and presented to the user.\n",
    "\n",
    "## Benefits of Using Langchain\n",
    "\n",
    "- **Efficiency**: Langchain streamlines the process of working with large databases by focusing on relevant schema information, saving time and resources.\n",
    "\n",
    "- **Clarity**: Users can communicate with the database using natural language queries, enhancing the accessibility of database interactions.\n",
    "\n",
    "- **Accuracy**: By providing context and relevant schema data, Langchain helps ensure that the SQL queries generated by the LLM are precise and meaningful.\n",
    "\n",
    "This integration of Azure Open AI, MS SQL, and Langchain offers a powerful solution for effectively handling larger databases while maintaining a user-friendly and efficient experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Server Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_tools import create_extraction_chain_pydantic\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_openai import AzureChatOpenAI #Use Azure OpenAI for secure chat\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings #Use Azure OpenAI for secure embeddings\n",
    "from langchain_community.agent_toolkits.sql.base import create_sql_agent\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import pyodbc\n",
    "import re\n",
    "import ast\n",
    "import ast\n",
    "import re\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from operator import itemgetter\n",
    "from decimal import Decimal\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OpenAI configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"\"\n",
    "openai.api_version = \"2023-12-01-preview\"\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\n",
    "\n",
    "\n",
    "# Database connection parameters\n",
    "server = ''\n",
    "database = ''\n",
    "username = ''\n",
    "password = ''\n",
    "driver = 'ODBC+Driver+18+for+SQL+Server'\n",
    "driver2 = '{ODBC Driver 18 for SQL Server}' ## change according to your driver\n",
    "\n",
    "# sqlalchemy connection needed for retrieving relevant tables \n",
    "db_uri = f'mssql+pyodbc://{username}:{password}@{server}/{database}?driver={driver}'\n",
    "engine = create_engine(db_uri)\n",
    "\n",
    "db = SQLDatabase(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the large language model\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"mdxgpt4\",\n",
    "    temperature= 0,\n",
    "    openai_api_type= AgentType.OPENAI_FUNCTIONS\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the relevant tables from the database\n",
    "\n",
    "Leverage GPT to retrieve the most relevant tables from a database based on a given query and context. This approach is especially beneficial in environments with numerous tables, as it streamlines the process of interacting with large databases. Instead of passing the entire database schema—which could exceed the input token limit of the model—this method focuses on first identifying and retrieving the tables most pertinent to the query. By doing so, it minimizes the amount of schema information required as input, making the process more efficient and manageable. This targeted retrieval ensures that only the necessary data is processed, leading to quicker and more accurate responses from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model for tables\n",
    "class Table(BaseModel):\n",
    "    name: str \n",
    "\n",
    "# Get usable table names from your database\n",
    "table_names = \"\\n\".join(db.get_usable_table_names())\n",
    "\n",
    "# System message for extraction chain\n",
    "system = f\"\"\"Return the names of ALL the SQL tables that MIGHT be relevant to the user question.\n",
    "The tables are:\n",
    "\n",
    "{(table_names)}\n",
    "\n",
    "Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed.\"\"\"\n",
    "\n",
    "# Create an extraction chain for table names\n",
    "table_chain = create_extraction_chain_pydantic(Table, llm, system_message=system)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names(table_list) -> str:\n",
    "    names = []\n",
    "    for item in table_list:\n",
    "        names.append(item.name)\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_chain = table_chain| get_table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_chain = {\"input\": itemgetter(\"question\")} | table_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema_query = ''' SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE \n",
    "                FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                ORDER BY TABLE_NAME, ORDINAL_POSITION; '''\n",
    "\n",
    "\n",
    "\n",
    "def get_table_schema(table_list):\n",
    "    schema = []\n",
    "    for item in table_list:\n",
    "        schema.append(db.get_table_schema(item))\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databse for Hight Cardinality Columns\n",
    "\n",
    "\n",
    "A vector database for high cardinality columns can be helpful during the querying process by comparing top matching words and potentially replacing any misspellings. Let's break down how it works:\n",
    "\n",
    "1. Vector Database: A vector database is a data structure that stores vectors, which are mathematical representations of data points. In this case, the vectors represent words or phrases in the columns with high cardinality, such as proper nouns, keywords, or text columns.\n",
    "\n",
    "2. High Cardinality Columns: High cardinality columns refer to columns that have a large number of distinct values. For example, a column containing names of people or products may have high cardinality because there can be thousands or even millions of unique names.\n",
    "\n",
    "3. Top Matching Words: When querying the vector database, you can compare the input query with the vectors stored in the database. By using techniques like cosine similarity or nearest neighbor search, you can identify the top matching words or phrases that are most similar to the query. This allows you to find relevant matches even if there are slight variations or misspellings in the input.\n",
    "\n",
    "4. Replacing Misspellings: One of the advantages of using a vector database is its ability to handle misspellings. By comparing the query with the vectors in the database, you can identify potential misspellings and suggest alternative words or phrases that are similar to the query. This can help improve the accuracy of the search results and provide better user experience.\n",
    "\n",
    "Overall, a vector database for high cardinality columns provides a powerful tool for efficient querying and handling misspellings. It leverages vector representations and similarity measures to find top matching words and suggest replacements for misspelled queries, improving the accuracy and effectiveness of the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company\n",
      "Customer\n",
      "Customer_Regeion\n",
      "Sales_Territory\n",
      "SalesRep_Name_1\n",
      "PO\n",
      "PG\n",
      "Part_Number\n",
      "RM_FG\n",
      "Part_Description\n",
      "Full_PG_2\n",
      "PG_2\n",
      "Ship_to_Name\n",
      "Ship_To_State\n",
      "Ship_To_Country\n",
      "Ship_to_Sales_Rep\n",
      "Open_Order\n",
      "Open_Rel\n",
      "TYPE\n",
      "Company\n",
      "Customer\n",
      "Customer_Region\n",
      "Sales_Territory\n",
      "SalesRep_Name_1\n",
      "Credit_Memo\n",
      "PG\n",
      "Part_Number\n",
      "RM_FG\n",
      "Line_Description\n",
      "Full_PG_2\n",
      "PG_2\n",
      "GL_Account\n",
      "PO\n",
      "Ship_To_Name\n",
      "Ship_To_State\n",
      "Ship_To_Country\n",
      "Ship_To_Sales_Rep\n",
      "TYPE\n",
      "Consolidated_Customer_Name\n",
      "Ship_To_Name\n",
      "Part_Number\n",
      "Consolidated_Customer_Name\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noun_extraction_query = \"\"\"\n",
    "SELECT TABLE_NAME, COLUMN_NAME\n",
    "FROM INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE DATA_TYPE IN ('nvarchar', 'varchar')\n",
    "AND TABLE_NAME <> 'database_firewall_rules'\n",
    "\"\"\"\n",
    "# Use db.run instead of cursor.execute\n",
    "result = ast.literal_eval(db.run(noun_extraction_query))\n",
    "\n",
    "\n",
    "# Store unique values from each column in this list\n",
    "proper_nouns = []\n",
    "\n",
    "\n",
    "if result:  # Check if the result list is not empty\n",
    "    for row in result:\n",
    "        table_name = row[0]\n",
    "        column_name = row[1]\n",
    "\n",
    "        # Query to get all values from the column\n",
    "        query = f\"SELECT {column_name} FROM {table_name}\"\n",
    "\n",
    "        # Use db.run instead of cursor.execute\n",
    "        result = ast.literal_eval(db.run(query))\n",
    "\n",
    "        # Check if the majority of the values are numerical\n",
    "        numerical_values = [value for value in result if re.match(r'^\\d+$', str(value[0]))]\n",
    "        if len(numerical_values) > len(result) / 2:\n",
    "            continue  # Skip this column if the majority of the values are numerical\n",
    "        print(column_name)\n",
    "        # Query to get all unique values from the column\n",
    "        query = f\"SELECT DISTINCT {column_name} FROM {table_name}\"\n",
    "\n",
    "        # Use db.run instead of cursor.execute\n",
    "        result = ast.literal_eval(db.run(query))\n",
    "\n",
    "        # Add the unique values to the list\n",
    "        proper_nouns.extend([value[0] for value in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns = list(set(proper_nouns))  # Remove duplicates\n",
    "pd.DataFrame(proper_nouns).to_csv('Proper_Nouns.csv', index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns = pd.read_csv('Proper_Nouns.csv', encoding='utf-8', header=0)\n",
    "len(proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up openai embeddings\n",
    "\n",
    "embedder = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=\"text_model\",\n",
    "    openai_api_type= AgentType.OPENAI_FUNCTIONS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 0\n",
      "Processed batch 1\n",
      "Processed batch 2\n",
      "Processed batch 3\n",
      "Processed batch 4\n",
      "Processed batch 5\n",
      "Processed batch 6\n",
      "Processed batch 7\n",
      "Processed batch 8\n",
      "Processed batch 9\n",
      "Processed batch 10\n",
      "Processed batch 11\n",
      "Processed batch 12\n",
      "Processed batch 13\n",
      "Processed batch 14\n",
      "Processed batch 15\n",
      "Processed batch 16\n",
      "Processed batch 17\n",
      "Processed batch 18\n"
     ]
    }
   ],
   "source": [
    "# Define batch size\n",
    "batch_size = 500  # Adjust the batch size based on your needs and system capabilities\n",
    "\n",
    "# Split the proper_nouns list into batches\n",
    "batches = [proper_nouns[i:i + batch_size] for i in range(0, len(proper_nouns), batch_size)]\n",
    "\n",
    "# Initialize an empty list to hold all embeddings\n",
    "all_embeddings = []\n",
    "\n",
    "\n",
    "batch_number = 0\n",
    "#create vector from first row, save then add the rest of the rows\n",
    "vector_db = FAISS.vector_store(batches[0], embedder)\n",
    "vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "for batch in batches[1:]:\n",
    "    await vector_db.aadd_texts(batch)\n",
    "    vector_db.save_local(f\"faiss_index\")\n",
    "    print(f'Processed batch {batch_number}')\n",
    "    batch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.load_local('faiss_index', embedder, allow_dangerous_deserialization=True)\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = '''\n",
    "        As an MS SQL expert, please create a syntactically correct MS SQL query in response to the given input question. Adhere to the following guidelines:\n",
    "\n",
    "        - List specific column names instead of using \"SELECT *\", even when all columns are needed.\n",
    "        - Use only read-only operations (e.g., SELECT) and avoid commands that modify the database (like UPDATE, CREATE TABLE). (most important!)\n",
    "        - For the current date, use CAST(GETDATE() as date).\n",
    "        - Focus on the provided table and column information. Use columns that exist in the specified tables.\n",
    "        - Replace 'LIMIT' with the 'TOP' clause\n",
    "        - It is crucial to format numerical results with two decimal places and a thousands separator. Please ensure to use the 'FORMAT' function in SQL for formatting currency and numerical outputs, especially in computations like summing or subtracting prices and costs. (important!)\n",
    "        - If result is expected to be currency such as profit, or sales price, please format the result as currency with a dollar sign, thousands separator, and two decimal places.\n",
    "        - Exclude extraneous explanations; only return the query in MS SQL syntax. Your must be a runnable sql query!\n",
    "        - Use SUM insteead of SELECT COUNT(*) for numerical aggregations.\n",
    "        - If you need to select rows use the headers provided in the schema_info instead of using SELECT *.\n",
    "        - Format quantity values with a thousands separator.\n",
    "        - When translating a natural language query into SQL, particularly when the request involves calculating rankings or comparisons for each year or month, you should utilize the PARTITION BY clause within window functions instead of solely using GROUP BY. \n",
    "          The PARTITION BY clause is essential for dividing the dataset into distinct partitions (like per year or per month) to perform calculations independently within each partition. This approach is crucial for scenarios where you need to identify top performers, trends, or comparisons within each time segment.\n",
    "          For aggregate calculations that summarize data across the whole dataset, continue to use GROUP BY\"\n",
    "        - Limit the number of rows returned by the query to {top_k} unless instructed otherwise.\n",
    "                --------------------------------------------------------------------------------\n",
    "\n",
    "    Tables and Columns to be considered:\n",
    "        {schema_info}\n",
    "\n",
    "\n",
    "\n",
    "        (Note: Do not include 'database_firewall_rules', 'sysdiagrams', or any tables with 'aspnet' in their names.)\n",
    "        -------------------------------------------------------------------------------------------------\n",
    "        Cross-verify the spelling of nouns against this provided, though non-exhaustive, list: {proper_nouns}.\n",
    "        Correct the spelling of company names, product names/ parts, or other porper nouns when constructing the query by making use of the provided list of keywords.\n",
    "\n",
    "        Your response should exclusively contain the MSSQL query in the correct syntax, utilizing 'TOP' instead of 'LIMIT', without any extra explanations or text.\n",
    "\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", sys_prompt), (\"human\", \"{input}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_agent = create_sql_agent(llm=llm, db=db, agent_type=\"openai-tools\", verbose=True)\n",
    "query_chain = create_sql_query_chain(llm, db,prompt=prompt)\n",
    "RunnablePassthrough.assign(table_names_to_use=table_chain)|query_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain = (\n",
    "    itemgetter(\"question\")\n",
    "    | retriever\n",
    "    | (lambda docs: \"\\n\".join(doc.page_content for doc in docs))\n",
    ")\n",
    "chain = RunnablePassthrough.assign(proper_nouns=retriever_chain) | query_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```sql\\nSELECT TOP 5 CAST(Customer AS NVARCHAR(MAX)) AS Customer, SUM(CAST(USD_Total_Price AS MONEY)) AS TotalSales\\nFROM [CURRENT_SALES]\\nGROUP BY Customer\\n```'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is the total sales for each customer?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(user_query):\n",
    "    query = query_chain.invoke({\"question\": f'{user_query}',\"proper_nouns\": \"\"})\n",
    "    query =  query.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "    result = db.run(query)\n",
    "    result = re.sub(r\"Decimal\\('([^']*)'\\)\", r\"\\1\", result)\n",
    "    result = ast.literal_eval(result)\n",
    "    result = tabulate(result, headers='keys', tablefmt='pretty_grid')\n",
    "    return \"SQL Query: \\n\" + query + \"\\n\\n\" + \"Result: \\n\" + result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Query: \n",
      "SELECT TOP 5 Company, Year, SUM(USD_Total_Price) AS Total_Sales\n",
      "FROM CURRENT_SALES\n",
      "GROUP BY Company, Year\n",
      "\n",
      "Result: \n",
      "0        1            2\n",
      "----  ----  -----------\n",
      "10US  2017  4.66686e+07\n",
      "20BV  2017  1.08592e+07\n",
      "30CH  2017  2.66865e+06\n",
      "10US  2018  4.21011e+07\n",
      "20BV  2018  1.00496e+07\n"
     ]
    }
   ],
   "source": [
    "print(run_query(\"What are the total sales for each company for each year?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated how to use the Langchain library in conjunction with Azure OpenAI and MS SQL to efficiently interact with large databases. The Langchain library provides a streamlined approach to extract schema information only for the tables relevant to the user's query, making it practical to work with databases that have numerous tables and rows.\n",
    "\n",
    "The notebook uses several Python libraries and frameworks, including Pydantic for data validation and settings management, SQLAlchemy for SQL toolkit and Object-Relational Mapping, and OpenAI for AI models. It also uses the AzureChatOpenAI class from the Langchain library to initialize the large language model.\n",
    "\n",
    "The Langchain library, combined with Azure OpenAI and MS SQL, offers a powerful solution for effectively handling larger databases. It allows users to communicate with the database using natural language queries, enhancing the accessibility of database interactions. By providing context and relevant schema data, Langchain helps ensure that the SQL queries generated by the Language Model are precise and meaningful.\n",
    "\n",
    "In real world applications, this could be used to allow no technical employees or clients query large sql databases effortlessly, empowering them to make more data driven decisions.\n",
    "\n",
    "Overall, the integration of Azure OpenAI, MS SQL, and Langchain offers a user-friendly and efficient experience for effectively handling larger databases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
