# Langchain LargeDB Notebook

This notebook demonstrates how to use the Langchain library in conjunction with Azure OpenAI and MS SQL to efficiently interact with large databases. The Langchain library provides a streamlined approach to extract schema information only for the tables relevant to the user's query, making it practical to work with databases that have numerous tables and rows.

## Setup and Configuration

The notebook uses several Python libraries and frameworks, including Pydantic for data validation and settings management, SQLAlchemy for SQL toolkit and Object-Relational Mapping, and OpenAI for AI models. It also uses the AzureChatOpenAI class from the Langchain library to initialize the large language model.

### Imports

The notebook begins by importing the necessary libraries and modules. These include standard Python libraries like `os`, `re`, `pandas`, and `numpy`, as well as specific modules from the Langchain library and other dependencies.

```python
from langchain.chains.openai_tools import create_extraction_chain_pydantic
from langchain.chains import create_sql_query_chain
from langchain_core.pydantic_v1 import BaseModel
from langchain_openai import AzureChatOpenAI
from langchain_community.utilities import SQLDatabase
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.agents.agent_types import AgentType
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.agent_toolkits.sql.base import create_sql_agent
```

### Configuration

The notebook requires configuration of OpenAI and database connection parameters. These include the OpenAI API type, version, and base, as well as the Azure OpenAI API key and endpoint. The database connection parameters include the server, database, username, password, and driver details. These values need to be filled in before running the notebook.

```python
openai.api_type = "azure"
openai.api_base = ""
openai.api_version = "2023-12-01-preview"

os.environ["AZURE_OPENAI_API_KEY"] = ""
os.environ["AZURE_OPENAI_ENDPOINT"] = ""

server = ''
database = ''
username = ''
password = ''
driver = 'ODBC+Driver+18+for+SQL+Server'
driver2 = '{ODBC Driver 18 for SQL Server}'
```

## Database Interaction

The notebook demonstrates how to interact with a SQL database using the Langchain library. It shows how to initialize a large language model, extract schema information from a database, and generate SQL queries based on natural language inputs.

### Large Language Model Initialization

The large language model is initialized using the `AzureChatOpenAI` class from the Langchain library. The `openai_api_version` and `azure_deployment` parameters need to be set according to your Azure OpenAI setup.

```python
llm = AzureChatOpenAI(
    openai_api_version="2023-12-01-preview",
    azure_deployment="",
    temperature= 0,
)
```

### Schema Extraction

The notebook includes code for extracting schema information from a database. This involves running a SQL query to get all table and column names from the database. The result is stored in a list of proper nouns.

```python
noun_extraction_query = """
SELECT TABLE_NAME, COLUMN_NAME
FROM INFORMATION_SCHEMA.COLUMNS
WHERE DATA_TYPE IN ('nvarchar', 'varchar')
AND TABLE_NAME <> 'database_firewall_rules'
"""
result = ast.literal_eval(db.run(noun_extraction_query))
```

### Query Generation

The notebook demonstrates how to generate SQL queries based on natural language inputs. This involves creating a SQL agent and a query chain, and then invoking the chain with a natural language question.

```python
sql_agent = create_sql_agent(llm=llm, db=db, agent_type="openai-tools", verbose=True)
query_chain = create_sql_query_chain(llm, db,prompt=prompt)
RunnablePassthrough.assign(table_names_to_use=table_chain)|query_chain
```

## Conclusion

The Langchain library, combined with Azure OpenAI and MS SQL, offers a powerful solution for effectively handling larger databases. It allows users to communicate with the database using natural language queries, enhancing the accessibility of database interactions. By providing context and relevant schema data, Langchain helps ensure that the SQL queries generated by the Language Model are precise and meaningful.

In real-world applications, this could be used to allow non-technical employees or clients to query large SQL databases effortlessly, empowering them to make more data-driven decisions.

## Requirements

To run this notebook, you will need:

- Python 3.6 or later
- The following Python libraries: `os`, `re`, `pandas`, `numpy`, `pyodbc`, `sqlalchemy`, `openai`
- The Langchain library and its dependencies
- An Azure OpenAI account and API key
- Access to a MS SQL database

Please ensure to replace all placeholder values in the code with your actual values before running the notebook.

You can reference this official Langchain documentaiton for more infomation
https://python.langchain.com/docs/use_cases/sql/large_db